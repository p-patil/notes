1. Transistors
All of modern, electronic computing relies on the principle of building computation out of hardware units with the capability to either block or permit the flow of current. By wiring together such components in the right way, it's possible to persistently represent a bit, and further through appropriate circuit design, to perform mathematical computations on those bits.

The earliest modern computers were electromechanical, and relied on mechanical relays. Essentially, part of a metal conductor was designed so as to be able to swing shut onto another metal pipe, or to open; with current flowing through the other end of the pipe, the opening and closing of the "switch" pipe either permits or blocks the flow of current. The opening and closing is controlled by a control wire, which essentially is a wire near the switch which has an electromagnetic coil in it, so that when current is activated through the wire, it would magnetically pull the switch closed, and release it when the current stopped. So, relays could be used to block or permit current flow; however, they had a lot of disadvantages. Because something is mechanically swinging, there are simple mechanical limits to the switch speed - about 50 times per second. Further, because there are moving parts involved, relays were subject to wear and tear, and had a very high failure rate, and were not reliable. They were also especially prone to environmental factors such as insects (this is where the term "computer bug" originated) that affected the switching.

A major improvement to relays was vacuum tubes. These operated using something called a thermionic valve, which has two electrodes inside a vacuum-sealed glass bulb. When one of the electrodes is heated, it emits electrons (this is called thermionic emission), and the other electrode, if positively charged, will attract the electrons. This allows for current flow. This permits one-way current flow, serving as a device called a diode; to make it into a proper switch that can be turned on and off, a third control electrode that sits in between the two electrodes is inserted; if the control electrode is positively charged, it will allow electron flow between the two initial electrodes; if, however, it's negatively charged, it will not. Because vacuum tubes aren't mechanical, they can operate much faster and are more reliable than relays. These were the basis of radio and telephones for 50 years. Vacuum tubes could still be finicky and could burn out like lightbulbs, and further were quite expensive. Importantly, vacuum tubes, when their cost and reliability improved sufficiently, could be used in computers in the thousands and allowed the computers to be programmable (relay-based computers had to be programmed for each calculation). Even still, vacuum tube computers broke down quickly and had to be repaired daily.

In 1946, Bell lab scientists invented the transistor. Transistors exploit atomic properties of semiconductors (materials that conduct electricity better than an insulator, but worse than a metal) to act as a switch for permitting or blocking current flow. Most commonly, silicon is used as a semiconductor; silicon makes a good semiconductor because it has four valence electrons, allowing it to bond with four other silicon atoms. So, pure silicon arranges itself in a tetrahedral lattice. This on its own poses no reason for serving as an excellent semiconductor, but in practice the base semiconductor material is doped (adulterated by inserting small amounts of a foreign substance) with a small number of other atoms. If atoms with, say, 5 valence electrons are inserted (most commonly phosphorous is used, as it's similar enough to silicon to fit in the lattice), in order to fit into the tetrahedral lattice, they'll have to release one electron, which preserves the structure of the lattice but adds a small amount of relatively free-floating electrons. This is known as n-type (for negative) doping; in p-type (for positive) doping, an element with 3 valence electrons (commonly boron) is inserted, which also preserves the lattice but creates a small amount of electron holes, which can "flow" just the same, but as conceptual positive charges. Note that both n-type and p-type semiconductors are neutrally electrically charged; the difference is in whether they allow negative or positive charges, respectively, to move within them. These semiconductors can be sandwiched together to form a transistor, either with two n-type semiconductors surrounding a p-type (a configuration known as NPN) or two p-type surrounding an n-type (PNP). A source wire of electrons is connected to one end of the sandwich, and a drain wire to the other; a gate is connected to the middle, but is insulated from the middle semiconductor with an oxide dielectric. In an NPN transistor, at rest electrons from the n-type will diffuse to fill the holes in the middle p-type, forming a "depletion layer" that depletes the amount of free electrons; thus, the p-type will now, being saturated, prevent the further flow of electrons. However, when voltage is applied to the gate, the resulting electric field attracts electrons from the n-type into the p-type; if the field is strong enough, it will overcome the depletion layer and allow electrons to move straight through, from the source wire into the n-type, through the p-type, into the other n-type, and into the drain wire. In a PNP transistor, the same effect happens, but in the reverse direction and with holes instead of electrons. Hence, by applying or removing the voltage at the gate, the current either flows or doesn't, creating a switch. Very importantly, transistors, which rely only on atomic properties, can be made very small and operate very fast; further, being composed predominatly of solid materials and exploiting only the laws of electromagnetism, do not wear and tear and thus are very reliable. These are very significant improvements to vacuum tubes. Transistors have been getting smaller and smaller as manufacturing methods for the semiconductors improve, and are now merely tens of atoms thick; however, there is a physical limit to how small they can get, as eventually the p-type in the middle will be so small that, due to quantum mechanical effects, electrons will merely tunnel through even in the absence of a voltage at the gate.

2. Electronic computing
How exactly does the ability to switch on or off the flow of current through a wire allow for computation? The key idea is that transistors can be put together to form processing units that live on a higher abstraction layer, and can store bits - that is, can persistently and reliably store readable and writable representations of boolean values. It's perfectly possible to represent higher-base numbers, but since high and low voltage is used for the representation, by dividing up the voltage range into more segments for each digit in the representation, the margin for error in the voltage value becomes lower. Binary maximizes the margin of error. This principle of storing bits will be the foundation of computer memory. For computation, transistors can also be put together to form gates that have input and output wires, which carry voltage as representations of 0 or 1, and switch the transistors in such a way as to implement a logical operation, such as AND, NOT, or OR.

Note that for a single transistor, if we treat the control wire as an input and the drain wire as the output, the transistor implements the identity logical operator. However, if we instead branch the source wire, then the other branching wire (ie the wire not connecting to the drain), when treated as the output, will carry current precisely when the source-drain wire connection does not (think of the output wire as "competing" with the source wire; if the source wire is blocked by the low-voltage gate, current will instead flow into the output branch; if current is free flowing from source to drain, it will bypass the output wire) (typically in this setup the drain is simply connected to ground). This implements the NOT operator, using a single transistor and another wire.

The AND operator can be implemented by merely stringing two transistors together. By connecting one transistors drain to the other's source, and treating both control wires as input, current will flow from the first transistor's source all the way through to the second transisor's drain if and only if both control wires carry voltage (ie both switches are closed).

Whereas the AND gate wires two transistors in series, we can build an OR gate by wiring two transistors in parallel. Current flows from one mutual source wire which branches to become the source wire of two transistors, whose drains similarly meet at a junction and merge to form a single drain for the whole system. Again treating both control wires as inputs, current flows through from the system source through to the system drain if at least one of the transistors is on; if any one of the branches (or both) has a closed switch, current will be able to get through to the drain.

The three fundamental operators - AND, NOT, and OR - can themselves be combined to form many other logical operators, and in fact are Turing complete. This means that the symbolic manipulation of binary numbers that obeys the rules of addition, subtraction, multiplication, and division can be implemented with the right configuration of the three fundamental logic gates. So, for addition, given two fixed-length sequences of bits, each sequence representing a number in binary, corresponding pairs of bits can be fed into a two-input circuit built out of the right configuration of logic gates; each such circuit has one output wire, representing the value of the corresponding bit in the sum, and a "carry" wire, which is connected as input to the next circuit. So, each such mini-circuit has three inputs (two bits, one for each number, as well as the carry from the previous mini-circuit). For each mini-circuit, we implement an adder for the three input bits - if all three are 0, both output and carry are 0; if one bit is 1, the output is a 1 but carry is 0; if two bits are 1, the output is 0 but carry is 1; and, finally, if all three bits are 1, both output and carry are 1. Put together, with N mini-circuits, an N-bit adder can be implemented. Similarly, we can implement subtraction, multiplication, and division (the latter two can also be implemented by iterating the former two, though this is less efficient).

3. Binary representations of data
So, we now have the ability to wire transistors together in ways that can store bits and perform logical or even mathematical operations on those bits. In the integrated circuits composing the hardware of the computer, single wires carry single bits - represented by low or high voltage - and go in and out of gates and other mini integrated circuits that implement operators. Binary, the base-2 number system, is used to encode integers as numbers, being composed of four bytes (1 byte = 8 bits) on a 32-bit system. To represent negative integers, a scheme called two's complement is used. The simplest scheme would be to simply use the top bit as a minus or plus sign, and the bottom 31 bits for the number's value, but this would make arithmetic a bit difficult. Instead, an idea is to represent the negative of a number X with the idea that such a representation merely needs to have the property that, when added to the normal, positive representation of X, the result is zero. So, the scheme, known as two's complement, is to take the positive value, X, of the number, represented in 32-bit binary as normal, and invert every bit - then, when this number was added back to X, all the 1's in one number will match up to 0's in the other, resulting in 32 1's. Since we want zero, instead we invert X and then add 1; this way, the sum will be 32 1's, plus another one, which will be 2^32 + 1 but will of course in 32-bit integers overflow to zero as desired. This way, arithmetic between positive and negative numbers can use the same hardware as that required for arithmetic between only positive numbers.
