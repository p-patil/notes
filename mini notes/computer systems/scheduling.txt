	scheduling

Modern computing systems almost always require balancing many more processes than available processors, and are responsible for providing the illusion of true parallelism to processes. As such, the operating system is required to intelligently swap out processes and determine which process has control of the CPU at which time, while balancing different constraining factors such as providing a fast response time to the user, meeting deadlines by which processes need to have certain computation finished by, minimizing the amount of time the CPU is idle, etc. Different scheduling algorithms represent different compromises between common scheduling goals, such as (1) maximizing throughput, (2) minimize response time, (3) minimize latency, (4) maximize fairness (i.e. every process should get some time on the CPU), (5) minimize starvation (the number of processes that never get CPU access due to higher-priority processes constantly entering the ready queue and taking control instead), (6) prevent deadlock (situations in which two (or more) processes are circularly waiting on resources held by each other, e.g. process A holds resource X and is waiting on resource Y, and process B holds resource Y but is waiting on resource X, leading to system freeze), etc. There are various scheduling frameworks and algorithms implemented by different operating systems, but because the kernel is not a constantly running block of code on its own processes, and is required to be run periodically to continue managing the entire system, most operating systems incorporate a form of preemptive time slicing; that is, some periodic fixed interval of time is determined (typically between 10 and 100 milliseconds) called the time slice, and every time slice, the running process is automatically interrupted and switched into kernel mode to allow the kernel to perform its periodic check-up on the system and execute any actions it needs to that weren't initiated by an interrupt.

The scheduling of processes can, at a high-level, into three components, which delineate the different time scales the scheduler operates on.

1. Long-term scheduler
The OS maintains a queue of processes waiting to use the CPU, called the ready queue. Meanwhile, other processes that are blocked on synchronization primitives (such as locks), I/O operations, or other operations that run in the background on which a process must wait before resuming computation, exist in various queues tracking blocked processes. There exists a long-term scheduler whose purpose is to determine, periodically on larger time scales, when to swap processes out of a blocked queue and into the ready queue. In so doing, it also has control over the size of the ready queue, thereby setting guidelines for the high-level degree of overall concurrency, and is in charge of providing a good balance between I/O-bound and CPU-bound processes (generally speaking, processes are either undergoing heavy computation and are thus CPU-bound, or are blocked on prolonged I/O access of some kind, such as hitting the file system, a database, a network socket, etc., and are thus I/O-bound; CPU-bound processes require use of the CPU whereas I/O-bound processes are primarily waiting and only use the CPU in short bursts).

2. Short-term scheduler
The short-term scheduler handles the intricacies of determining, at each process swap at the CPU, which process on the ready queue is to be swapped into the CPU next. The long-term scheduler is in charge of determining when to perform these process switches, but the short-term scheduler is in charge of determining which process actually runs next. As such, the short-term scheduler is also invoked at process switches that are invoked by other channels than the long-term scheduler, such as an interrupt or system call. When deciding which process from the ready queue to swap into the CPU, the short-term scheduler must account for various factors such as the processes' priorities (which are heuristic measures of the importance of a process), processes' urgency, the length of processes total running time, etc. At the minimum, the short-term scheduler runs every time slice, though typically runs more frequently depending on interrupts and the behavior of the long-term scheduler.

3. Dispatcher
The final component involved with scheduling is called the dispatcher; it isn't actually involved in the nature of the implemented scheduling algorithm, but rather is a module used to actually perform the context switches. Whenever processes are switched out, the current process' context must be popped out of the registers and CPU and stored in a block in memory somewhere, after which the new process' execution context (consisting of register values, memory space, etc.) is switched in (meaning the CPU is given access and can continue running the process' code from where it last left off). The dispatcher is what actually handles this context switch. It also handles the transition from kernel mode to user mode and back, which involves a similar context switch but between a user thread and a kernel thread (and is therefore a more heavyweight and lower-level context switch).

Scheduling algorithms

1. First come, first served
Under this paradigm, the ready queue is essentially a chronologically ordered queue in which the process that arrived first is executed next, and process switches only occur when the currently running process terminates. This is the simplest scheduling algorithm, and is sub-optimal for throughput (since long processes might block many short ones, which would ideally be run first to increase the ratio of processes that have been completed to time). The algorithm does guarantee no process starvation, since every process is guaranteed to be run at some point.

2. Earliest deadline first
This paradigm is designed to strictly enforce process' meeting real-world deadlines, and thus is useful in systems in which meeting deadlines imposed by the real world, by which certain computation must be finished, such as self-driving cars (e.g. by the time a certain stoplight is reached, the process in charge of computing whether to stop must be finished, no matter what). Specifically, all processes have associated deadlines, processes are run until they complete, and the next process to be run is the one whose deadline is earliest.

3. Shortest remaining time first
Under this paradigm, the next process to run on a process switch is the one with the least amount of time remaining in its computation. As such, it requires each process to know in advance how long its computation will take, which is often an impractical assumption in practice, forcing the system to instead rely on heuristic estimations. Because the process with the shortest amount of time remaining will always be the process with the least amount of time remaining, given that it's currently executing, process switches subsequently only occur at process completion. This process is vulnerable to starvation, in particular for very long processes which will always have a long time remaining, a factor that never changes if the process is never run. The algorithm is designed to maximize throughput.

4. Fixed-priority preemptive scheduling
Every process is assigned an unchanging priority at initiation, the processes are run directly in order of their priority. This means that if a process with a higher priority than the currently executing process enters the ready queue, the currently executing process is immediately swapped out in favor of the higher-priority process. Thus, process switches occur either at process completion or when a process with a higher priority than the currently executing process enters the ready queue (either because it was created or because it's no longer blocked). Actually determining what priority to assign a process depends on various related algorithms that account for the nature of the process.

5. Round robin scheduling
This is another very simple scheduling algorithm in which, at each time slice, processes are automatically swapped out in favor of the next process in line, where processes are ordered, like first come first serve, chronologically by when they entered the ready queue. Thus, newly created processes join the ready queue in the back, and process switches occur at every time slice (or, obviously, at process completion). This policy is optimal when all processes in the ready queue have similar running times, and is designed to optimize the average response time (since processes respond as soon as their finished, whereas in other schemes that require preempting processes before they're finished, processes are delayed from responding). The behavior of a system with this scheduling system is highly dependent on the size of the time slice in use - at very small time slices, the policy approximates shortest time remaining first, though the context switching overhead begins to consume the entire system process time, while at very long time slices, the policy essentially becomes first come first serve.

Modern scheduling algorithms are designed to be adaptive, with dynamically changing priorities and policies that change based on past system and process behavior. This exploits the fact that computer behavior is far from random, and often process behavior over time can be predicted by looking at past behavior. Thus, the OS can implement a simple priority-based scheduling algorithm, but make priorities constantly changing based on amount of CPU time used (to account for fairness), time remaining, importance of the process, whether the process is blocking higher-priority processes (i.e. priority donation), etc.