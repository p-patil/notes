NOTES - piyush patil
-Scientific method: A pool of techniques used to obtain new knowledge, investigate phenomena, and prove ideas. 
    -Method: 1. Define a question.
         2. Record observations and gather data relevant to the question.
         3. Create a hypothesis to explain the data in terms of the question.
         4. Test the hypothesis using an experiment.
            -The experiment must be reproducible. This is needed for step 8.
         5. Analyze data.
         6. Interpret data by drawing conclusions that lead to an improved hypothesis.
         7. Publish results.        
         8. Retest the hypothesis.
    -Demarcation Problem: Distinguishing non-science from science using certain criteria.   
        -Logical positivism: Only statements on empirical observations or logical axioms, theorems, or propositions can be considered scientific. 
        -Verificationism: A statement is only scientific if it can be proven true or false. Similarly, the scientific method can only be applied to questions for which a definite answer exists that can be found. 
        -Pragmatism: The effects or consequences of an idea constitute the essential criterion for determining meaning or value.
            -Intelligent practice: Using observations to form theories that are then applied to practices with a specific goal in mind.
        -Falsifiability: A proposition must be able to be proven false; it must be possible for observation to clash with the hypothesis. This is because of the asymmetry of deductive logic; one cannot use observation to determine a general law (i.e. induction) but one can use observation to disprove a general law. Trying to prove an unfalsifiable statement using observation or the scientific method is therefore unscientific and outside the realm of logic.
            -Example: "All people have ten fingers" is falsifiable because it theoretically conflicts with an observation of a person without ten fingers.
                  "Computers exist" is unfalsifiable because it cannot clash with the observation that a computer doesn't exist.
                  "It will rain here in 1 billion years" is falsifiable in principle but not in practice.
____________________________________________________________________________________________________
                    PHYSICS
-Theories of Everything (TOE):Explain and link all physical phenomena. Can, in principle, predict the outcome of any experiment with sufficient data.
    -String Theory: Electrons and quarks are not 0-dimensional but contain within then tiny 1-dimensional vibrating strings to energy that vibrate in different ways to determine
     the type of particle. Requires 10 dimensions (9+1 universe). 
        -History of String Theory:  
        1. (1943-1958): The notion of the S-Matrix arises, and in the 1960s many physicists use it as a substitution for quantum field theory, which had problems at the time. Quantum chromodynamics later solved the problems, so many physicists abandoned the S-Matrix, but S-Matrix Theory led to the development of String Theory.
        2. (1970): Yoichiro Nambu, Holger Bech Nielsen, and Leonard Susskind propose a model that represents nuclear forces as vibrating, one-dimensional strings. However, this model failed to predict observations. 
        3. (1974): John Schwarz and Joel Scherk find that the the patterns of string vibration in the theory are very similar to bosonic properties, and it was discovered that the properties exactly matched those of the graviton (hypothetical). This led to bosonic string theory (still taught today).
        4. (1984-1989): First Superstring Revolution: Scientists discovered that string theory could describe all elementary particles and interactions. It was discovered that 6 extra dimensions were necessary to obtain N = 1 supersymmetry. Five different forms of string theory sprung up, Type 1, Type 2a, Type 2b, Heterotic, and E-Eight by E-Eight string theory.
        5. (1994-2000): Second Superstring Revolution: Edward Witten discovers that a new 11-dimensional model could unite all 5 forms of string theory; he called this M-Theory and no one knows what the "M" stands for. 
    -M Theory:Extended version of string theory, but with 11 dimensions. Supposedly unifies all 5 forms of string theory. Increases coupling constant (denoted g, it determines
     the strength of an interaction), which causes more dimensions to come out.
    -Loop quantum gravity: Space is extremely fine fabric of interwoven finite quantized loops of excited gravitational fields (spin networks, and are called spin foam as a unit)
     Doesn't require higher dimensions. Quantizes gravity under quantum mechanics. Unlike string theory, does not unify forces.
    -Twistor Theory: Maps the geometric objects in Minkowski space (conventional 3+1 spacetime) into 4-dimensional spatial objects. This 4-dimensional space for 3+1 objects is called Twistor space. 
        -Twistor String Theory: Edward Witten proposed that one could unite String Theory and Twistor Theory. String vibrations in Twistor space. 
-Principle of Least Action: Dynamical systems tend towards trajectories in which "action" (the area under the curve of the graph of trajectory versus time) is minimized.
-Standard model of particle physics: A theory that describes subatomic fundamental reactions in terms of three main forces: electromagnetism, the strong nuclear force, and the weak nuclear force (notice gravity is not unified in this theory). There are twelve fundamental particles of matter, four fundamental guage bosons (not including the theoretical graviton) and a force carrier for the intrinsic property of inertia.
    -Fundamental particles: The twelve fundamental particles can be divided into six quarks, which make up composite particles called hadrons, some of which (namely, protons and neutrons) compose atomic nuclei and therefore most matter, and six leptons, tiny particles with a spin of 1/2 (particles with a spin of 1/2 are called fermions) that are NOT subject to the strong nuclear force, but instead obey the Pauli exclusion principle (which states that two identical fermions cannot occupy the same quantum state at the same time). 
        -Quarks: The six quarks are up, down, charm, strange, top, and bottom. These six quarks have varying masses, but all have a spin of 1/2. The up, charm, and top quarks have electrical charges of +2/3, while the down, strange, and bottom quarks have electrical charges of -1/3. 
            -Color charge: A new kind of "charge" is taken upon by quarks and gluons: a color charge. Analogous to electric charge, color charge describes differences between particles in relation to the strong nuclear force, rather than the electrical force. Rather than two charges as in the electrical force (positive and negative), there are three color charges (red, green, blue). Color charge is conserved as is electrical charge, and a combination of a red, a blue, and a green quark will have a net color charge of zero, and is said to be "colorless". Antiquarks have corresponding color charges of antired, antigreen, and antiblue; any color charge paired with its complement (e.g. red with antired) will also be colorless. All free particles have a color charge of zero, such as baryons (made up of three quarks) and meson (quark and antiquark). 
                -Color confinement: The principle that color charged particles cannot be isolated singularly. This means that a single quark will never be observed by itself, but only in groups of other color charged particles. The origin of this principle is unknown.
        -Leptons: The six leptons are the electron, electron-neutrino, muon, muon-neutrino, tau, and tau-neutrino. The electron, muon, and tau all have electrical charges of -1, while the electron-neutrino, muon-neutrino, and tau-neutrino have no electrical charge. All leptons have spin 1/2. The charged leptons interact with other particles to form matter, while the non-charged leptons rarely interact with any matter at all and so are rarely observed. 
        -Bosons: The four bosons are the gluon (governs strong nuclear force), the photon (governs the electromagnetic force), and the W and Z bosons (govern the weak nuclear force). The gluon and photon are massless and have no charge, while the W and Z bosons have mass. The Z boson has no electrical charge, while the W boson can have an electrical charge of either +1 or -1. All bosons have a spin of 1. Note that that bosons (particles with integer spin) are not subject to the Pauli exclusion principle. 
-General Theory of Relativity: Geometric theory of gravitation. Unifies Newton's law of universal gravitation (Every point mass attracts every other point mass with a force directly proportional to the product of their masses and inversely proportional to the square of the distance between them) and special relativity. Describes gravity as an intrinsic property of spacetime; the curvature of spacetime is directly related to the energy and momentum of whatever matter or radiation is present (Specified in Einstein field equations). 
    -Differences from classical physics: Gravitational time dilation, gravitational lensing, Einstein shift (A.K.A. gravitational redshift; light originating from a gravitational source appears redshifted from a weaker gravitational point), gravitational time delay (light takes longer to reach Point B from Point A if there is a gravitational object in the way). 
    -Not yet unified with quantum physics to produce a self-consistent theory of quantum gravity 
    -Fundamental principles: 
        -Special Theory of Relativity: Generalizes Galileo's principle of relativity (all uniform motion is relative; no absolute state of rest) Speed of light is the same for all observers, but each observer has its own inertial reference frame. 
            -Differences from classical physics: length contraction, time dilation, relativity of simultaneity (simultaneity (2 events happening at the same time) is not absolute; it depends on the observer's inertial reference frame).
            -Predicts mass-energy equivalence (E = mc^2)
        -Equivalence principle: The force experienced by an observer at rest on a massive body is the same as the force experienced by an observer in a non-inertial reference frame. 
            -Basically, Gravity = Acceleration
        -World Line: The unique path of an object as it travels through 4-dimensional spacetime.
        -Riemannian geometry: (A.K.A. elliptical geometry) Study of curved surfaces.
            -Contrasts Euclidean geometry (flat surfaces)
            -Distance between two points on a flat surface is a line; on a curved surface it is a minimal geodesic. Line: a set of points such that any two pair of points is linked by a minimal geodesic. 
            -Gauss curvature: (A.K.A. sectional curvature) tool for finding HOW much a surface is curved. To find the curvature at a given point, find the tangent plane to the curved surface at that point. If a surface only touches the tangent plane at the given point, it has positive or zero curvature at that point (spheres and paraboloids have positive curvature all around)
            -Gravitational lensing: we observe it when the Hubble telescope and a certain point have more than one minimal geodesic, due to curved 3-dimensional space.
    -Doubly Special Relativity: Alters special relativity by including an observer-independent upper bound to energy (i.e. Planck energy) and an observer-independent lower bound to measurements of length (i.e. Planck length).
-Novikov Self-consistency principle: If a time traveler wants to changes the past, he has already done so. The present is a state in which, in the past, the time traveler has ALREADY gone back and changed something, even before he makes the decision to GO BACK in this present. 
    -The probability of changing the past in a way that would cause inconsistencies is, therefore, 0.
    -Assumptions made: Either (a) there is only ONE timeline or (b) other timelines (such as those suggested by the many-worlds interpretation) are inaccessible.
    -Closed timelike curves: worldline of a particle in closed spacetime that returns to its starting point.
-Plank equation: E = hf; the energy of a photon is the plank constant times its frequency, h = 6.6E(-34). This equation gives rise to the photoelectric effect and first proposed that light was quantized.
                        PARTICLE PHYSICS
    -Boson: Obeys Bose-Einstein statistics (which predicts the distribution of bosons in an area of thermal equilibrium)
        -Cooper pair: 2 electrons (or other fermions) bound together at cold temperatures.
    -Gauge Boson: Force carrier
    -Fermion: Obeys Fermi-Dirac statistics (which predicts distribution of fermions in an area of thermal equilibrium)
        -Hadron: Particle made of quarks. Divided into two families:
            -Baryons: 3 Quarks
            -Mesons: 1 quark and an antiquark
                -Pions: pi meson; pi+,pi0,pi-. Lightest mesons, play a role in strong force.
        -Quarks, leptons
        -Exotic atoms: normal atoms but with one or more subatomic particles replaced with other particles of the same charge (e.g. electrons replaced with muons or protons replaced with positrons)
            -Protonium: proton and an anti-proton orbiting it
            -Muonic atoms: proton and orbiting muon
            -Pionic atom: proton and orbiting pion
    -Exotic matter: Matter that doesn't follow conventional laws.
        -Particles with negative mass
        -Undiscovered hypothetical particles
            -E.g. exotic baryon (3 quarks bound together, as normal, but with additional subatomic particles)
        -Uncommon states of matter (e.g. Bose-Einstein condensates and quark-gluon plasma)
            -Bose-Einstein Condensate: Gas of pure bosons is superchilled to near absolute zero. Bosons are forced into lowest quantum/energy state, and quantum effects become macroscopic.
            -Quark-Gluon plasma: when temperatures get hot enough, quarks and gluons exist in free states. About 4 trillion degrees Celsius.
        -Poorly understood states of matter (e.g. dark matter)
    -Dark matter: doesn't interact with light, weird properties
        -Hot dark matter: hypothetical form of dark matter with ultra-relativistic particles. (travel at close to the speed of light)
        -Cold dark matter: very common, slow particles.
            -WIMPs: (weakly interacting massive particles) small particles that do not interact with any force stronger than the weak force; this means that they do not emit radiation or interact with photons and that they do not interact with matter at all, since the strong force doesn't apply, have not been discovered
            -Axions: light particles with self-interaction. Have not been discovered, but solve Strong CP-Problem
            -MACHOs: (Massive compact halo objects) large condensed objects like black holes, planets, brown dwarfs, etc; usually dark and non light emitting.
        -Proton decay: Hypothetical, protons decay into lighter subatomic particles, pions and positrons. One of the few unobserved effects of GUTs.
        -Magnetic Monopole: Hypothetical particle with one magnetic pole, not two. It would have a net magnetic charge. Unobserved but predicted by most GUTs.
    -Elephant Trunks: Large interstellar pillars of gas and dust that form, usually located near O type and B type stars
    -Black hole: A region of spacetime with such a high concentration of mass in a given volume that the density of the region can be considered infinite; the region has so much gravity that not even light can escape. The membrane that separates the point of no return from the region in which it is still possible to escape the black hole (i.e. the boundary where the radius is such that the gravitional force produces an escape velocity of the speed of light) is called the event horizon. As matter spirals into the black hole, it reaches enormous speeds which cause it to have enormous friction; this enormous
             friction causes the matter to heat up and glow, producing some of the brightest objects in the universe; these objects are called quasars. 
        -Schwarzschild Radius: The distance from an object's center where if all the object's mass was compressed into a sphere of that size, the escape velocity of such a point would be the speed of light, creating a black hole.
            -Formula: r = ((2)(G)(m))/(c^2) where G is the gravitational constant, m is the mass of the object, and c is the speed of light in vacuum.
                -Formula can be derived by setting kinetic energy (required to break the gravitational force, i.e. escape velocity) equal to the potential energy of the object (due to gravity of the larger object)
        -Chandrasekhar Limit: The maximum mass of a stable white dwarf. Because white dwarves rely on electron degeneracy pressure (the electrons, disjointed from the nuclei of the atoms, repel each other and produce an outward force that balances with the gravitational self-attraction) to remain stable; once a white dwarf's mass exceeds the Chandrasekhar limit, it becomes either a neutron star of a black hole.
            -The limit = 1.44 solar masses, or 2.864E(30) kg
        -Hawking Radiation: Virtual particles erupt next to the event horizon of a black hole. When the tidal force of the black hole overcomes the threshold energy of the pair, the pair is ripped apart. Because the gravity lost work doing this, The energy that created the mass of the two particles comes from the black hole, whose mass supplies the energy (since mass and energy are the same thing). The black hole loses 2 particles' worth of mass-energy, but the falling particle adds 1 particle's worth of mass-energy, so in total, the black hole loses the mass of one particle. This causes it to "evaporate" (tiny parts of it continuously annihiLating) and emit radiation (continuous stream of positive particles).
            -Temperature of a black hole, in terms of Hawking radiation, is inversely proportional to the mass of the black hole. 
            -The entropy of a black hole based on Hawking Radiation = (0.25)(A-sub(s)) where A-sub(s) is the surface area of the black hole's event horizon measured in Planck areas.
        -No Hair Theorem: Black holes are described only by three external properties; all other properties' information disappears in the formation of a black hole.
            -The properties: mass, electric charge, angular momentum. 
            -Two black holes, even if one is made out of matter and the other antimatter, will be indistinguishable from the outside if the above three characteristics are the same. 
        -Black Hole Information Paradox: As one puts more energy (or mass, equivalently) into a region, it rises with entropy, which, from an information theoretic perspective, can be defined as the amount of uncertainty in the informational state of the system, or, in layman's terms, the complexity of the system. As one puts complex information into a black hole, the information is uniformly distributed along the surface area of the black hole and is now, according to the No Hair Theorem, described only by 3 characteristics; the system has become more simply even with the addition of more energy.
                         This means, equivalently, that the amount of entropy in the system has decreased following the addition of energy, which contradicts the second law of thermodynamics (entropy always rises or stays the same).
            -Alternate interpretation: Another way to look at the paradox is that because many physical states of a system can evolve into just one described by the No Hair Theorem, information has been destroyed, which violates quantum mechanical principles. Also, information is supposed to be forever lost in a black hole, but if black holes radiate, then information must be leaking out. 
                -Solution: As an object falls into a black hole, it disturbs the black hole's radiation field and so the information about that object could leak out of the black hole in the form of mangled fluctuations.
            -The maximum amount of information in a given region of space is equal to the surface area of a black hole with a volume equal to the given region of space; any more information would cause the volume and therefore the initial given region of space to change. This upper bound is approximately = 1.4E(69) bits / square meter.
            -Hooft-Susskind Holographic Principle: A possible solution using a string theoretic framework. It states that everything in 4 dimensional spacetime can be described in information on the surface of space in the units of one bit of information per Planck area. 
            -Another solution is that objects take an infinite amount of time to reach the singularity, which is a point at the center of the black hole, and so the black hole evaporates before any information is destroyed.
        -Proton sphere: A spherical membrane enclosing the black hole, similar in topology to the event horizon, on which protons tangent to the membrane are trapped in orbit around the black hole. 
            -Radius of the proton sphere = 1.5 r-sub(s) where r-sub(s) = Schwarzschild radius. This only applies to non rotating black holes.
        -Ergosphere: An oblate spheroid (a 3D version of an ellipse) that marks the boundary such that anything in the ergosphere must be accelerating, because an object would have to move faster than the speed of light in the opposite direction to remain in zero relative motion. 
        -An observer observing an object falling into a black hole would perceive that the object gets progressively slower and slower as it approaches the Schwarzschild radius, never approaching it.
            -This is because as the object accelerates faster and faster as it nears the black hole, the light it emits has a harder time escaping as well as a longer distance to travel as it goes to the observer; after the object hits or passes the Schwarzschild radius (which in reality, will happen) it, theoretically, wouldn't be seen anymore.
        -Proper time: (c^2)(tau)^2 = (c^2)(t^2) - x^2 where c = speed of light, t = measured times, and x = measured distances. For any two frames of reference, with different t and x values, the tau (proper time) will be equal.
            -Schwarzschild metric: In polar coordinates, (c^2)(tau)^2 = (1 - (r_s)/(r))(c^2)(t^2) - (r^2)/(1 - (r_s)/(r)) - (r^2)(d(omega)) where r_s = Schwarzschild radius, t = time.
        -Penrose process: A theoretical process for extracting energy from a rotating black hole. The rotational energy of the black hole is located outside the event horizon in the ergosphere. All the objects in the ergosphere are dragged by the rotating spacetime in the same direction as the black hole's rotation. If a lump of matter enters the ergosphere, it can split into two. The momenta of the pieces may be such that one of the pieces escapes into deep space while the other falls into the black hole. 
            -Kerr metric: One exact solution to Einstein's field equations describing the empty spacetime outside a black hole.
    -Omega: Greek letter that represents the matter density divided by matter density needed for a flat universe. About 0.3 (95% accurate), this includes dark matter. Dark energy contributes the other 70%.
        -Flat universes have a total energy density of 0, since the gravity has negative energy which cancels out the positive, leading not only to an omega value of 1 but also a flat universe. A flat universe is the only universe that can be formed from nothing. Quanum fluctuations produce the universe. The universe is flat, as experiments prove (Lawrence Krauss) in the Boomerang Experiment.
    -Quantum Mechanics: Basic principles. Branch of physics dealing with physical phenomena where the range of motion is close to the order of the plank constant. Concepts:
        -Copenhagen Interpretation: QM deals with probabilities and is not objective
        -Uncertainty: it is impossible to know simultaneously and with complete accuracy both the position and momentum of a particle
            -Heisenberg's uncertainty principle: (delta-x)(delta-p) >/= (h-bar)/2; delta-x = change in position, delta-p = change in momentum (impulse)
        -Duality: all particles exhibit both wave-like and particle-like properties
            -Matter Wave: (A.K.A. de Brogile wave) wavelength of a particle is inversely proportional to particle's momentum (faster particles have small wavelengths). Therefore, frequency of matterwaves is directly proportional to the particle's total energy (kinetic energy and rest mass)
        -Complementarity: A phenomenon can be viewed one way or another, but not both simultaneously. Duality (above) is an example.
        -Decoherence: The IRREVERSIBLE process by which a pure state (the state is in coherent superposition: one or the other) becomes a mixed state (superposition, but not necessarily coherent). http://www.quora.com/What-is-quantum-decoherence
            -Decohere (v) - Two objects are decohered if they are causally separated: one has no effect on the other and vice versa
        -Superposition: When an electron is unobserved, it exists in, theoretically, all possible states; it is in a state of superposition
        -Entanglement: Two particles interact physically and then separate. This type of interaction causes the two particles to share the same quantum state, indefinite of important factors like position, momentum, spin, etc. According to the 
        Copenhagen Interpretation, this shared state is indefinite until measured. When it is measured, the superposition breaks down and the observer gets a definite value for one of the particles in the pair (i.e. clockwise spin); this causes the entangled partner 
        to take on the correlated value (i.e. counter-clockwise spin)
        -Pauli Exclusion Principle: No two electrons may occupy the same quantum state.  
        -Non-locality: Information can transcend spatial barriers. Classically, to affect and object you have to utilize space; you can pick it up, throw something at it, blow on it, etc. On a quantum scale, particles affect other particles without doing anything through space. Particles transfer information with no tangible or measurable mechanism.
            -Nonlocal Aharonov-Bohm Effect:An electrically charged particle can be affected by an electrical field even when the value of the vector of the electromagnetic field at the particle's location is 0.
            -Nonlocal Lagrangian
        -Tunneling: particle tunnels through a barrier that it couldn't go through classically. This is due to the probability of the electron appearing on the other side of the barrier.
        -Wavefunction: a mathematical description of where a particle is. it is not like a map; it only has the probability cloud of where the particle COULD be
            -Wavefunction collapse: when you measure the particle, you have a definite value for it, so the probability cloud "collapses" and all the possible probabilities go surging to one point (determined based on the probability of that point) and that is where you measure the particle.
                -(abs(psi))^2 is a real number that describes the probability of finding a particle at a given time given its position. 


-30% of the aggregate GDP of the developed world stems from inventions based on quantum mechanics.
-Kugelblitz: concentration of light and energy in a location to the extent that the amount of energy in an enclosed location forms a black hole. the main difference between a kugelblitz and a conventional black hole is that the former is formed from energy, not mass. The light becomes self trapped and forms an event horizon.  -Basal Ganglia: a group of nuclei in the brains of vertebrates that are involved in motor control, procedural learning (i.e. habits), cognitive or emotional functions.
-Circadian Rhythm: Controlled by thalamus, biological process that maintains an entrainable (linked to the environment's chronology) oscillation, or cycle, of 24 hours.
        -Cycle: 2:00am - Deepest sleep | 4:30am - Lowest body temperature | 6:45 - Sharpest rise in blood pressure | 7:30am - Melatonin secretion ends | 8:30am - bowel movement likely | 9:00am - Highest testosterone secretion | 10:00am - High alertness | 2:30pm - Best coordination | 3:30pm - Fastest reaction time | 5:00pm - Greatest muscular strength and cardiovascular efficiency | 6:30pm - Highest blood pressure | 7:00pm - Highest body temperature
            | 11:00pm - Melatonin secretion begins | 12:30pm - Bowel movements suppressed 
        -Disruption: has negative effects (i.e. jet lag), and can lead to bipolar disorder, some sleep disorders, etc. Some drugs influence the cycles (i.e. cocaine)
    -Hermann Ebbinghaus: German psychologist who pioneered study of memory. First person to describe the learning curve. Discovered:
        -Forgetting Curve: The decline of memory retention over time. Best ways to increase strength of memory: better memory representation (i.e. mnumonic devices) and repetition based on active recall; each repetition in learning increases optimum interval needed before the next repetition. Initially, one may have to repeat within days, but later within years.
            -Equation: The curve can be roughly described by the equation R = e^(-t/S), where R = memory retention, S = relative strength of memory, t = time.
            -Strength of memory: durability of memories in the brain. The stronger a memory, the longer one retains it.
        -Spacing Effect: Animals (including humans) remember better with spaced representation (studied a few times over a long period of time) than with massed representation (studied repeatedly in a short period of time).
        -Ebbinghaus used psuedowords to test memory, as they had no prior cognitive connections (CVC trigram).
-Cold fusion: Electrolysis of a palladium rod immersed in heavy water. Produces energy, supposedly. About 70% success rate. Lost momentum in 1980s.
-HOMGARR mnemonic device for conditions of life.
 Homeostasis, Organization, Growth, Metabolize, Adaption through natural selection, reproduction (sexually or asexually), Respond to stimuli
-de Broglie wavelength: (1924) Showed particle-wave duality.
    -lambda = h/p where lambda = wavelength of a particle (from particle-wave duality), h = Planck's constant, and p = momentum (or mv, where m = mass and v = velocity)
-Davisson and Germer: Performed double-slit experiment
-Schrodinger: Developed Copenhagen interpretation.
    -Schrodinger's Equation: Derivation:
        1. E = E-sub(K) + E-sub(P), where E = total energy, E-sub(K) = kinetic energy (or 0.5mv^2), and E-sub(P) = potential energy (or u).
        2. E = (0.5)(m)(v^2) + u, by substitution.
        3. E = ((p^2)/2m) + u, where E-sub(K) = ((p^2)/2m) because 
            a. p = mv
            b. p^2 = m^2v^2
            c. 0.5mv^2 = 0.5m^2v^2(m^(-1))
            d. 0.5mv^2 = ((0.5p^2)/m)
            e. 0.5mv^2 = p^2/2m
        4. Derive the wavefunction
            a. psi = e^(i*alpha*x) = cos (alpha*x) + i*sin(alpha*x)
            b. If psi = cos (kx - wt), where k = 2pi/lamda (where lamda = wavelength)
            c. cos (kx) = cos ((2pi/lamda)*x)
            d. When lambda = x (distance = wavelength; implies that the wave is not moving), cos (kx) = 1 and so cos (kx) is a standind wave (has v=0)
            e. cos (kx-wt) is a traveling wave (v>0)    
               When kx-wt = 0, the amplitude is maximized
                x/t = w/k
                v = w/k
            f. If lamda = h/p and p = mv then k = p/h-bar (where h-bar = h/2pi)
            g. psi = e^(i(kx-wt))
        5. d(psi)/d(x) = (ik)(psi)
        6. d^2(psi)/d(x)^2 = (ik)^2(psi)
        7. If k = p/h-bar then d^2(psi)/d(x)^2 = -(p^2/(h-bar)^2)(psi)
           -(h-bar)(d^2(psi)/d(x)^2) = (p^2)(psi)
        8. If E = p^2/2m + u then (E)(psi) = ((p^2)(psi)/(2m)) + (u)(psi)
           Time Independent Schrodinger Equation (TISE): (E)(psi) = -((h-bar)^2/(2m))(d^2(psi)/d(x)^2) + (u)(psi)
        9. Derivation of Time Dependent Schrodinger Equation:
            a. E = (h-bar)(w) = (h)(f) where f = frequency
            b. psi = e^(i(kx-wt))
               d(psi)/d(t) = -(iw)(psi)
            c. (E)(psi) = (h-bar)(w)(psi)
               Multiply both sides by -(i/h-bar): -(i/h-bar)(E)(psi) = -(i/h-bar)(d^2(psi)/d(x)^2) + (U)(psi)
               (E)(psi) = (h-bar/-i)(d(psi)/d(t)) = ((h-bar)(i)(d(psi)))/d(t)
            d. Substitute into TISE: ((h-bar)(i)(d(psi)))/d(t) = -(h-bar)^2/2m)(d^2(psi)/d(x)^2) + (u)(psi)
    -The equation: (i)(h-bar)(d/dt(psi)) = -((h-bar)^2/(2m))(del)^2(psi) + (V)(psi); the right side of the equation is the Hamiltonian, or the total energy of the system (kinetic plus potential); the Laplacian (del operater squared) allows this to be translated into a vector field in 3 dimensions. The left side of the equation gives the energy of a particle in the system at any given time in terms of its wavefunction.
        -Kinetic energy: The term on the right side representing kinetic energy is -((h-bar)^2/(2m))(del)^2(psi). In quantum mechanics, the momentum of a particle is given by p = -(i)(h-bar)(d/dx); x = position. Classically, E = (0.5)(m)(v^2) and because p = (m)(v), E = (p^2)/(2m). Therefore, E = -(h-bar)^2/(2m)(d/dx)^2; the del operator is added to translate into 3 dimensions.
        -Left side term: In quantum mechanics, the wavefunction is defined thusly: psi(x, t) = (A)(e^((i)(kx-wt))); A = amplitude, x = position, w = wave number = (f)(2pi). Differentiating the wavefunction with respect to time produces d(psi)/dt = -(i)(w)(psi). Multiplying both sides by (i)(h-bar) produces (i)(h-bar)(d(psi)/dt) = (h-bar)(w)(psi). Because E = hf and f = w/(2pi), E = (h-bar)(w). d(psi)/dt = -(i)(w)(psi) and so the term we have by differentiating the wavefunction (i)(h-bar)(d(psi)/dt) = (h-bar)(w)(psi) = (E)(psi). This is the left side of the equation.
        -To express it simply: H(psi) = (i)(h-bar)(d/dt(psi)) where H is the Hamiltonian. 
-Laplacian: (A.K.A. Laplace operator) A differential operator, written as a nabla squared, is the del operator operated on itself for some function; therefore, (del)^2 f(x) = del (dot) (del*f(x)) = divergence of the gradient of f(x). 
    -Interpretation: A uniform field is constant everywhere, but few fields are truly uniform in practice. To describe how close a field is to uniform, one can discuss how much some point on the field, which has some field value, deviates from the average value of the field around it; if each point on the field is exactly equal to the average of the points around it, then the field is perfectly uniform, and if each point on the field is close to the average of the points around it, then the field is close to uniform (though not quite). 
        -Note that the average value of the field around a point is the average value of the point's neighborhood, and is the average value of all points on the surface of a sphere centered at the point in question with some radius (the average value can be found by: Avg = (total value) / (number of points) = (total field value on surface) / (surface area) = (surface integral over the sphere, centered at the point with some radius, of the field) / (4*pi*(radius)^2) )
        -The Laplacian measures the rate at which the average value of points in the neighborhood of a point P (in space) differs from the field value at the point P, which is f(P), as the radius of the sphere increases. For a truly uniform field, the sphere can have any arbitrarily large radius, but if the field is uniform everywhere, every point is the same and the difference will always be 0. For a nearly uniform field, as the points about the point P become further and further away from P, their average value differs more and more from P, but the difference is always small. How fast the rate of this difference changes is measured by the Laplacian.
        -Laplace's equation: Since the Laplacian is proportional to the difference in the average value of neighboring points and the value of the center point itself, a truly uniform field always exhibits behavior such that (del)^2 (psi) = 0. This is Laplace's equation.
            -For small radii a, (del)^2 (psi) is approximately equal to (6/(r^2))(psi(P) - psi_(avg)) where psi_(avg) = average value of points neighboring P.
        -The second derivative of psi with respect to time, the acceleration of the field's values, is proportional to the Laplacian by the wave speed squared. 
            -Intuitively, how fast the rate at which field values on a field change is proportional to the operator describing how much a point differs from the average of neighboring points. As the discrepancy in a point's value and the point's neighborhood's value goes up (meaning the field is getting less uniform), the acceleration of the field's values also goes up; the acceleration refers to how fast the speed of field-value-change is changing.
-Einstein-Podolsky-Rosen Paradox: (A.K.A. EPR Paradox, 1935-1936) Attempts to show shortcomings in quantum mechanics. If two particles are pair produced, they are entangled. This means that if, in two separated locations, the spin of one particle in the y-dimension is "up" then the spin of the other particle in the y-dimension instantaneously is known to be "down" (because the two particles share a wavefunction). This also means that if you measure the spin of one entangled particle in the y-dimension, you cannot measure the spin of the other entangled particle in the x-dimension, because doing so would give you the spin in both dimensions, which    
 violates the Heisenberg Uncertainty Principle. This implies that information is traveling faster than the speed of light, vioLating special relativity, and quantum mechanics must therefore be revised.
    -Solutions: 1. The particles DO communicate instantaneously and special relativity is wrong.        
            2.  At the particles' moment of creation, they are endowed with information within them known as hidden variables. This requires a huge amount
                -Hidden Variable Theory: (***INCOMPLETE***) 
            3. All particles already have the information of all other particles in the universe from the Pauli Exclusion Principle. 
-Information: any event that affects the state of a dynamical system. That which can be used to distinguish one thing from another.
    -Quantum information: physical information held within a quantum state of a quantum system. its unit is the quantum bit, or qubit, analogous to the bit.

-Scalar: Simple physical quantity unchanged by transformations (e.g. Newtonian translations, Lorentz transformations, etc.). It is thus described as a single number.
 Vector: (Euclidean vector) A physical quantity with a scalar magnitude but also with a direction in Euclidean space. Has 3 components, each with one basis vector (vector parallel to an axis of the Cartesian coordinate plane and representing a particular component of the vector)
    -Vector field: Euclidean space with a vector assigned to each point to represent not only the magnitude but also the direction. This is useful in describing a field's strength throughout space. 
 Tensor: Arrays of numbers or functions that represent physical phenomena or objects and can transform based on coordinate transformations. Tensors are characterized by ranks, with each rank referring to the dimension of the tensor (a rank 0 tensor is a scalar; a rank 1 tensor is a vector, etc.) In 2 coordinate systems, a tensor exists to mediate between 2 vectors; when 2 vectors share a coordinate system, each of their corresponding components (x, y, z) are parallel and can be manipulated in such corresponding pairs, x components with x components and so on. In different coordinate systems, each component of a vector (1 per dimension) acts simultaneously on all 3 components of another vector from another coordinate system, so the (rank 2) tensor that describes it, if both coordinate systems are 3-dimensional, is a matrix with 3^2 = 9 components.
    -Rank 2 Tensors: Matrices. Two dimensional arrays of numbers of functions defining quantities at points in n-dimensional space. They describe more complex quantities than simply direction and magnitude, such as torque. While a vector has 3 components, a tensor has 9, each with 2 basis vectors, i.e. each component describes two quantities of the tensor, not one (as with vectors), (e.g. In a solid object, describing the location of the area vector (vector with magnitude proportional to an area and perpendicular to that area) and the force acting on it).
    -Rank 3 Tensors: Describe yet more complex phenomena and have a 4 dimensional array. 
        -E.g. Riemann curvature tensor: describes the curvature of spacetime in Einstein's theory of relativity. Since spacetime is 4 dimensional, each component of the 4 dimensional array has 4 quantities for a total of 256 quantities. 
    -Tensors describe bosons.
 Spinors: A two component vector matrix with one column and complex components. 
    -Spinors describe fermions and are used in the Dirac equation.
        -Dirac equation: (***INCOMPLETE***)
    -A 360 degree rotation produces a spinor's negative, and so a 720 degree rotation produces the spinor itself. 
-Evidence and problems of the big bang: 
    -Evidence: 
        -Four Pillars of the Big Bang:
        1. Hubble-type expansion, accelerated rates of galactic movement away from earth. Proven by redshift. 
        2. Cosmic background radiation
        3. Relative abundance of light elements like hydrogen and helium; the big bang theory predicts and explains this phenomenon. Using the big bang model, it is possible to calculate the ratios of the elements, and they fit with data.
        4. Large scale distribution (superclusters and filaments) and evolution of galaxies (distant galaxies are at the exact stage in galactic evolution as predicted by the big bang model, as are quasars. Galaxies at similar differences but that formed either recently or shortly after the big bang have vast differences as well).
        -Primordial gas clouds: these primordial gas clouds that are predicted to occur in the first few minutes of the big bang have been observed in 2011, and its composition matches theoretical expectations. 
    -Problems:
        -Flatness: The universe may have positive, zero, or negative curvature. Shortly after the big bang, just at a few minutes, the energy density must have been something like 10^14 times the critical density. There is no explanation, then, for how the universe has not yet suffered a heat death or a big crunch.
            -Solution: The universe inflated so rapidly in the beginning (inflatons) that the energy density was very close to critical and so spacetime was "smoothed out".

-Rate of universe's expansion: 46.2 miles (74.3 km) (plus or minus 1.3 miles) per second per megaparsec, meaning that as a galaxy increases in distance by megaparsecs, the rate of expansion accelerates by 42.6 miles per second (e.g. a galaxy 1 megaparsec away moves away from us at a rate of 42.6 miles per second, and a galaxy 2 megaparsecs away moves away from us at a rate of 85.2 miles per megaparsec, and so on)
    -Alternatively, the formula V = H_0 * D can give you the velocity of recession for a galaxy D megaparsecs away (using proper distance), where H_0 is Hubble's constant, given above as 42.6 miles per second per megaparsec.
    -Note that because galaxies are gravitationally attracted to each other, they have a relative velocity (i.e. a peculiar velocity in this context) that needs to be compensated for in Hubble's Law, which gives the rate of expansion of the universe at a distance.
        -A derivation of Hubble's Law is given below in the section "-Friedmann-Robertson-Walker metric" subsection "-Derivation", number 1-5.
        -Finger of God effect: Inaccurate distance measurements at large distances due to the elongation of very distant galaxies in redshift space, caused by the galaxies in a cluster to have peculiar velocities.
    -Parsec = 3.26 lightyears; megaparsec = about 3 million light years.
-Friedmann-Robertson-Walker metric:  The exact solution to Einstein's field equations of general relativity that gives the properties of the rate of expansion of the universe. 
    -The formula is (a-dot)^2/(a(t))^2 = 8pi/3(G)(rho(t)) - C/(a(t))^2
            where a-dot = the time derivative of the scale factor, a(t) = scale factor, G = fundamental gravitational constant = 6.67E(-11), rho(t) is the density of a sphere with radius of initial distance between two galaxies and center at one of the galaxies (note: the density is a function of time because it is changing as the volume changes, which changes as the radius changes), and C is a constant.
    -Derivation: 1. The proper distance between two galaxies is given by the scale factor a(t), which relates changing distance with time, multiplied by the distance at the initial time t_(0), which is usually set to the age of the universe. The derivation assumes that the universe is homogeneous and isotropic.
            D(t) = (a(t))(d_(0))
             2. Differentiating both sides with respect to time, where the dot represents the time derivative, gives
            dD/dt = (d_(0))(a-dot)
             3. Because the change in proper distance with respect to a change in time is defined to be velocity,
            v = (d_(0))(a-dot)
             4. Multiplying the right side of the equation by a(t)/a(t), which is just 1, gives
            v = (d_(0))(a(t))(a-dot)(1/a(t))
            and because D(t) = (d_(0))(a(t)), from (1), it follows that
            v = D((a-dot)/a(t))
             5. Because (a-dot)/a(t) is defined as the Hubble constant, H, it follows that Hubble's law can be derived:
            v = DH
             6. Both galaxies of a pair of galaxies for which the function of distance is concerned are affected by all the mass within a sphere centered at the other galaxy and with radius of the initial distance d_(0). Although the mass of the galaxies outside the sphere also affects the system, the attraction cancels itself out when the universe is assumed homogeneous and isotropic. By Newton's law of gravitation:
            F = (G)(M)(m)(1/D^2) where M = mass of the sphere and m = mass of the galaxy
            F = (G)(4pi/3(D^3))(m)(1/D^2) = (G)(4pi/3)(D)(m)
             7. The potential energy is given by
            E_(p) = -G(M)(m)(1/D)
             8. The total energy is a constant at all times, by the laws of thermodynamics, and is given by the sum of the kinetic energy and the potential energy:
            E = 0.5mv^2 - G(M)(m)(1/D) = C
            Multiplying the entire equation by 2 and then dividing by m simplifies the equation to
            v^2 - (G)(M)(1/D) = C(2/m) = C
             9. From (1) and (3), it follows that:
            ((d_(0)(a-dot))^2 - (2GM/(d_(0)(a-dot)) = C
             10. m = (Volume of sphere)(Density), where the sphere refers to (6) and where density can be denoted by rho(t), because it is changing with time.
             m = (4pi/3(D^3))(rho(t))
             11. Substituting m by the term given in (10) and substituting D by the term given in (1) gives
             ((d_(0)(a-dot))^2 - (2G/((d_(0)(a-dot)))(4pi/3((d_(0)(a-dot))^3)(rho(t)) = C
             The (d_(0)(a-dot) can be canceled and the (d_(0))^2 can be factored out, which simplifies the equation in (10) to:
             (d_(0))^2((a-dot)^2 - (2G)(4pi/3(a-dot)^2)(rho(t)) = C
                 Because (d_(0))^2 is a constant, dividing both sides by (d_(0))^2 gives:
             (a-dot)^2 - (G)(8pi/3(a-dot)^2)(rho(t)) = C
             12. IsoLating (a-dot)^2 and dividing both sides of the equation by (a(t))^2 gives the finalized formula:
             (a-dot)^2/(a(t))^2 = H^2 = 8pi/3(G)(rho(t)) - C/(a(t))^2 where H = Hubble's constant
    -Implications: Open universe: If C/((a(t))^2 is positive, then the universe's rate of expansion accelerates forever, and the universe perpetually expands. 
               Closed universe: If C/((a(t))^2 is negative, then the universe's rate of expansion will eventually reverse, causing the universe to contract.
               Flat universe: If C/((a(t))^2 = 0, then the universe's rate of expansion approaches 0 (i.e. a perfectly static universe) as time approaches infinity.
                -This implies that the universe's rate of expansion would asymptotically approach a static universe, but would never get there
-Zero Point Energy: (A.K.A. ZPE) The energy of the non-gravitational fundamental forces at their lowest quantized energy level, as a quantum mechanical system (i.e. temperature = 0, hence zero point (the point at which T = 0)). The energy that remains when all other energy is removed from a system (e.g. As the temperature of helium approaches 0 K the helium remains a liquid, not a solid, due to ZPE; it will convert to a solid if the pressure is increased to 25 atm).
    -ZPE exists because of the uncertainty principle. The total mechanical energy in a system, E, is given by E = (1/2)(m)(v^2) + V(x), where the first term is the kinetic energy and the second is the potential energy, as a function of position. To find the energy of the ground state, minimize E. This means find the point in phase space where v = 0 and the position is such that V(x) is minimized (i.e. d/dx( V(x) ) = -F(x) = 0, since force is the negative space derivative of potential energy). However, this implies that this minimizing position is known, as well as the minimizing velocity, which violates the uncertainty principle. Thus, even at the minimum point of the total energy E, there is some uncertainty in the value of v and x, which are close to 0 but not quite. This leaves a tiny amount of energy unaccounted for even in the lowest, ground state; this energy is ZPE.
    -Example: Quantum harmonic oscillator: Vibrations of a diatomic molecule. They have potential energy proportional to the squared displacement from equilibrium. 
        -The energy levels of the system are quantized and is given by E_(N) = (n+0.5)(h-bar)(frequency); the frequency is described below.
            -The frequency for a diatomic molecule is given by omega = sqrt(k/m_(r)) where k = bond force constant and m_(r) = reduced mass = ((m1)(m2)/(m1+m2))
                -Bond force constant: Law for materials that obey Hooke's law (i.e. Hookean materials, linear-elastic materials) that states that the extension of a spring is proportional to the load. F = -kx where F = restoring force, x = displacement from equilibrium, and k = spring constant (or force constant)
            -Quantum harmonic oscillators will never be brought to rest due to ZPE = 0.5hf
        -Harmonic oscillator: Returns to its initial state after being displaced, because of a restoring force. If that restoring force is the only force, then the system is a simple harmonic oscillator.
 The non-gravitational fundamental force fields follow this principle. For example, electromagnetic radiation is disturbances in the electromagnetic field, with each wave characterized by frequency (and by extension energy), direction, and polarization. Each wave is one mode, and each mode is subject to Heisenberg's uncertainty principle, and so each mode has at least 0.5hf for its energy.
 The spectral energy density = density of modes * energy per mode and because the density of the modes = (sum of all interval's frequencies)^3, (very small energy per mode)(very large spatial density of mode) = very high ZPE energy density per cubic centimeter. 
 All of space is filled with zero point energy fluctuations in the omnipresent zero point field. Zero point fields are Lorentz invariant. When accelerating in a zero point field, the subject feels the force of a thermal spectrum on it, with apparent temperature (not true temperature, which isn't even involved) proportional to acceleration.
    -Casimir effect: Two metal plates, placed in a vacuum at micrometers apart, will tend to stick to each other due to zero point energy. The distance between the two plates is so small that wavelengths of electromagnetic radiation of that size is inhibited, and at such small distances such wavelengths may have significant frequencies, and so those modes of those omitted waves do not balance the external zero point energy. There is thus a zero point external pressure that pushes and keeps the plates together.
        -Alternate explanation: Virtual particles are constantly popping into existence only to annihilate moments later. When two plates are placed very close together, certain virtual particles cannot "fit" between the plates, since their associated wavelengths (since particles also behave as waves and therefore have associated wavelengths) exceed the distance between the plates, and so there are, on average, more virtual particles outside the plates than inside. This creates a net inward pressure that pushes the plates together. At this scale, the smoothness and molecular nature of the plates is a significant factor.
        -Widely regarded as hard evidence for zero point energy.
    -May explain dark energy (it has a radiation to pressure ratio of w = -1 (normal radiation has a radiation to pressure ratio of plus/minus 1/(3c^2)), and the negative nature gives ZPE negative (repulsive) curvature). However, the predicted amount of ZPE (assuming the nature of ZPE continues up to the Planck frequency) is 120 orders of magnitude greater than predicted (if it were legit, the universe would be expanded into oblivion within microseconds via the Big Rip).]

-Entropy: 
    -Shannon entropy: In an information theoretic context, the concept of entropy is a property of discrete random variables, and is related to the information content of the variable. In particular, the entropy of a random variable is the expected (ie average) amount of information produced by the random variable. By "produced" what we mean is that if we hold consecutive trials of the random variable, we'll glean more and more information about its underlying distribution, depending on what values we observe; we can think of each trial as putting us in some state, depending on the value of the random variable, and each subsequent trial as moving us to a different state. The amount of information conveyed in a trial of the random variable, then, is related to how surprising we find it to move into the new state given by the random variable given our current state. This idea of "surprise" of an event is inversely related to how likely the event is. Given an event E, we expect it to take 1 / Pr(E) trials before we observe E, so this quantity is proportional to the surprise. If we do observe E, then the amount of information we receive is the proportional to the length of the message encoding the surprise of the event, which is given by log(surprise) = log(1 / Pr(E)). The entropy, then, is simply the expected value of the information content, giving the formula
        entropy = sum over probabilities p_i of (p_i * log(1 / p_i))
    Another way to think about this is in terms of functional equations; we have a vague concept of the information content of an event, and given our intuition about its relationship to surprise, we can impose some constraints that this concept should satisfy:
        1. It ought to be continuous, since similarly probabilistic events should have similar information contents.
        2. If the random variable is in fact deterministic, ie there exists some n such that Pr(X = n) = 1 for random variable X, so that the variable always takes the same value, then the information content should be zero. This is simply because if the random variable is deterministic, we never change states, and hence gain zero new information from subsequent trials of the variable.
        3. The entropy should be maximized if the variable has the uniform distribution, so that p_0 = p_1 = ... = p_i = ... for every probability p_i, since there's no way to predict the next state of the variable better than a random guess, so each trial gives us maximal information.
        4. It ought to be additive, since we expect surprise to be additive, for two joint events.
    The function log(1 / p_i) is the only function (technically, family of functions) satisfying these conditions. Note that throughout the above explanation, "log" refers to the logarithm with base 2.
-Lorentz invariance: The property of being the same in any Lorentz frame (Any frame within inertial coordinate systems, with 3 spatial coordinates and 1 time. Each frame is in uniform motion with respect to the others and any 2 frames have the same interval) and being unchanged by a Lorentz transformation.
    -Lorentz Transformation: A mathematical way of converting two observers' differing frames of reference to each others'. 
    -Inertial frame of reference: A frame of reference that describes spacetime homogeneously and isotropically.
    -Lorentz contraction: As the magnitude of an object's velocity with respect to an observer in an inertial frame of reference approaches the speed of light, the apparent length of the object with respect to its proper length decreases by an amount described by the equation
                  L = L-sub(0) / Lorentz factor where L is the apparent length and L-sub(0) is the proper length.
    -Lorentz factor: (a.k.a. Lorentz term) A constant in the derivation of Lorentz transformations. The equation:
        -Lorentz factor = 1/(sqrt(1-(v^2/c^2))) where v is the relative velocity between inertial reference frames and c is the speed of light.
        -The Lorentz factor = dt/d(tau) where t = time, tau = proper time
        -The Lorentz factor is used in:
            -Lorentz transformation: The coordinates of inertial reference frame A to inertial reference frame B (x,y,z,t) to (x',y',z',t') where v is the relative velocity between the inertial reference frames is given by
                         x' = (Lorentz factor)(x-vt)
                         t' = (Lorentz factor)(x/c^2)) where c is the speed of light.
            -Time dilation: The change in time over an inertial frame transformed into the change in time over an inertial frame with a relative velocity, v, is given by the equation:
                    Delta-t' = (Lorentz factor)(delta-t)
                -This implies that a moving observer experiences a longer clock than a stationary one.
            -Lorentz contraction, mentioned above.
            -Relativistic mass: The mass of an object moving at a velocity, v, between inertial frames of reference, where m-sub(0) is the rest mass is given by the equation:
                        m = (Lorentz factor)(m-sub(0))
            -Relativistic momentum: The momentum given by an object with relativistic mass with respect to an inertial frame of reference where m-sub(0) is the rest mass and v is the relative velocity is given by:
                        p' = (Lorentz factor)(m-sub(0))(v)
-Free Energy: Amount of internal energy of a system that can be used to do work. 
    -Gibbs free energy: A thermodynamic potential (a way of describing the thermodynamic state (set of values and properties of a state needed to reproduce that system) of a system; scalar function) measuring the amount of energy that can be converted to work in an isothermal (constant temperature) and isobaric (constant pressure) system. 
        -Described by Gibbs as the total amount of energy that can be converted to work without disturbing the temperature or pressure of the system.
        -Represented as delta-G
    -Helmholtz free energy: Thermodynamic potential measuring the amount of internal energy that can be converted to work without disturbing the temperature of volume of a system. Minimized at an equilibrium. 
        -Represented by "A" or "F"
        -A = U - TS where U = internal energy in joules, T = absolute temperature in Kelvin, and S = entropy in joules/Kelvin
    -Legendre transformation: (***INCOMPLETE***)
-Oh-My-God Particle: On October 15, 1991, an ultra-relativistic proton was detected with a speed of 0.9999999999999999999999951c and an energy 40 million times that of the energy of the fastest proton ever induced by humans. The proton is only 1.5 femtometers/second behind the speed of light.
-Relativistic aberration of light: Because the speed of light is finite, the apparent and true positions of a very distant object with respect to an observer vary if either the observer or the object is moving with a velocity perpendicular to the light, since the light projected by the star at Position A, Time A is not accurately where the star was at Time A, because the star has moved in the time it took for the light to reach the observer.
-Schwarzschild Radius: The distance from an object's center where if all the object's mass was compressed into a sphere of that size, the escape velocity of such a point would be the speed of light, creating a black hole.
    -Formula: r = ((2)(G)(m))/(c^2) where G is the gravitational constant, m is the mass of the object, and c is the speed of light in vacuum.
-Time line of Big Bang (very early/very late):
    -Planck Epoch: All four forces unified into one
    -Grand Unification Epoch: Gravity separates, magnetic monopoles (hypothetical) formed
    -Electroweak Epoch: Strong Force separates
    -Inflationary Epoch: Era of accelerating expansion in a field (unproven) called the inflaton 
    -Baryogenesis: Matter and antimatter annihilation in Sakarhov conditions
    -Big Rip: Dark energy increases without limit (called phantom energy). When the energy density (amount of energy per square unit of space) increases to the level that all atoms break apart (explode). 100+ billion years
    -Heat Death: Thermodynamic free energy is all gone. Maximum entropy. 10^150+ years
-Distant future of universe:
    -Big freeze: (10^14 years) Stars burn out, black holes evaporate, galaxies decondense
    -Big crunch: (100+ billion years)(note: probably not going to happen, if it does, gives rise to bouncing inflationary theory, and serial megaverse)
    -Big rip: (20+ billion years)(note: may not happen) Universe expansion gets to a rate such that all matter is torn apart
    -Vacuum metastability event: if the universe is in a very long-lived false vacuum (metastable (stable for a certain period of time, but not the most stable) sector of space that appears to be a vacuum but isn't due to instanton (pseudo-particles) effects that cause it to tunnel to a lower energy state) a small region of the universe could tunnel into a lower energy state, destroying all structures instantaneously and expanding at near light speeds. All systems naturally evolve towards their ground state (ie the state of lowest total energy), and it is possible that the current state of the universe is not the ground state. The ground state would be a true vacuum, an area of space completely devoid of anything (energy, matter, forces, etc.). The theory is that the current state of the universe is a false vacuum, meaning it is a point of low energy, but not the lowest, and small changes in the state of the universe might cause the metastable (temporarily but not truly stable) state to collapse into the true ground state. Mathematically, on a graph of energy versus the universe's state, a false vacuum would be a local minimum, whereas the true ground state is the global minimum.
    -Heat death: (10^150 years)
-Timeline of the Early Universe:
    -Supersymmetry breaking: At an energy level of at least 1 TeV (electroweak symmetry scale) masses of partners and superpartners are no longer equal, explaining why we haven't found any superpartners yet.
    -Quark epoch: 10^-12 to 10^-6 s: forces split into four, all fundamental particles acquire mass via the Higgs mechanism 
    -Hadron epoch: 10^-6 to 1 s: Quark-Gluon plasma cools into hadrons. Baryons like protons and neutrons can form. 
    -Lepton epoch: 1 to 10 s: Most hadrons and anti-hadrons annihilate, and so leptons and anti-leptons dominate. At 10 seconds the temperature falls to a point in which anti-leptons and leptons are no longer created; most leptons and anti-leptons annihilate, leaving a small amount of leptons.
    -Photon epoch: 10s to 380K yrs: The energy in the universe is mostly photons, which dominate the universe. They frequently interact with protons and electrons.
        -Nucleosynthesis: 3 - 20 min: The temperature falls to a point where atomic nuclei can form. Nuclear fusion: protons and neutrons combine into atomic nuclei. Protons + Neutrons -> deuterium; deuterium + deuterium -> helium-4. There is now three times as much hydrogen as helium, and only trace amounts of other elements, maybe a little Lithium and possibly Boron.
        -Matter Domination: 70,000 yrs: Densities of relativistic matter (atomic nuclei) and relativistic radiation (photons) are equal. Jeans length (determines the smallest possible structures that can form based on pressure vs gravity) begins to fall and matter grows. Cold dark matter dominates, setting the stage for gravitational clusters and collapses in inhomogenities that occurred.
        -Recombination: 377K - 150M yrs: Hydrogen protons and helium nuclei are ionized; there are no electrons. They soon turn into atoms as they acquire electrons and deionization occurs. At the end of recombination, most protons are bound in neutral atoms. 
            -Decoupling Event: Now, the mean free path (Average distance traveled between collisions) for photons becomes infinite. These photons present in the Decoupling are the same photons of the cosmic background radiation we see today.
    -Dark Ages: 150 - 800M yrs: Prior to Decoupling, Photon-Baryon fluid makes the universe "foggy". All baryonic matter is ionized plasma. As they gained electrons in Recombination, they released photons, helping along the Decoupling and creating a transparent universe. 
    -Structure Formation:
        -Reionization: 150M - 1B yrs: The first stars and quasars form (courtesy of gravitational collapse). Intense radiation reionizes the universe. Most of the universe (from this point onwards) is plasma.
        -Formation of Stars: The first stars form and begin nuclear fusion. 
-Nucleocosmology (a.k.a. cosmochronology): new technique used to determine timescales of astrophysical objects by using abundances of heavy elements, similar to geochronous dating. Already successfully determined age of Sun.
-Oxygenated perfluorocarbon: Breathable liquid. Properties: transparent, more viscous than water
-Black Swans: unlikely or rare events that defeat the purpose of a thought experiment. although deemed improbable, they have a huge impact.
-ARIS survey: 93% of scientists are atheists, atheists make up 10% of America
-Noosphere: sphere of human thought, comes after geosphere and biosphere
-DNS Cache Poisoning: (a.k.a. DNS Spoofing) redirecting a DNS request from a certain domain name to a false IP address that links to a malicious website. Would increase if SOPA and PIPA were passed,
 as more people would use untrustworthy DNS servers. EX: typing "www.google.com" redirects to a virus infecting website with a false IP address
-Types of Computing
    -Quantum: Uses the quantum states of subatomic particles instead of binary bits
    -Optical: (a.k.a. photonic computing) Uses photons produced by lasers to replace electrons. Current computers use electrons' movement in-and-out of transistors to do logic.
    -Organic: Uses complex and intelligent systems in the environment to process information. The systems are aware of their environment and communicate freely.
            -Properties: 
        -Self-organization
        -Self-healing
                -Self-protection
                -Self-configuration
-Planck Units: (A.K.A. Natural units) physical units of measurement in that the five universal constants take on the value of 1 when expressed in terms of Planck units. Plank's constant, h, is also the smallest degree of uncertainty allowed by the Heisenberg Uncertainty principle of position times momentum.
    -Universal Constants that normalize to 1 by Planck units: 
        1. Gravitational constant: (G) about 1.2E -4, where F = G( (m1)(m2) / (r^2) ) where:
            F = force between two objects, m1 = object 1's mass, m2 = object 2's mass, r = distance between objects
        2. Reduced Planck's Constant: (h, Plank's constant divided by 2pi (see Planck's constant below)
        3. Speed of light in a vacuum: (c) 299,792,458 m/s = 671,000,000 mph; Used in mass-energy equivalence
            -Mass-energy Equivalence: 1. Kinetic energy of an object: E-sub k = ( (m0)(c^2) ) / (sqrt(1 - (v^2)/(c^2)) - (m0)(c^2) )
    -Planck Temperature: (a.k.a. absolute hot) 1.41E32 Kelvin. At this temperature, the wavelength of the energy emitted from the energy source is Planck length and therefore cannot get any smaller; conversely, the temperature cannot get any hotter.
        -Theoretically, infinite amounts of energy can be added to a system, but after Planck temperature our theories break down (due to a lack of theory of quantum gravity) and so we do not know what would happen.
            -Particle energy is so high that the gravitational force is as strong as the other fundamental forces (requires a TOE).
        -At 4 trillion degrees Celsius, quark-gluon plasma is formed; string theory allows for matter in this state to reach temperatures higher than the conventional limit of 1E30 Kelvin.
-Light Cone (a.k.a. Null Cone): the path that a flash of light (originated at a single point in space and single moment in time) WOULD take through spacetime. If space were a
 2D Plane and time was a vertical axis, the event E would have a future light cone and a past light cone emanating from that point, with E being the points of the two "reflected" cones, and the cones being on top (future) and bottom (past) of the plane. In real life, space is 3D and time is a line in the fourth dimension, with the cones being
 4D cones with spheres at cross sections (just as 3D cones have circles at cross sections).
    -All events within the future light cone can be affected by E, and all events in the past light cone can affect E (both are given the respective amount of time and space on the graph.
-Negentropy: (of a living system) is the entropy it exports to keep its own entropy low. Lies at the intersection of entropy and life. 
    -Things go from chaos to order. Something that reduces entropy (i.e. the Sun is the dominant source of negentropy for life on earth).
    -Equal to free energy (amount of work a thermodynamic system can perform).
-Coulomb's Law: (A.K.A. Coulomb's Inverse-Square Law) The force of attraction or repulsion between two point charges is directly proportional to the product of the magnitudes of the two charges and inversely proportional to the distance between the points squared.
    -Coulomb's Constant (A.K.A. electric constant, electrostatic constant)(subscript "e") k-sub(e)  = (4pi*epsilon(naught))^(-1) = (c^2)(10^(-7) H/m) = 8.897552E9 N*m^2/C^2
        -Epsilon(naught): Represents electrical permittivity, which is a measure of resistance encountered when forming and electrical field. Resistance typically comes from a dielectric (an electrical insulator that polarizes (inhibits electrical fields) the field).
____________________________________________________________________________________________________
-Series: The sum of a sequence of terms
    -Convergence: The series eventually sums to a number
        -All series that have a finite upper bound converge
    -Divergent series: Goes on to infinity
    -Ratio test: A test for testing if a series is convergent or divergent
        -1. Replace every index value in the argument of the series with that variable index plus one and divide by the original argument, then simplify.
         2. Take the limit as n approaches infinity of the above argument.
         3. If the limit is less than 1, the series converges; if it is greater than 1, it diverges; if it is equal to 1, additional methods are needed.
    -P series: A series from n = 1 to infinity of (1/n^p)
        -Converges if p>1, diverges if p = 1 or p < 1
    -Root test: A test of convergence or divergence.
        1. Take the nth root of absolute value of the argument a-sub(n).
        2. Take the limit as n approaches infinity of the above argument.
        3. If the limit is less than 1, the series converges; if it is greater than 1, it diverges; if it is equal to 1, additional methods are needed.
    -Alternating series: A series in which the signs of the terms alternate; it is of the form "the series from n=0 to infinity (or some other upper bound) of ((-1)^n*a-sub(n))"
        -Proving convergence: a-sub(n) must be decreasing (that is, a-sub(n + 1) < a-sub(n) or a-sub(n + 1) = a-sub(n)) and the limit as n approaches infinity of a-sub(n) = 0. 
    -Power series: An infinite series of the form "the sum from n = 0 to infinity of (a-sub(n)*(x-c)^n)" where a-sub(n) is a changing constant.
        -Power series as representations of functions: ManipuLating the equation "the sum from n = 0 to infinity of x^n when abs(x)<1 = 1/(1 - x) to represent functions. 
        -Convert function to the form 1/(1-x); x can be any term(s); and the series will converge when the absolute value of x (the arbitrary term(s)) is less than 1. Solving that inequality produces the interval of convergence. 
            -In some cases, such as when the numerator is not 1, pull the numerator out to convert the main term to the desired form, put it in a series of the form described above, and multiply the numerator back in, then continue. 
        -All functions have power series representations. 
    -Taylor series/Maclaurin series expansion: The constant is equal to the nth derivative of the center of the series (or the a in the (x-a)^n) divided by the factorial of n, or f^(n)(a)/n!.
        -To find the Maclaurin series of a function, the a value must be given and f(x) must be given. Solve for the derivatives and plug in the a value so find the series. 

-Imminent lawless action: A standard in the US that restricts freedom of speech such that people can be arrested on the grounds of speech that incites a violation of the law that is both imminent and likely.
    -Established by SCOTUS in Brandenburg v. Ohio                       
-Obama's policies:
        -American Recovery and Reinvestment Act of 2009 (ARRA), or The Recovery Act: Response to the late-2000s recession; primary goal was job creation. Also attempted to provide temporary relief programs to those in need. 
            -Invested in education, infrastructure, health, and green energy. 
            -Total $831 billion; expanded unemployment benefits and social welfare provisions. Based in Keynesian economic theory.
        -Tax Relief, Unemployment Insurance Reauthorization, and Job Creation Act of 2010: extended "Bush tax cuts" (Economic Growth and Tax Relief Reconciliation Act of 2001 and Jobs and Growth Tax Relief Reconciliation Act of 2003, lowered tax rates)
        -Obamacare: (AKA Patient Protection and Affordable Care Act). Individual mandate: Requirement by law for all persons in the USA to purchase health care; controversial; upheld by Supreme Court (SCOTUS) as constitutional on the grounds that it DOES NOT fall under the commerce clause, but rather, Congress' power to levy taxes.
                Obama did say on September 20, 2009, that this "is absolutely not a tax. What it's saying is that other people aren't going to carry your burden."
            -Allows FDA to approve more genetic drugs.
            -Increases rebates for drugs people get through Medicare (Guarantees citizens over 65 as well as young persons with disabilities health care, paid for by the rest of society's taxes)
            -Fast food chains have to show calories next to foods.
            -80:20 Rule: If a company isn't spending at least 80% of its premiums (Risk Premium: minimum amount of money by which the Expected Return on a risky asset must exceed the known return on a risk-free asset, making the risk worth it) on medical care, it must rebate the excess.
            -"Snooki Tax": 10% tax on indoor tanning booths.
            -Kids can be insured by their parents health insurance until age 26.
            -Under 19, people with pre-existing conditions cannot be ignored by insurance companies.
            -Insurers have less power to change the amount of money people pay.
            -Insurers can't drop customers once they get sick.
            -Insurers need an appeal process once they turn down a claim. Previously, one needed to file a lawsuit.
            -Companies handling elderly have reduced costs.
-Ceteris paribus: All other things are, for simplicity's sake, assumed to be equal and constant. Literally "Other things" and "equal", respectively. Eliminates black swan events.
-Production Possibilities Frontier: Curve (on coordinate plane) of possible oppurtunites, and their costs.
-2011: UN declares Internet access a human right
-Solipsism: I am the only existing consciousness since no one else can be proven to exist.
    -Problem of other minds: how can I know that others have minds and are conscious?
    -Anosognosia: A syndrome in which people have a disability but deny having it at all.
-Swampman Teleportation: All my atoms break down freely in one spot, then OTHER atoms configure themselves in the exact same way somewhere else. Is that still me?
    -Alternatively, if a surgeon removes cells, one by one, from person A to person B, at what point has person B become person A?
-Decision Theory: Used to make decisions
    -Used in: Choice under uncertainty (expected value, probability), intertemporal (different outcomes at different points in time), complex decisions
    -Decision Support System: Computer program that uses given axioms, such as information gained through personal knowledge, raw data, documents, or business models. Aid businesses in making decisions
    -Expert System: Computer system that emulates the decision-making ability of a human expert. First truly successful forms of AI.
-Project MK-ULTRA: Mind control, interrogation resistance, etc.
-Limerance: Passionate Love, associating meeting a certain person (your lover) with immediate reward. Opposite of committed, long-term love (i.e. marriage)
-Hedonic Treadmill: Money makes one happy, but only up to a certain point, or threshold. This is, in the US, $75,000 per year
    -Tendency of people to return to a (relatively stable) level of happiness even after major changes (positive or negative), and stay at this level.
-Red Queen's Hypothesis: constant adaption required to maintain relative fitness against other species.
    -Handicap Priniciple: Organisms with high fitness exhibit this high level of fitness by squandering resources simply because they can.
        -Ex: A very fast or intelligent bird may keep lookout for other birds, thus increasing its own chance of death, but showing other birds, and potential mates, their level of fitness.
-Unitarity: Modification to the equations of quantum field theory that guarantee that all possible outcomes reveal at most 1.
-Depression evolved as a type of way to inhibit people from trying to reach unattainable goal and thusly wasting time and resources. Similar to how pain stops us from doing physically damaging things.
    -Depressive realism: less affected by delusions of locus of control, cognitive dissonance -> optimism bias & Dunning-Kruger effect
-H1-B Visa: Allows highly accomplished foreigners to come to the US with temporary citizenship if they work in a highly specialized field. 
-Axiology: Study of value.
-Ontology: Study of existence.
    -Consequentialism: Something is right based on its consequences.
    -Deontology: Something is right with regard to a moral guideline or law.
-Condorcet method: candidate is elected based on the percent of population that favors him over other candidates
    -Voter Paradox, or Condorcet's Paradox: Majority wishes conflict with themselves. 
        -Ex: Voter 1 prefers A, B, then C.
             Voter 2 prefers B, C, then A.
             Voter 3 prefers C, A, then B.
         If C is elected, then it is wrong because only 1/3 of the population prefers C the most, while 2/3 prefer other candidates (i.e. B) more. Same argument applies to all candidates.
-Alternative Voting: Voting by ranking, not choosing. Eliminates spoiler effect.
    -Spoiler effect: 
-Dunbar's number: the theoretical cognitive limit to the amount of meaningful relationships a person can have; number is between 100-230, commonly rounded to about 150
    -Meaningful: One knows the person in question, who he is, his state of being, and how he affects his environment. This is meaningful.
    -Only covers CURRENT relationships, not past ones.
    -Number may be higher or lower depending on long-term memory size.
    -Directly correlated function of the size of the neocortex.
        -log_10(N) = 0.093 + 3.389 log_10(C) where N = mean group size for any given taxon and C = ratio of neocortex to total brain
         Statistical analysis using the equation: r^2 = 0.764, correlation (r) of group size to neocortical ratio = 0.874, a significance test yields P < 0.001
-Chlorine trifluoride: Most flammable substance known to man. In enough quantity (900kg) can burn through a foot of concrete, and 3 feet of sand and gravel.
    -Component in rocket fuels. 
    -Is a Hypergolic propellant, meaning it ignites upon contact with another substance (a great many substances) and requires no spark.
    -Reacts violently with almost anything, even common fire retardants such as sand, water-based suppressors (fire extinguishers), and asbestos.
    -Is a fluorinating agent (ie halogenates with organic materials).
        -Halogenation: A chemical reaction in which a halogen reacts with some other substance, usually organic, by adding itself (sometimes replacing existing molecules) to the substance's molecular structure. This can rip molecules apart and is often a very violent exothermic reaction. 
    -Oxidization properties: Is a better oxidizer than oxygen. 
-Botulinum toxin, found in botox, is the world's deadliest poison. An amount equal to a grain of salt can kill a 200lb man. 4kg, properly dispersed, could wipe out all humans on earth.
-Fluoroantimonic acid: Most acidic acid in the world, can burn through glass.
    -Explodes on contact with water.
    -When it reacts, it gives off poisonous fumes that kill everyone around it.
-Cavitation: Small, high pressure bubbles form that, when pop, can produce shock waves, heat up to 15,000 K, and loud noises loud enough to kill fish.
    -Inertial: Cavity (bubble or void) rapidly collapses and produces a shock wave.
    -Non-Intertial: Bubble, due to some form of energy input, oscilliates in a certain way.
    -May be used purposefully for medical purposes such as microsurgery, within cells. Possible cancer cure.
-Psychologism: a generic type of psychology where psychology plays a central role in grounding and explaining some other, non-psychological type of fact or law. 
    -Logical: logical truths or mathematical LAWS are derived from or explained by psychological facts or laws.
    -Mathematical: mathematical concepts are derived from or explained by psychological facts or laws.
    -Advocated by John Stewart Mill, criticized by Gottlob Frege, Charles Sanders Peirce, and Maurice Merleau-Ponty
-Gottlob Frege (mentioned above) founded analytical philosophy (emphasis on clarity and argument and a respect for natural sciences)
-Caloric Restriction: (without malnutrition> dietary regime with that restricts caloric intake
    -Positive effects: Reduced risk of cardiovascular disease, improved memory, increased life span
-Epigenetics: "above genetics". tells genome what to do. analogy: software is epigenetics, hardware is genetics; epigenome tells genome what to do. Acts as a tag towards genes as to what is expressed relatively.
    -Methyls: Latch on to cells genes and control whether or not it is expressed
    -Histones: Bind the cell, with varying levels of tightness, controls how much the gene is expressed
    -Epigenome can change throughout life pertaining to type of food intake, stress level, nutrition, drug use, etc.        
-Simo Hayha: Highest number of sniper kills in any major war
-Toba catastrophe: Between 69,000 and 77,000 years ago, a volcanic supereruption took place at Lake Toba, killing the entire human population except for about
 10,000 people and only 1,000 breeding pairs
    -Resulting in an evolutionary bottleneck in human evolution (decreases genetic variation and makes it harder to evolve and adapt)(Increase genetic drift)
    -Plunged the earth into a 6-10 year volcanic winter and a subsequent 1,000 year cooling episode
-Extinction Event: (biotic crisis, mass extinction, extinction-level-event(ELE)) sharp decrease in abundance and diversity of macroscopic life
    -5 major ELEs (in which at least 50% of the planet's life was wiped out)
        -In chronological order:
            -Ordovician-Silurian extinction event: (450 Ma-440 Ma)(60% of marine invertabres died) At the time, all life was confined to the seas. 
             Causes: 1. Movement of Gondwana (large supercontinent of the southern side) into south polar region->global cooling, glaciation, sea level fall->eliminates habitats
                    ->ELE
                 2. Gamma ray burst from a hypernova within 6,000 light years (20 second burst means the ozone layer gone)->exposes animals to high levels of ultraviolet radiation->extinction
            -Late Devonian extinction: (374 Ma)(50% of all genera dead)
             Causes: Widespread anoxia
            -Permian Triassic Extinction Event: (a.k.a. Great Dying)(251 Ma)(83% of earth's genera dead) 96% of marine life dead, 70% of land life dead. Only known mass extinction 
             that applied to insects.
             Causes: 1. Impact event: Large space debris of some kind collides into the planet
                 2. Volcanism: Flood basalts (volcanic eruption coats land or ocean floor with lava)->Volcanic eruption (largest ever)->Siberian traps form.
                        Consequences: Acid aerosols, global warming, ice age, etc.
                 3. Methane hydrate gasification:Methanogens release methane->reduction in carbonite ratio->death
            -Triassic-Jurassic extinction event: (199.6 Ma)(50% of genera dead)
             Causes: 1. Gradual climate change caused by sea level fluctuations
                 2. Asteroid impact: Unlikely, no crater exists to prove it
                 3. Massive volcanic eruptions
            -Cretaceous-Tertiary extinction event, or  Cretaceous-Paleogene extinction event: (65.5 Ma)(Most life dead)
             Causes: 1. Asteroid impact event
                 2. Flood basalts occur, creating Deccan traps (evidence)
                 3. Multiple impact event
-VY Canis Majoris: Largest star in the known universe (Red hypergiant 5,000 light years away, 1800-2100 solar radii)
 Eta Carinae: most luminous star in the observable universe
-Nucleus accumbens: collection of neurons that forms the main part of the ventrial stiatum. Plays a role in reward, placebos, laughter, pleasure, addiction, aggression, and fear
    -Social happiness makes you less susceptible to addiction
    -Stratium: A subcortical part of the forebrain that receives input from the cerebral cortex (covers the cerebrum; divided into 2 cortices, plays a role in memory, attention, perception, thought, language, and consciousness) and outputs into the basal ganglia. 
        -Ventral stratium: A subportion of the startium that contains the nucleus accumbens and the olfactory tubercule; considered a reward center.
-Doomsday argument: Probabilistic argument that claims to predict the number of human members of the species that will exist.
    -A.K.A. Carter catastrophe (named after Brandon Carter in 1983, the first to explicitly propose the argument)
    -Assume a sample space of our fractional position (A/B) of (0,1] with 0 = no humans have lived (beginning) and 1 = maximum  humans that have lived (time of extinction). 
     Then, there is, say, a 0.95 (95%) chance that WE are in the interval (0.05, 1]; 95% chance that we are in the last 95% of the human race that will ever live (a.k.a. that we are NOT in the first 5%). 
    -If our position is A, and the total number of humans EVER is B, then there is a 95% chance that B=20A (since A/B=0.05).
    -Philosopher John A. Leslie, who believes the argument, says that 60 billion humans have lived so far (A = 60 billion). Then, there is a 95% chance that the number of humans that 
     will EVER live is 20 x 60 billion = 1.2 trillion humans. 
    -Assuming the world population stabilizes at 10 billion and an average life expectancy of 80 years, it is estimated that the remaining 1140 billion humans will be born in the next 9120 years.
    -So, there is a 95% chance of extinction in the next 9120 years.  
-Eight Circuit Model of Consciousness: Transhuman theory proposed by Timothy Leary; splits the human brain into eight spheres of activity. The first four circuits are necessary for survival, but the last four circuits are progressively more difficult to attain, with fewer people attaining them.
    -Larval circuits: First and lower circuits that deal with normal psychology:
        1. Oral biosurvival: Covers nourishment, safety, etc. The circuit is imprinted early in life. Sometimes equated to oral stage in Freudian psychosexual development. Activated by opiates and opioids (e.g. heroin).
            -Positive imprint: Environment is seen as innocuous and welcoming, building trust.
            -Negative imprint: Environment is seen as hostile, causing perpetual anxiety.
        2. Emotional-territorial: Aggression, territoriality, domination versus submission, etc. Imprinted during toddler years. Activated by alcohol. 
            -Dominance imprint: Aggressive behavior that causes the alpha male attitude in people. 
            -Submission imprint: Cooperative, complaisant behavior. 
        3. Neurosemanticdexterity: Deals with higher cognition, such as calculation, prediction, extrapolation, invention, and language. Extends to physical coordination. Activated by stimulants (e.g. amphetamines, cocaine, etc.)
        4. Socio-sexual: Deals with sexual pleasure (as opposed to procreatory purposes), politics of the social structure, and cultural values. Imprinted by the first orgasm and mating experiences. Not activated by any specific drug, though some suspect that it is activated by enactogens (Psychoactive drugs characterized by euphoria linked to empathy and emotional closeness, e.g. MDMA (i.e. ecstasy)).
    -Stellar circuits: Upper circuits that deal with psychic, spiritual, and psychedelic mental states. 
        5. Neurosemantic: Consciousness of the body that produces non-conceptual feelings of well being; activation triggers detachment from first four circuits. Activated by cannabis, tantra, yoga, zen mediation.
        6. Neuroelectric: (A.K.A. metaprogramming) Concerned with neurological self awareness. Activated by LSD, mescaline found in peyote, and psilocybin mushrooms (i.e. shrooms).
        7. Neurogenetic: (A.K.A. morphogenetic) Connection to life and evolution in general. Ancestral DNA-RNA feedbacks are involved. Sometimes equated to Carl Jung's collective unconsciousness (the part of the unconscious mind that autonomously and involuntarily structures memories into experiences). Activated by same triggers as those for circuit 6.
        8. Psychoatomic: (A.K.A. quantum non-local) Illumination of super-spacetime awareness and non-local awareness. Activated by DMT, ketamine, and high doses of LSD.
    -Pretty much total bullshit; pseudoscience.
-Tsar Bomba: Soviet Union bomb detonated in 1961 (test drive) and is the most powerful bomb ever. Thermonuclear fusion. 500 megatons.
-Qualcomm Tricorder X PRIZE: Phone gadget that measures and diagnoses illnesses and disorders better than a doctor.
-Nap: A short period of sleep, typically during daylight hours, and are taken as a response to daytime drowsiness. 20 minutes or 1-2 hours
    -Negative effects: May steal time away from night sleep, which is much more beneficial. May mess up sleep cycles.
    -Positive effects: Refreshes the mind, improves alertness, boosts mood, and increases productivity.
        -37% less chance of heart disease
        -Improves certain memory functions
    -Power nap: 18 - 25 minutes. Does not allow sleep to go past stage 2 of the sleep cycle. If you go longer, you get sleep inertia (grogginess, sleepiness, crankiness after sleep)
-Monday mornings are so depressing that on average, one doesn't smile until about 11:16 a.m.    
-Recognized "superhumans":
    -Ma Xiangang: Feels no pain with electricity coursing through him, can give electromagnetic deep tissue massages with only his hands
    -Dean Karnazes: Can run forever at a 7-10 minute mile pace. Went from New York to San Fransisco in 75 days, non stop but for food. Muscles resist damage through use.
    -Steven Wiltshire: Perfect, indelible visual memory.        
    -Kim Peek: Indelible memory in general. Ability comes from a congenital birth defect.
    -Wim Hof: Invulnerable to cold. Climbed Mt. Everest in shorts. Through intense meditation, he can control his autonomous nervous system and immune response to cold.
    -Isao Machii: Japanese man with superhuman reflexes. Slices BB pellet with a sword in midair. Doesn't use his eyes consciously, but perceives them through a sixth sense. Anticipates position of pellet.
-Fermi's Paradox: The size of the universe, it's age, and the adaptability of life suggests that life should have formed in the universe. However, there is no evidence that and advanced civilization ever achieved space
 colonization. This means that the space colonization step in evolution (the last step) is incredibly difficult for SOME reason. Therefore, humans are likely for that unknown reason, to go extinct soon.
    -Neocatastrophism: Life exterminating events (i.e. gamma ray bursts) act as galactic regulators. Possible solution to the SOME reason
    -Great Filter: that SOME reason that prevents lifeforms from evolving into space colonization. 
-Foot-Pound force (a.k.a. foot-pound): Unit of energy. Energy transferred in applying a force of one pound through the displacement of one foot. Corresponding SI unit is Joule.
-Ex post facto law: law that charges actions committed before the law prohibiting them was passed.
-Ipso facto: "By the fact itself", or by definition.
-Munchhausen Trilemma (a.k.a. agrippa's trilemma): philosophical term. states that nothing can be proved through logic, because all proofs or all evidence relies on a trilemma:
    -Circular argument: Theory and proof support each other. Argument is based on circular reasoning. 
        -Coherentism: Accepts circular horn of trilemma.
    -Regression argument: Theory requires proof A. Proof A requires Proof B. And so on ad infinitum. Argument is based on infinite regession.
        -Infinitism: Accepts regression horn of trilemma. Uncommon. 
    -Axiomatic argument: Theory relies on predetermined axioms which may or may not be true and cannot be proven as such because of this very trilemma.
        -Foundationalists: Accepts axiomatic horn of trilemma. 
-Determinism: The belief that if certain conditions are given, nothing else could happen but a particular result, regardless of the number of trials.
-Voluntary Human Extinction Movement (VHEMT): Environmental movement that calls for people to abstain from reproduction in order to provoke a gradual humankind extinction. 
    -An extinction of humanity would be beneficial (enormously) to the environment and the earth in general, including almost every living organism in the biosphere.
        -Prevent environmental degradation
        -Prevents extinctions of other species that benefit the ecology of the planet.
        -Founded in 1991 by Les U. Knight
    -In contrast to contemporary natalism (a belief that promotes reproduction and glorifies parenthood, anti-abortion and anti-contraception. Widely used in religion.)
-Senescence: Biological property of aging. Over time, changes in molecular structure of cells accumulate to damage metabolism, resulting in death. 
    -Cellular level: After cells hit the Hayflick limit (40-60 cell divisions in humans; varies per species; correlated with average life expectancy of species in general) they can no longer divide.
        -Telomeres: Series of repetitive nucleotide sequences at the ends of chromosomes that protect it from deterioration (or from fusing with other chromosomes). With each cell division, telomeres get shorter, and after the Hayflick limit cells cannot further divide.
            -Telomerase reverse transcriptase: (A.K.A. TERT or hTERT in humans) replenishes telomere ends. Dangerous as it can lead to uncontrollable growth of cells that are resistant to apoptosis (cell death caused by intracellular mechanisms; cells are programmed to die eventually), leading to cancer.
    -Biological immortality: cells that are not subject to Hayflick limit. 
        -Immortalization: subjecting telomerase to cells. Common immortalized cell lines are cancerous cells HeLa and Jurkat.
        -Organisms with possible immortalization: 
            1. Tardigrades: (A.K.A. water bears) highly resilient microscopic animals. When extreme conditions hit (e.g. vacuum of space, extreme dehydration, extreme cold or heat, huge amounts of radiation, etc.) tardigrades go into suspended animation by completely draining their bodies of 99% of water, and then waiting out the conditions.
            2. Hydra: Organisms that do not age. It is unknown how they replenish their telomere ends. Cells continually divide within the hydra, diluting the effect of toxins and defects.
            3. Turritopsis nutricula: Species of jellyfish that uses transdifferentiation to replenish cells after sexual reproduction.
                -Transdifferentiation: (A.K.A. lineage programming) mature somatic cells (cell forming the body of an organism) turns into another. The cycle can repeat indefinitely.
    -Oxidative stress: in organisms dependent on oxygen, oxidative stress slowly kills. Oxygen forms stable bonds with hydrogen, helping organisms to create sugars, but about 2% of the time oxygen bonds with only one hydrogen, not 2 and becomes a free radical (extremely reactive). The oxygen binds with chemical structure of cellular molecules (e.g. DNA, fats, lipids, ribosomes, etc.).
    -Study of biological death: biogerontology.
-Logical reasoning (3 types): 
        1. Deductive reasoning: (A.K.A. top-down logic) Reasoning from a given set of premises to reach a logically certain conclusion, using only the premises given. 
        2. Inductive reasoning: (A.K.A. bottom-up logic) Reasoning that creates generalizations and general, umbrella propositions that are based on specific examples. 
            -Probabilistic reasoning, as specific examples never provide certainty unless all possible variations are examined, since without doing so it is possible for a counter-example to exist, no matter how unlikely. 
        3. Abductive reasoning: FormuLating possible explanations for observed examples. If B is observed, then A can be abduced from B and is said to be sufficient but not necessary for B; this is because although A is a possible explanation for B, it is not the only one and not known to be true for sure.
            -Example: If it is observed that the lawn is wet, then the proposition "It rained last night" is a possible explanation for the observation and therefore can be abduced from evidence and is sufficient but not necessary for the observation that the lawn is wet.
    -Converse: Reversal of a logical conditional statement; Given the conditional A that Q -> P, the converse of A is that P -> Q. The converse is not always true even if the original is.
    -Obverse: Negation of a logical statement; Given the conditional A that Q -> P, the obverse of A is that -Q -> -P. The obverse is not always true.
    -Contrapositive: Reversal of the negation of a conditional statement; Given the conditional A that Q -> P, the contrapositive of A is that -P -> -Q. The contrapositive is always true if the original is.
-MIT researchers, led by Jay W. Forreester's Institute, says the world could suffer from global economic collapse by 2030 if the rate of consumption of resources does not change negatively. 
-Graham's Number: Solution to the problem of hypercubes in Ramsey theory (problems on least amount of elements for a certain property to be held). Answer: G64, which is 3 g63 arrows 3
    -Problem: Connect all vertices of an n-dimensional hypercube to make a graph of 2^n vertices. Color the edges (lines connecting two points) red or blue, varying. What is the smallest value of n (the smallest number of dimensions
     such that...) so every colored edge has a complete subgraph on 4 coplanar vertices?
-Flynn Effect: Substantial and long-term increase in average IQ. 
    -Normally, the average is set to 100 with the standard deviation being from 15-16.
    -Test scores have been linearly increasing over time. The average of 100 is consistently being reset to higher points on older IQ scales.
    -Psychometrics: study (psychology) that deals with the theory of psychological measurement, mostly in knowledge, abilities, attitudes, personality, etc. 
-Doppler effect: When a source is continuously emitting waves picked by some observer and the source and observer are in relative motion, the frequency of the emitted waves changes (in the perception of the observer. Consider an source of waves moving relative to a observer (equivalent to the observer moving relative to the source). If the source is moving towards the observer, once the source emits a wave, it moves forward simultaneously, so the next wave emitted is emitted at a distance closer to the original wave than if it had simply remained still. Formalizing the situation a bit, say the speed of emitted waves is v and the speed of the source is u. Once the wave is emitted, it propagates forward at speed v. If the next wave is emitted a time t later, the wave travels a distance v*t. If the source were stationary, the next wave would be emitted at time t and have traveled a distance of 0, while the initial wave has traveled distance v*t. Another unit of time later another wave is emitted, and the initial
         wave has traveled distance v*2t, the second wave a distance of v*t, and the third wave a distance 0. The pattern is that successive waves always have the same distance between them. If the emitted waves correspond to the crests or troughs (or any equivalent part of a wave throughout the period, really), then the constant distance corresponds to the wave's wavelength. However, if the source is moving at speed u, then if the source emits a wave and then emits another wave at time t, the first wave has only traveled a distance v*t - u*t = (v - u)*t relative to the source, instead of the "natural" distance v*t. Thus, instead of a wavelength of v*t, we have the compressed wavelength of (v - u)*t (which corresponds to increased frequency, of course). A similar argument shows that if the source is moving away from the observer, the frequency drops (because the wavelength increases). Thus, if a wave-emitting source is moving towards an observer, the observer detects the waves as having higher frequency than they 
         do, and if the source is moving away from the observer, the observer will detect waves at a lower frequency than normal.
    -Formula: f = (v + v_r)/(v + v_s) * f_0, where f = observed frequency, v = speed of wave in the medium, v_r = velocity of observer relative to medium, v_s = velocity of source relative to medium, and f_0 = natural frequency of emitted waves. Note that v_r and v_s are velocities and so can be negative (though v refers to a speed and therefore cannot be negative).    
    -Examples: The Doppler effect accounts for redshift observed in astronomy, the sound car engines make as cars drive by, and other physical phenomena. 
-Paper on Existential Risks (global, terminal)(Nick Bostrom, prof. of Philosophy at Oxford)
-Florida's Stand Your Ground Law: Removes the "duty to retreat"(a threatened person must try to de-escalate a situation before using deadly force). Now, threatened people (threatened meaning merely trespassing by a drunk man) can use deadly and fatal force (shooting or stabbing to death).
-Logical Fallacies:
    -Reductio ad absurdum: Extending an argument into absurd proportions and criticizing the outcome.
    -Association Fallacy:A is a B. A is also a C. Therefore all B's are C's.
        -Guilt by association: Ex: My dog has four legs. My cat has four legs. Therefore my dog is a cat.
        -Honor by association: An object is good because something related to it is good.
            -Used heavily in marketing; attractive people saying a product is good associates the product with good, psychologically.
-Balanced-Budget Amendment: A constitutional rule that says that a state cannot spend more than its income. Requires a balance between the projected repeipts and expenditures of the government.
    -Have been added to many states in the USA.
    -Proposed (most recently by Rick Santorum) to be added to the US Constitution.
    -Makes an exception in times of war, national emergency,  recession, or a supermajority vote.
        -Supermajority Vote: Any vote that exceeds the simple majority (over 50%) and is most often required whenever the rights of the minority come into question.
-Earmark: A legislative provision that directs apporved funds to specific projects or directs specific funds to exceptions from taxes or mandated fees.
-Loaded Language: Wording that attempts to influence a certain audience by appealing to emotion (i.e. pro-life or pro-choice)
-Prima Facie: "On the face of it" 
    -Legal Definition: A prima facie case is one with sufficient evidence to convict prior to lawsuit or criminal prosecution, unless there is sufficient counter evidence at the trial itself.
    -Obvious stuff
-Drake Equation: Equation used to estimate the number of detectable extraterrestrial civilizations in the Milky Way Galaxy.
    -N = (R*)(Fp)(Ne)(Fl)(Fi)(Fc)(L) where 
        -N = number of civilizations, R* = average rate of star formation in our galaxy, Fp = fraction of those stars with planets, Ne = average number of planets that can support life, Fl = fraction that actually go on to support life, Fi = fraction that go on to support intelligent life, Fc = fraction of civilizations that can detect our communications, L = length of time for these civilizations to see these communications
-Square-Cube Law: (Galileo 1638 - "Two New Sciences") Volume grows faster than area. This principle is useful, in bio-mechanics and engineering.
    -Law: If any object grows by a multiplier X, the surface area grows by a factor of the multiplier squared, and the volume grows by a factor of the multiplier cubed.
-Geneva, Switzerland: Second most populous city in Switzerland, after Zurich. Most populous city in Normandy (French speaking part of Switzerland). 
    -Landmarks: CERN Lab, Jet D'Eau Fountain, Palace of Nations (Previous headquarters of League of Nations and current Secretary Office for UN), Red Cross, Internet Hall of Fame, World Intellectual Property Organization (WIPO; encourages creativity and innovation), European headquarters of UN, UN High Commissioner for Refugees (UN Refugee Agency; supports and protects refugees), UN High Commissioner for Human Rights, World Health Organization,
                World Trade Organization (supervises and liberalizes international trade; provides a frame for negotiation and formalizing trade agreements), World Economic Forum, European Broadcasting Union 
        -Full List here: http://en.wikipedia.org/wiki/List_of_international_organizations_based_in_Geneva
    -Fifth most expensive city in the world
    -Referred to as the world's "Peace Capital" and most compact metropolis
    -Third highest quality of living of any city in the world (narrowly behind Zurich) and 13th most important financial center in the world (third in Europe, behind London and Zurich).
-Positivism: view that data derived from sensory experience and logical/mathematical treatments of said data are, together, the source of all knowledge.
-Realism: view that reality exists independent of observers
-Friedrich Nietzsche: Philosopher in existentialism, nihilism, postmodernism. Concepts:
    -Death of God: One can no longer turn to God as a source of absolute morality. Humans are no longer able to believe in in any such cosmic order because they don't recognize themselves in it. Leads to rejection of objective morality, and eventually nihilism.
    -Perspectivism: Philosophical view that all ideations (idea generations) take place from different perspectives. There are many perspectives or views in which the truth holds. No way of seeing the world is absolutely true, but NOT all perspectives are equally valid. 
        -Rejects objective metaphysics
    -Ubermensch: overall goal for humanity to set for itself
        -Suicide Note: this is the technological genesis of a relative God.
    -Amor Fati: "Love of one's fate"; Everything that happens in life, including suffering, is a good thing. Acceptance. 
        -Nietzsche didn't create the idea, but was one of its strongest supporters.
    -Eternal return: The universe, with a finite probability, if it exists in infinite spacetime, will recur an infinite amount of times.
        -Prevalent in ancient Hindu and ancient Egyptian culture
    -Will to power: The main driving force in man. Represents achievements; the yearning of wanting more; greed.
        -Backed up by moving towards self-actualization of full potential in Abraham Maslow's hierarchy of needs.
        -Maslow's Hierarchy of Needs: A psychological theory that describes humans' innate curiosity, overall motivations, and sense of purpose. It includes a pyramidal hierarchy of five psychological states Maslow deems as mentally necessary, in that order.
            -Hierarchy: 
                1. Physiological needs: These are basic necessities that allow the human body to continue to function, and are thus the first tier of necessities; without these, none of the other four matter. This stage includes things like food, water, shelter, sex, protection, air, etc.
                2. Safety: After physiological needs are met, the human subconscious strives for its basic security needs to be met, such as personal security against harm, financial security, overall health and lack of medical problems, and insurance in case of adverse circumstances. This sense of security is second in the hierarchy.
                3. Love: This stage is reached when the first two are fully satisfied, and describes the innate desire for meaningful relationships with other humans. This includes friendship, strong familial relations, romantic support, etc. This stage also describes a sense of belonging. Humans always need a sense of belonging when in social groups, no matter the size of the group. Note that it is equally important to love and to be loved.
                4. Esteem: This stage describes the innate desire for respect from others. It boosts self-esteem and gives a sense of value to the person. Self-respect is also a component, alongside respect from others. 
                5. Self-actualization: This is the final stage that all humans strive towards, but never reach. It describes the need to realize one's full and complete potential. It is the desire to be everything one can be, and accomplish everything one can and one wants to accomplish. The person must not only reach his goals, but excel in them and master them.
    -"Life-Affirmation": Honestly question anything in life, no matter the ethos or pathos that drives it, should it use life's resources.
    -Master-Slave Morality: Two central versions of morality: master morality and slave morality. Morality here means the formation of a certain culture. Nietzsche criticized master morality.
        -Master morality: "Morality of the strong willed". Essence of this is nobility; "good" = noble, strong, powerful. Weighs actions on a scale of good or bad consequences. Master creates value.
        -Slave morality: Reaction to oppression; centered in weak, not strong. Characterized by pessimism and cynicism. Essence is utility (good is useful for a community, not strong). "Good" = humility, kindness, and sympathy. 
-Asexual reproduction: reproducing by itself. Types:
    -Binary Fission: Organism splits in half, both halves mature, split again. (i.e. bacteria, protists, etc.)
    -Budding: Little buds pop off the original animal and then become their own beings. (i.e. hydras)
    -Vegetative: Reproduce through bulbs or tubes or rhizomes
    -Parthenogenesis: Forming embryos inside itself without a need for fertilization by a male. (i.e. arthropods, some lizards)
    -Fragmentation: Parent breaks into little pieces, each of which can split and reproduce again.
-Fibonacci Sequence: 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 477, 610, ...
    -Number of spirals in plants are always Fibonacci numbers
    -Divide a Fibonacci number by the previous one will approach Phi (1.618...) or the Golden Proportion
        -Part 1: http://www.youtube.com/watch?v=ahXIMUkSXX0&feature=youtu.be
         Part 2: http://www.youtube.com/watch?v=lOIP_Z_-0Hs&feature=youtu.be
         Part 3: http://www.youtube.com/watch?v=14-NdQwKz9w&feature=youtu.be
    -Lucas Numbers: closely related to Fibonacci numbers, same as Fibonacci sequence but starts with 1, 2 not 0, 1
        -Also observed in plants
-Justice as Fairness (John Rawls): Rawls' conception of justice, an egalitarian state. Divided into two main principles: liberty and equality. Equality is further divided into two sub-principles: Fair Equality of Opportunity and Difference Principle.
    -Liberty Principle (ranked first): all people have certain inalienable basic liberties.
    -Equality Principle (ranked second): establishes distributive justice (socially just allocation of goods in a society; in a society following this princple, there would be no incidental inequalities in income distribution.
        -Fair Equality of Opportunity (ranked first): offices and positions are open to any person, regardless of ethnicity, race, etc.
        -Difference Principle (ranked second): regulates inequalities in income distribution; only permits inequalities that work to the advantage of the worst-off. 
-Forms of Justice: 
    -Distributive justice (see above)
    -Procedural Justice: fairness in the process of resolving disputes and resource allocation. Connected to due process (USA) and fundamental justice (Canada), etc. 
        -Three Ideas of Procedural justice put forth by John Rawls:
            -Perfect procedural justice: Has (A) an independent criterion for what a just outcome is and (B) a procedure that guarantees the just outcome be achieved.
            -Imperfect procedural justice: Has (A) of perfect procedural justice, but no system of guaranteeing the just outcome be achieved; has (A) but not (B).
            -Pure procedural justice: No criterion for what a just outcome is other than the process itself.
        -Three models of procedural justice:
            -Outcomes model: Fairness of procedure depends on producing correct or just outcomes. Ex: in a criminal trial, the fair outcome is condemning guilty and exonerating innocent. in a legislative process, the fair outcome is good laws (unfair outcome is bad laws).
                -Limitations: if two procedures have equivalent outcomes, they are equally just. Ex: Dictatorship is as just as a direct democracy so long as they both guarentee the outcome of good medical care.
            -Balancing model: Fairness of procedure depends on balancing between costs and benefits of the process. 
            -Participation model: allows those that are affected by the outcome of a procedure to participate in the procedure. 
    -Restorative justice: Focuses on the needs of the victim and the accountability of the offender. The offender is encouraged to repair the harm they've done, by repaying stolen money, community service, etc. State provides help to the offender to ward against future offenses.
    -Retributive justice: Proportional punishment is a fair response to crime. "Let the punishment fit the crime."
-Teleological reasoning: justice that takes into account the telos (the goal or point of the object being contested) a d distributes objects based on who would fulfill the object's purpose - its telos - the best.
    -Ex: when distributing flutes, the best flutes should go to the best flute players, because that's what flutes are for. 
        -the fact that this method also serves utilitarianism (everyone enjoys better music if the best flute players get the best flutes) is pure coincidence
-Forms of Nihilism:
    -Moral Nihilism: (a.k.a. ethical nihilism) morality does not exist as something inherent to objective reality. No action is necessarily preferable to another.
    -Existential Nihilism: (my favorite) life has no intrinsic value or meaning. With respect to the universe, the entire human species is insignificant and means nothing; if it were suddenly to go extinct nothing else would necessarily happen because of the extinguishing of life.
    -Epistemological Nihilism: extreme form of skepticism where all knowledge is denied.
    -Metaphysical Nihilism: there MIGHT be no objects at all. extreme form is that existence itself does not exist. Similar to solipsism except this kind of nihilism also denies the self.
    -Mereological Nihilism: (a.k.a. compositional nihilism) objects composed of "parts" do not exist (even applies to objects existing in time made up of temporal "parts"). Only basic building blocks that can't be broken down further can exist. The world full of concrete objects that WE see is just a human misconception.
    -Political Nihilism: follows the regular nihilist's rejection of assertions without proof, but goes further and rejects the necessity of the fundamental social and political norms: government, family, law, etc. More like a form of utilitarianism than nihilism.
-"Four Horsemen of New Atheism": Richard Dawkins, Christopher Hitchens, Sam Harris, Daniel Dennet.
    -New Atheism: religion should not be tolerated, but rather criticized  and exposed by rational argument wherever its influence arises.
-Chaos Theory: The extreme sensitivity of a system to tiny changes in initial conditions that can result from uncertainty in measurement, which will always be present, gives rise to a dynamical instability known as chaos. Because small changes have such drastic effects, mathematically deterministic predictions are no better than random chance. However, the accuracy degrades over time, and so in the short term, marginally accurate predictions can be made.
    -Characteristics: For a system to be chaotic it must:
        1. Be sensitive to initial conditions.
        2. Be topologically mixing (No matter how far apart 2 points are, eventually they will meet).
        3. Have dense periodic orbits (things that are arbitrary close to each other for arbitrarily long amounts of time can behave differently).
    -Determinism: If one knows the initial conditions (the starting direction, magnitude, velocity, etc.) then one can predict the past and future behavior of the universe. Also, there exist certain systems given initial conditions, that given those same conditions, nothing else could happen. One can make long-term predictions.
    -Initial Conditions cannot be measured to 100% accuracy: because infinitely precise instruments are impossible, it is impossible to know with 100% accuracy the initial conditions, and thus a prediction on the system's behavior. However, it was assumed that the more accurately one knew the initial conditions, the more accurate the prediction.
     If one shrunk the degree of imprecision, one proportionally shrinks the degree of uncertainty in prediction and thus increases accuracy ("shrink-shrink rule")
    -Henri Poincare: Father of chaos theory. Discovered systems of dynamical instability (a.k.a. chaos) which do not obey the "shrink-shrink rule"; these were mostly system of three or more planetary bodies where all have a significant effect on the rest. Tiny imprecisions were magnified into huge variations of outcomes. Because, after a certain degree of chaos,
     the degree of long-term prediction is no more accurate than random chance, no matter the imprecision in measurement of initial conditions. These "complex systems" or "chaotic systems" are not deterministic. 
    -The extreme sensitivity to variations in initial conditions is known as dynamical instability, or chaos.   
    -In 1963, Edward Lorentz wrote a mathematical program to study the weather, specifically the effect of the sun's heating the atmosphere on the rise and fall of air currents. Computer code is completely deterministic, and since the program ran on mathematical formulas, Lorentz SHOULD get the same results from the same initial conditions. This simply didn't happen. This was because, when he looked closer, Lorentz wasn't actually
     inputting the same initial conditions in over and over again; they were slightly different, with differences incredibly small; practically indistinguishable differences. Therefore, the weather is a chaotic system. In order to perform a weather prediction with full accuracy, one must take an infinite number of measurements. Butterfly effect: small differences lead to huge variations in outcome. 
    -Chaotic systems are now known to be the norm in the universe.
    -Some scientists believe that the presence of chaos - randomness occurring in the deterministic laws of physics on a microscopic level - is necessary for large scale physical patterns. Some scientists believe that the presence of chaos is what gives the universe its "arrow of time". This is the entropy theory.
    -Is the universe deterministic? Still an open question.
    -This chaos (possibly arising from randomness in quantum mechanics on the atomic level) gives the universe its arrow of time; things go from ordered to chaotic; entropy rises.
    -Dynamical system: a function describes the movement of a point in n-dimensional geometrical space with respect to time.
    -Bifurcation theory: the onset of chaos causes small changes in the parameters of a nonlinear system, which cause qualitatively different solutions. Bifurcation is a period (interval on the plot; usually the period is used to compare distance of the solution plot to the attractor, and different periods of different plots with disturbances from the parent plot are usually compared to observe chaos) being doubled, quadrupled, etc. 
                 When a complex dynamical system becomes unstable, an attractor draws the stress and causes the system to split.
    -Attractor: a point or defined set of points (such as a circle) towards which a dynamical system evolves over time. Opposite: repeller.
        -Strange attractor: in a plot of a dynamical system with more than one attractor, strange attractors are attractors to which the plot jumps unpredictably. It is impossible to predict when the solution will hit one or the other attractor.
    -http://order.ph.utexas.edu/chaos/index.html
-Arbitrage: Taking advantage of a price difference between two markets. Usually, currencies adjust to the changes (invisible hand) but when the economy is not in equilibrium, this adjustment doesn't happen immediately, and so one can take advantage of it to make money from nothing.
    -Eventually, the more one does this over and over again, in theory, the markets will adjust. In the market where one is buying huge amounts of good at low prices, DEMAND increases, and so price goes up. In the market where one is selling huge quantities for a profit, SUPPLY increases and price goes down. Since price of purchase in Market A goes up and profit from selling in Market B goes down, the markets will soon balance.
        -Therefore, arbitrage is only temporary.
    -Triangular Arbitrage: Exploiting a difference in prices of three different currencies. 
        -Cross Rate: If you know the exchange rate of currency A to dollars, and that of dollars to currency B, you can find the expected exchange rate of currency A to B. If that value is different than the actual market exchange rate, triangular arbitrage is feasible. 
        -Arbitrage: (Important to remember: You're starting currency must be the same as your ending currency)
            -Process: Dollars -Convert-> Currency A -Convert-> Currency B -Convert-> Dollars = Profit
            -Converting in one direction gives your profit, while the other gives you a loss, so plan the arbitrage on paper first to see which direction.
-Law of the Land: Article VI, clause 2 in the Constitution, known as the Supremacy Clause, explicitly says that three things  the Constitution, federal laws, and treaties together shall be the supreme law of the land
-Short Selling: Borrowing a share, selling it for a price. Then if the stock price drops, you buy it back for a lower price, and thus a profit, and give the share back. Profit off of stock declines.
-Late-2000s Global Financial Crisis: (***INCOMPLETE***)
    -Causes: (***INCOMPLETE***)
-Evolution Support:
    -Classification: Organisms naturally fit into groups, e.g. humans naturally fit into the ape group, apes naturally fit into the primate group, then into the mammal group, the vertebrate group, the animal group, etc. This can easily be explained if we say that humans shared a recent common ancestor with all the other apes, then a more distant common ancestor with the other primates, then a more distant common ancestor with the other mammals, etc. 
     It's not just humans; all organisms fit naturally into groups that show a kind of branching pattern, like a family tree. The most logical explanation for this is common ancestry - groups that seem more closely related shared a more recent common ancestor.
    -Homologous structures: The human arm, the bird's wing, the whale's flipper and the cat's leg all share the same basic pattern of bones (one long bone, a joint, two small bones, a wrist made of lots of little bones and then five digits on the end) , even though they serve very different functions (human arms for grasping, bird wings for flying, whale flippers for swimming and cat legs for walking.) If they are built for very different tasks, why do they have 
     the same basic structure? The answer is that humans, birds, whales and cats all share a common ancestor who had that kind of limb, and that over time, it's been modified to fit the needs of those four very different creatures. (There are many, many other structures too that have different functions but the same basic underlying structure, showing that they came from modifications from a common ancestor. These are known as homologous structures.)
    -Genetic evidence: Perhaps the most powerful piece of evidence for evolution. Organisms that share more recent common ancestors, and are therefore more closely related, have more of their DNA in common. Humans, for example, share about 99.9% of our DNA with other humans, 98% with chimpanzees, 93% with rhesus monkeys, about 90% with mice, and so on... we share 50% with bananas! From this evidence we can draw a branching tree of who's more closely and less closely related to humans. 
     Doing the same thing for other species allows us to work out who's related to who, and we get the same kind of tree we get from comparing similarities and differences (evidence 1) or looking at anatomical structures (evidence 2). Moreover, this is the same sort of technology used to convict criminals or in paternity tests - checking the similarities in DNA to find out who's related to whom.
    -ERVs: ERVs, or endogenous retroviruses, are viruses that insert small bits of genetic code into the genome of the organism they infect. Biologists can easily see where an ERV has injected a bit of its own code, like a tag or mark. If ERVs infect a sperm or egg cell, then that genetic mark will be passed down to the children produced by that cell. By looking at the genetic markers left by ERVs in our genomes, we can therefore tell who shares a common ancestor with whom. And, of 
     course, we see the same pattern - all humans share most of their ERV markers, some of them with chimpanzees, fewer of them with gorillas, etc.
    -Embryology: In the first few days of development, a human embryo looks very similar to that of any other animal. After a few weeks, we can tell it's a vertebrate animal, but it doesn't yet look human - it could be a fish embryo! After a few more weeks, it looks like a mammal embryo, but we can't tell which mammal... and so on. Embryos of different species look very much alike in their early stages. The later on the embryos begin to look different, the more closely related the two species 
     are. Further evidence comes when we look at some of the features embryos possess in their early stages. For example, at one point, human embryos have gill slits, like a fish (although never functioning gills), then later on a long tail, like most animals, then a yolk sac, like the eggs of birds and reptiles, and later still we become covered in fur like most mammals which we shed. Why would humans go through phases with gill slits, tails, yolk sacs and fur if none of these features are of any 
     use to the adult? It's because our ancestors at one point had gills, tails, eggs with yolks, and fur (not at the same time, though!) We humans have kept those genes, and while they're inactive in adults, their effects show through at an early stage in development. And it's not just humans - embryonic snakes grow legs, which they lose, and horses grow five toes, of which four eventually shrink leaving behind the single hoof.
    -Vestigial organs and behaviors: These are organs that an organism's ancestors used for one purpose but either serve a different purpose or are not used at all in the modern organism. A good example's the human tail bone or coccyx, which serves no function in humans but is what remains of our monkey's tail, or our appendix or wisdom teeth. There are vestigial behaviors, too: for example, if a human baby falls backwards it will automatically grasp something, perhaps a throwback to our days when 
     falling out of trees was a concern. Or consider goosebumps, when our hair stands on end - not much good in warding off an enemy or trapping heat nowadays, but very useful for a furry mammal, which is why we get them when we're scared or cold. It's not just humans - whales for example have vestigial pelvises, although they have no legs for them to attach to.
    -Atavisms: These are "evolutionary throwbacks," mutations that re-awaken old, deactivated genes and produce characteristics that an organism's ancestors had, e.g. humans with tails or fur.
    -Pseudo-genes: These are strands of DNA with no discernible function. Closely related organisms, like humans and chimps, or lions and tigers, share large amounts of this "junk DNA" with each other even though it doesn't seem to give them an advantage - and again, the pattern of who has the most junk DNA in common fits together well with the family trees (or phylogenetic trees) established from other evidence like anatomy, embryos, ERVs, etc. The only way to explain this again seems to be common ancestry.
    -Molecular evidence: It's not just DNA that closely related organisms have in common; other molecules, such as cytochrome C, can vary from organism to organism and still be functional. More closely related organisms have the most similar cytochrome C. The same is true of blood proteins and other substances too.
    -Human chromosome 2: Here's something that's specific to humans. The other great apes have a pair of chromosomes that humans don't. Where did it go? The only way evolution can explain it is if two of the great ape chromosomes fused together to make a human chromosome. When we look closely at the structure of the human chromosomes and those of the other great apes, we can pinpoint the exact place this happened - where two great ape chromosomes fused to make human chromosome #2.
    -Ring species: For example, the Larus gulls; the Larus gulls in Europe can breed with those slightly different gulls in Asia, those in Asia can breed with those slightly different gulls further along in Asia, who can breed with those further along, and so on until we reach the east coast of North America. But those on the eastern coast of North America cannot interbreed with those on the west coast of Europe - they're too genetically different. This shows how it's possible for a species to change, 
     very gradually, until it becomes a new species. Ring species like this show in space what must happen in time.
    -Progression of the Fossil Record: When we look at the fossil record, we see organisms appearing in a sequence. For example, fish appear around 500 million years ago, amphibians around 360 million years ago, reptiles around 300 million years ago, early mammals around 200 million years ago, primates around 60 million years ago, apes around 30 million years ago, two-legged upright apes around 5 million years ago and humans around 200,000 years ago. Why no humans in rocks 500 million years old? The answer 
     must be that humans evolved later. If humans evolved from apes, which evolved from primates, from mammals, from reptiles, from amphibians, and from fish, then the fish needed to appear first, followed by the amphibians, the reptiles, etc. The sequence of fossils in the rock record perfectly matches what we need to explain the tree of life.
    -Transitional Fossils: Fossils that show a transition from one group to another group, e.g. Archaeopteryx, the reptile-like bird; Tiktaalik, the amphibian-like fish; Australopithecus, the human-like ape, etc. There are many, many, MANY more - just do a quick Google search!
    -Age of the Earth: Darwin's theory of natural selection tells us that over time, and if the environment changes, it's inevitable that organisms will evolve and adapt to fit it. Countless pieces of evidence mainly from geology but also from physics, chemistry and astronomy show the Earth is ancient - 4.5 billion years old - and life is at least 3.5 billion years old. Given that natural selection and mutation must happen, it's absurd to think that organisms could have stayed the same for that length of time - evolution must have occurred on such an old planet.
    -Evolution of bacteria: Many bacteria have evolved resistance to antibiotics through natural selection. Other bacteria have evolved the ability to digest nylon, which was only invented by humans in the 20th century. Why do we observe such changes happening so quickly in bacteria? Because they have short generation times - bacteria can produce a new generation in 20 minutes, while it takes more than 20 years for humans. This is why we have to wait for hundreds of thousands of years to see human evolution in action,
     but we can watch bacteria evolve over weeks or months in the lab or in nature.
    -Peppered moths: A case study in natural selection. The white form of the peppered moth was the most common in England until that country industrialized. As tree trunks turned darker with soot and industrial pollution, white moths were more easily seen by predators, and so the darker moths were more likely to survive and reproduce. Over time, the population of moths grew darker over time. This is evidence that natural selection drives the change in the gene pool of populations that we call evolution.
    -Geographical distribution of species: The way species are distributed tells us a lot about their evolution. For example, marsupials only exist in Australia and South America, although they could adapt to climates elsewhere. Why so restricted? It only makes sense if marsupials first evolved back when Australia and South America were connected to each other (and Antarctica, but not the rest of the world.) As the two continents broke apart, marsupials diversified on them, but never had the chance to spread anywhere else. (Marsupials in Antarctica would have died out 
     as the continent froze over.) The same is true of many species; their geographical distribution only makes sense in evolutionary terms.
    -Domesticated animals: These animals have been modified by humans over the years selecting who gets to breed and who doesn't. If humans can change an animal's gene pool through selection and produce such radically different types, why can't natural selection do the same thing?
    -Sub-optimal design: Evolution cannot work from scratch, it can only modify/tweak pre-existing organisms' features for new needs. This is why so many parts of organisms' bodies aren't as efficient as they could be. For example, the blood vessels in the human eye, or the path taken by the human testes as they descend into the scrotum. If they were designed from scratch, there would be more efficient ways to do it; evolution had to make do with what was already there and tweak it to fit the new environments.
-Takeovers in business:
    -Friendly takeovers: The bidder informs the board of directors before making a bid. The board of directors can recommend or deny the shareholders the accepting of the offer. In a private company, since the Board members and shareholders are usually the same, this is not required and a straightforward decision is made.
    -Hostile takeovers:  Company's board rejects offer, but bidder continues to pursue.  Techniques: 
        -Tender offer: The bidder offers the shareholders a tender offer, a public offer at a price above the market price (i.e. if a company's stock was $50, a bidder could offer $60 to the shareholders).
            -Creeping Tender Offer: Purchasing incremental amounts of a company's stock on the public market until one has enough power to effect a change in management to one that would accept a takeover offer. 
        -Proxy fight: Bidders try to convince a simple majority of the shareholders to replace the current management with one that will accept the offer. 
     Problems with hostile takeovers: a friendly bidder has access to all of the company's finances and plans and can make accurate decisions. A hostile bidder has limited public-only information about the target, and thus may be vulnerable to unknown risk. Because of this fact, banks are often unwilling to lend to hostile bidders.
    -Reverse takeovers: Private company acquires a public company, usually to float (get enough stock on the public market to be non-volatile) itself without an IPO.
    -Backflip takeovers: A company turns itself into a subsidiary (Company partly owned or wholly owned by a parent company (another company that owns more than half its stock)).
-Nuclear fission power: Fuel: enriched uranium, in inch-long pellets, diameter of a dime. These pellets are arranged into rods, which are then arranged into bundles of uranium rods.
        -Uranium rods left in water. With no human intervention, the uranium would just overheat and melt. In a power plant, control rods made of lithium, or any material that absorbs neutrons, are lowered into the water. 
        -Raising and lowering the rods allows one to control the fission reaction; to reduce heat they are lowered, and to speed up the reaction they are lifted (slightly, not completely, in both cases). Complete immersion of the rods shuts down the reaction fully, as is done in the event of an emergency.
        -The uranium rod heats the water, turning it into rising steam, which is then used to turn a turbine attached to a generator to convert the energy into electrical power.
    -Composition of a nuclear plant: Containment structure   (large cylindrical thing with a semi-sphere on top), control rods (in the middle, above the water), reactor (below the control rods, contains water and uranium rods), Pump (exit and entry tubes are installed. a pump constantly replaces hot water with more cold, until the fuel is spent.), steam generator (a large structure where the exit tube of the pump goes into. After the water is converted into steam, any excess cold water goes down another tube, which then becomes the entry tube, and the cycle continues.), a generator, a turbine, a cooling tower.
    -Protection: Concrete radiation barriers around reactor, with extra steel barrier around the reactor core; this is where refueling is done. Outer concrete structure also blocks leaked radiation, just in case, and is also strong enough to withstand an earthquake or plane crash.
    -Nuclear cross section: (of a nucleus) The probability, for a given atomic nucleus, that a sustainable chain nuclear reaction will occur. The area of the nucleus' cross section, measured in barns (1 barn = 10^(-28) square meters; denoted by lower case sigma), is directly proportional to the probability of nuclear reaction. 
        -Equation for the rate of a nuclear reaction:
         r = (phi)(sigma-sub(x))(p-sub(A)) where phi = flux of the neutron beam (reciprocal of time divided by volume), sigma-sub(x) = microscopic cross section (area of nucleus in barns), p-sub(A) = density of atoms in the target (reciprocal of volume)
-Nuclear Fusion Power: Fusing two nuclei together and harnessing the binding energy released. Range of the strong force = 2.5 proton diameters, and its far weaker competitor, the electromagnetic force, has a range dominant past 2.5 proton diameters; iron and nickel are the elements that provide the border between 2.5 proton diameters; this is why iron cannot be fused for energy by stars, since it takes more energy to fuse iron atoms than is provided due to the electromagnetic force's domination of the nucleus over the strong force due to a nucleus larger than the range of the strong force.
    -Coulomb barrier: The amount of force needed to overcome the electromagnetic repulsion between protons and get them close enough for the strong force to take effect and bind them together. 
    -Types of fusion:   
        -Proton-Proton Chain: Used by stars. First, 2 protons fuse. In the resulting diproton, a proton transforms into a neutron in beta-plus decay, releasing a positron and electron neutrino. The positron annihilates with an electron, releasing high energy photons. The now present deuterium fuses with another proton to produce a light isotope of helium, He-3. Whole process creates gamma rays and high energy hadrons. The process is very rare, as the conversion of a proton to a neutron in beta-plus decay happens very rarely, and so the first stage of the reaction is extremely slow. The vast majority of the time a proton is emitted and nothing is changed.
        -Deuterium-Deuterium: 2 Deuterium atoms -> He-3 atom + neutron.
        -Deuterium-tritium reactions: Deuterium + tritium -> He-4 + high energy neutron. Helium-3 also works as a substitute for tritium.
            -Energy dynamics: 
             Deuterium: binding energy of 2 MeV (million electron volts)
             Tritium: binding energy of 8 MeV
             Helium-4: binding energy of 28 MeV
             Relativistic Neutron: 0 binding energy
            Input = 10 MeV, output = 28 Mev; 18 MeV net gain per reaction.
    -Conditions for fusion:
        -Very high temperatures of about 100 million K.
        -High pressure; enough to get protons 1E(-15) m apart.
    -Achieving conditions necessary for fusion:
        -Magnetic confinement: Uses electromagnetic fields to exert enough pressure to induce fusion. Used by ITER.
            -Microwaves and neutron beams heat hydrogen gas into plasma (because plasma is ionized and therefore subject to electromagnetic forces). Superconductor magnets compress plasma into a doughnut shape, inducing fusion.
            -Structure of a Tokamak (doughnut shaped nuclear fusion reactor): Vacuum Vessel: holds plasma during reaction, is a vacuum.
                                              Neutral Beam injector: high energy particle beam that heats plasma to critical temperature.
                                              Magnetic field coils: superconductor magnets to confine plasma. 
                                              Transformers: Power the magnets.
                                              Cryostat: Cryogenic temperatures needed to cool the device.
                                              Divertors: exhaust helium port.
                                              Lithium blanket: High energy neutrons, rather than being wasted (since they have no binding energy) collide into a lithium blanket to decay into tritium (tritium breeding).
                -http://www.iter.org/mach
                -70 megawatt input energy, 500 megawatt output energy. 300-500 second reaction.                                   
        -Inertial confinement: Uses multiple lasers or ion beams to focus on a single point from multiple orientations in order to induce fusion. Experimented with by National Ignition Facility at Lawrence Livermore Lab in USA.
            -192 lasers focus on one point within the hohlraum (a cavity whose walls are in radiative equilibrium with the radiant energy within the cavity). There is a pellet of deuterium-tritium at the focal point. Lasers input 1.8 million joules for the deuterium-tritium. This energy heats the hohlraum and causes it to emit x-rays. Heat and radiation compresses deuterium-tritium into plasma and induces fusion. 
            -1 microsecond reaction (1 millionth of a second). Net gain of 50-100 times input energy.
    -Benefits to nuclear fusion:
        -Abundant fuel: 25 tons of helium-3 on the moon, enough to power the world for 500 years. Deuterium is extractable from seawater (heavy water). Tritium can be bred from lithium. 
        -Safe: reaction is fully controlled, technology provides multiple failsafes.
        -Clean: No air pollution, almost no radioactive waste (no more then background levels).
        -Cost effective: Net energy gain is massive.
-Types of Nuclear Reactors: (***INCOMPLETE***)
-The Myth of Nuclear Deterrence: Nuclear deterrence (the Cold War theory that asserts that because one nation has nuclear weapons, other countries will refrain from attacking with nuclear weapons due to fear of nuclear retaliation, resulting in MAD) is false.
    -Problems with nuclear deterrence:
        -We have already been narrowly close to nuclear war (Cuban missile crisis) and we can't afford to take chances with nuclear war.
        -Nuclear deterrence is ineffective against extremists and suicide-warfare type groups, should they ever obtain nuclear technology.
        -Nuclear deterrence is ineffective against enemies that cannot be located, such as many terrorists.
        -It presupposes that other countries are rational actors, when in reality, some are not or may degrade into irrationality with the threat of war hanging over them.
        -If it's so effective, why do we continue to invest in missile defense systems? These systems are admissions that nuclear deterrence is unreliable.
        -If we continue to build nuclear weapons in hopes of deterrence, more countries will get them and nuclear weapons will proliferate.
            -It could be argued that countries would get the weapons anyways, so why not keep them ourselves in case?
    -Therefore, the only way to avoid nuclear war is to place materials vital to the creation of nuclear weapons under international control.
        -Nuclear weapons convention: proposed international treaty that would prohibit the development, testing, production, stockpiling, transfer, use and threat of nuclear weapons, as well as provide for their elimination. 
            -International poll shows globally 76% favor of NWC.
            -France, Russia, UK, and USA oppose the NWC 
            -China, India, Pakistan, and North Korea support the NWC.   
-Shelf corporation: Company started on paper, but it metaphorically put on a shelf - it has no activity at all for a period of time, until someone who wants to start a company buys the shelf corporation, essentially still a brand new business since it has no activity, but can instantly have corporate longevity, thereby attracting more customers. It also saves time, gains access to corporate credit, and provides the opportunity to bid on contracts.
-IMF: International Monetary Fund, founded in 1945 with the purpose of "stabilizing exchange rates and assisting the reconstruction of the world's international payment system", right after WW2. 188 nations members.
    -Through a quota system, countries contribute money into a pool. Countries with payment imbalances can temporarily borrow money from this pool. 
    -The IMF works to improve the economies of its member countries.  
    -Stated Objectives: 1. Promote international economic cooperation.
                2. Promote international trade.
                3. Promote global employment. 
                4. Maintain exchange rate stability.
                    -By making financial resources available to the member countries in order to ensure that they all meet balance of payment needs (accounting records of all monetary transactions between a country and the rest of the world. This includes exports, imports of goods, imports of services, imports of financial capital, and financial transfers).
-Statistical significance: a result that is unlikely to have occurred by chance, and thus provides enough evidence to reject the default hypothesis "no effect".
    -Sigma: statistical significance in terms of standard deviation. 
        -Calculation: alpha = 1 - erf(n / sqrt (2))
                  where alpha is the probability, erf is the error function erf (x) = 2/sqrt(pi) * integral from 0 to x of (e^((-t)^2) * dt); this function actually has nothing to do with errors
-Thermobaric Weapon: (AKA Fuel-air bomb) explosive weapon that produces a shock wave lasting significantly longer than that produced by conventional explosives; useful for maximum casualties and damaging structures. Relies on oxygen (whereas conventional explosives have oxidizers (gunpowder = 25% fuel, 75% oxidizer)); this makes it useless underwater, high altitude, adverse weather.
-SLBM: Submarine launched from a submarine.
-MIRV: Multiple independently targetable re-entry vehicle: Collection of separate warheads in one missile. (Contrast: unitary missile: one warhead: one missile).
    -Purposes: (***INCOMPLETE***)
-ICBM: Intercontinental ballistic missile (ballistic missile: missile following ballistic flight paths; guided only in the brief initial phase) is a missile that can travel great distances (3500+ mi), mostly used to carry nuclear warheads.
-Warheads: A destructive material delivered by a rocket, missile, or torpedo.
    -Explosive: explosive charge disintegrates target, damages surroundings with shock wave.
        -Conventional: Gunpowder or high explosives (explosives that detonate; shock wave passes through material at supersonic speed, usually 3-9 km/s) very quickly break molecular bonds and release lots of energy.
            -Blast: shock wave.
            -Fragmentation: shrapnel with high kinetic energy pierces target.
            -Continuous rod: A hollow cylinder of interconnected metal rods (welded at the tops) is violently expanded (by explosion) into a zig zag pattern, producing a guillotine effect that is particularly useful anti-aircraft, as aircraft are resistant to shrapnel.
            -Shaped charge: the explosive is focused into a shaped metal liner, projecting the metal at a hypervelocity (3000 m/s) to penetrate heavy armor. 
                -Explosively Formed Penetrator: (EFP) a shaped charge is directed at another concave metal plate that deforms into a projectile mid air. Travels at super hypervelocity.
        -Nuclear: nuclear fusion or fission reaction causes tremendous energy release
    -Chemical: Toxic or highly acidic/basic chemical used.
    -Biological: Disease spread.
        -Entomological: use of bugs in warfare to directly attack enemy soldiers, destroy food supplies, or to spread a disease.
    -Kinetic: Pure motion.
-Nuclear weapons: Two types: fission (atomic bomb) and fusion (thermonuclear weapon; hydrogen bomb, fusion weapon)
    -Explosion creates a luminous fireball consisting of ionized matter and a powerful shock wave. Also produces radiation. 
    -So far, both the US and Russia (or USSR before 1991) have maintained a second-strike capacity (neither can attack powerfully enough to prevent an effective counterstrike), resulting in the threat of MAD (mutually assured destruction).
    -Fission Weapons: About 0.1% of the mass of the nucleus is converted into energy. Three fissionable elements used commonly: U-235, U-238, Pu-289. 1 kg of of U-235 (size of a golf ball) releases as much energy as does 17,000 TONS of TNT.
     Essentially, there are two methods. 1) The gun-type method: (used in Hiroshima bomb) A sub-critical mass of uranium is placed at the "barrel" of a gun, with another sub-critical mass in the barrel. Conventional explosives explode, with the shock wave propelling the sub-critical mass of uranium into the other one, creating a supercritical mass. This allows for an uncontrolled and self-sustaining nuclear fission reaction. 2) (used in Nagasaki bomb)(called implosion-triggered bombs) A sub-critical mass of plutonium, usually in a sphere, is 
     surrounded by conventional explosives, specifically, shaped charges. When detonated in unison, the charges compress the sub-critical mass of plutonium into a small enough volume for the sub-critical mass to become supercritical, leading to an uncontrolled and self-sustaining nuclear fission reaction. This method is far more difficult to design, as the explosives must project the exact same force in every direction against the sphere.
        -Critical Mass: the mass at which nuclei are close enough together (or numerous enough) for chain reaction to be probable enough to self-sustain.
            -Activation energy: The mass-energy of the uranium nucleus is greater than the sum of the mass-energies of the two sub-nuclei that result after fission (which is where the released energy comes from), but the reaction is not spontaneous because an initial input of energy is required before fission, ie the activation energy. This comes from the distortion of the uranium nucleus during the splitting process. Before the nucleus breaks into two nuclei, it must first expand into a dumbbell shaped nucleus that will then split. This dumbbell shape has more surface area than the initial Uranium nucleus and thus more mass-energy, and the difference in the (higher) mass-energy of the distorted Uranium nucleus and the rest Uranium nucleus is the activation energy required. For uranium-235, this is about 6.2 MeV. 
                -This is why U-238 doesn't readily fissile. Even with the bombardment of a thermal neutron, U-239's distorted nucleus only gains 4.5 MeV whereas the activation energy required is still about 6.2 MeV.
        -Neutron generator: small pellet of polonium and beryllium, separated by foil within the fissionable core.
            -In the generator: 1. Foil is automatically broken when sub-critical masses collide or compress, as the process releases alpha particles that break the foil.
                       2. Alpha particles + Beryllium-9 produce Beryllium-8 + free neutrons. Free neutrons initiate fission.
        -Tamper: a dense material with the purpose of allowing as much material to fission as possible before the bomb explodes.
            -Usually made out of Uranium-238. Tamper is heated up, then expanded by the fission core (during fission). The tamper thus exerts pressure back on the core, slowing the expansion. 
            -Also reflects free neutrons back into the core, increasing efficiency.
        -Weapons-grade uranium must be at least 90% U-235
        -Boosters: Using fusion reactors to create neutrons, which are used to induce fission at a higher rate. 
    -Fusion Weapons: Fusing nuclei to form a heavier atom, releasing enormous energy.
        -Problems: 1. Fuel, deuterium or tritium, is gaseous, and therefore hard to store. 
            -Solved by using lithium-deuterate (solid compound. it doesn't undergo radioactive decay at normal temperature.)
               2. Tritium has a short half-life.
            -Solved by using a fission reaction to produce the necessary tritium from lithium, instead of storing it.
               3. High compression, pressure, and temperature required to initiate fusion.
            -Fission reaction (mentioned in the solution to the half-life problem) releases x-rays, which provide the high temperatures necessary.
        -Composition of the bomb: At the top part of the bomb is an ordinary implosion-triggered weapon:
                            -This section contains a hollow Pu-239 pit in the center, deuterium-tritium gas (for boosting) in the center of the hollow plutonium pit, a U-238 tamper surrounding the pit of fissile material, separated from it by a vacuum, and finally, specifically placed explosives around the pit.
                           A cylinder of x-ray reflectors, typically made out of uranium; rather than reflecting radiation, the reflector works by (1) absorbing the heat from the primary and emitting the heat in x-rays to the secondary.
                            -The primary and secondary are at opposite ends of the cylinder; the empty spaces are filled with a material opaque to x-rays, such as polystyrene foam.
                           At the bottom part of the bomb is the secondary fusion component. It is comprised of a rectangularly shaped plutonium sparkplug (a sub-critical mass due to its shape) surrounded by the fusion fuel (lithium-6 deuteride), which is all surrounded by a U-238 tamper.
        -Process of the Teller-Ulam design: 
            1. The primary component, an implosion-triggered plutonium based fission weapon, is detonated.
                -a. Conventional explosives are detonated, compressing the plutonium pit by a factor of 2 to 4, converting the sub-critical mass to supercritical. 
                 b. A pellet of polonium and beryllium is separated from the mass by aluminum foil; upon reaching supercriticality, the plutonium releases a shockwave that causes the polonium to spontaneously release alpha particles. These alpha particles react with beryllium-9 to produce beryllium-8 and free neutrons.
                 c. The free neutrons initiate fission within the supercritical mass. 
                 d. Boosting: 50%-50% tritium-deuterium gas undergoes fusion from the high energy of the surrounding fissioning plutonium and releases even more neutrons, improving efficiency.
                 e. A U-238 tamper surrounding the entire mass is expanded by the energy released by the fissioning mass and exerts pressure back on the fission fuel, preventing premature explosion and ensuring that the bomb explodes when a maximum amount of fuel has been fissioned. The tamper also reflects neutrons back into the mass, increasing efficiency.
                 f. The weapon explodes; the U-238 tamper also undergoes fission, releasing even greater amounts of energy.
            2. The uranium x-ray reflectors absorb the 100 terajoules of energy (for a 30-kiloton bomb) contained in x-rays from the fission bomb. The energy is released onto the secondary stage.
                -The spectrum of the fission bomb is that of an approximate black body at a temperature of 50,000,000 K. 
                -The amplitude of the flux of the x-ray release is in trapezoidal pulses, with a one microsecond rise time, a one microsecond plateau, and a one microsecond fall time.
            3. Radiation-powered ablation-driven implosion: Intense heat (generated by x-rays) vaporizes the shield (on the exterior of the secondary), causing the pieces to fly apart with great velocity. This exerts huge amounts of pressure on the lithium deuterate (within the secondary).
                -Another reason why it's important for the exterior layers of the shield to fly off as they go; the shield maintains a temperature difference between the interior of the secondary and the rest of the bomb, allowing the maximum power transfer when the shield does vaporize.
            4. The sparkplug and fusion fuel are compressed by a factor of 30; the sparkplug (of U-238 or Pu-239) goes supercritical and undergoes fission.
            5. The shock waves from compression, intense heat from the radiation-implosion, and energy release from the fissioning sparkplug initiate fusion. 
                -Tritium breeding: Neutrons produced from the fissioning sparkplug react with the lithium to produce tritium. Tritium gas is difficult to store and has a short half life, so producing it during detonation is ideal.
            6. Fissioning rod, now finished exploding, gives off radiation, heat, and neutrons.
            7. The neutrons from the fusion reaction and any excess neutrons from the fissioning sparkplug induce fission in the U-238 tamper. 
            8. The fissioning of the tamper produces even more neutrons and heat, increasing efficiency of the fusion reaction and releasing massive amounts of energy.
            9. All the x-rays released, accounting for a proportion of the total energy release, can be transferred to a tertiary stage, with the same composition as the secondary, if a third stage exists, and so on, to increase the weapon yield.
                -Tsar Bomba was a 3-stage thermonuclear weapon, but 3 stages is rare as energy releases on the order of 50+ megatons is usually unnecessary.
            10. The bomb explodes. Entire process takes 600 nanoseconds (nanosecond = billionth of a second) and can be hundreds to thousands of times more powerful than a fission weapon.
    -Nth Country Experiment: In 1964, the US government tried to test how much effort was needed to construct a working nuclear weapon using only information available on the public domain (materials must be obtained by the subjects); three graduate students completed the project in 3 years.
-Biological warfare: The application of toxins and agents capable of damaging the internal biological systems of humans, animals, or plants to warfare. Specifically, bacteria, fungi, or viruses may be used to transmit deadly pathogens to the enemy population as an act of warfare. Biological weapons have uses as direct confrontational weapons against the enemy to kill or incapacitate the enemy military forces, but they can also be used as area denial weapons (weapons that prevent the enemy from occupying an area of interest) or as leverage in negotiation (the weapons can be threatened to be used to produce cooperation from the enemy). Under the international Biological Weapons Convention of 1972, bio-weapons are illegal, largely to prevent massive and unwanted civilian casualties during war and the spread of a deadly epidemic even long after the war's conclusion. 
    -Effectiveness: Though not necessarily the most destructive of all weapons, due to problems such as survivability of the vectors used, ease of dispersion, the fact that bio-weapons can take days to even begin taking effect, etc. However, relative to the cost of development and employment and to their mass, biological weapons are far more destructive than nuclear, chemical, or conventional weapons. However, in addition to the problems mentioned, te weapons can be difficult to control during dispersion; airborne pathogens in particular have the danger of unintentionally spreading to the civilian population or even to neutral or friendly forces. 
    -Use in offensives: Many viruses and pathogens are incredibly difficult to control and would almost certainly infect friendly forces or even spread worldwide (eg smallpox). However, bacteria (eg anthrax) can be carefully modified to only survive in a very narrow environmental range (eg a specific temperature range, at a certain level of humidity, etc.); this would help prevent the agent from spreading worldwide and to friendly forces (assuming they are outside the environment of attack). An ideal agent would have high infectivity (how effectively the pathogen spreads from host organism to organism) and high virulence (how quickly the pathogen acts and how deadly it is to the infected). 
        -Example of anthrax (Bacillus anthracis): Anthrax has characteristics of an ideal pathogen, but comes with its own host of problems. First, the pathogen hardens in spores that are very easy to disperse. Second, anthrax is not contagious (in the sense that it does not transfer from person to person) and so the application to biological warfare of anthrax would not cause unwanted destruction. Third, anthrax has a fatality rate of 90% or higher in the untreated infected population, and kills in 3 to 7 days. Fourth, because antibiotics exist, friendly forces would be protected in case of a backfiring. However, problems include the size of dispersion agents (1.5 to 5 micrometers in diameter), which causes the spores to stick together due to electromagnetic forces, hindering dispersion, and also susceptibility to damage from ultraviolet radiation or the climate (eg rain). It can also be difficult to store. 
    -Genetic warfare: (***INCOMPLETE***) 
    -Entomological warfare: A type of biological warfare that uses insects. The insects can be applied in three general ways. First, the insects can act as biological vectors by being infected with a disease and then released over the enemy population to spread the disease amongst the ranks. Second, they can be used to target agriculture and food sources of the enemy, dwindling their resources. Third, they can be used in direct confrontation with the enemy, such as the use of bees to sting enemy soldiers.
        -Operation Big Buzz: Among other operations, Operation Big Buzz was an operation in the US that occurred in May of 1955. The US had built a factory scheduled to produce 100 million mosquitoes infected with yellow fever per month. To test whether the mosquitoes would survive their dispersion drop and if they would actively seek food and be effective in biting the population, the US dropped 300,000 mosquitoes (uninfected) on its own population in the state of Georgia.
-Inverse-square law: Radiation (including heat) diminishes at a factor of the inverse square of the distance from the hypocenter.
             Blast pressure diminishes by an inverse factor of the cube of the distance.
-The Workers' Plea: 1. Tell me clearly what you expect of me.
            2. Give me the opportunity and resources to perform.
            3. Let me know how I am getting on.
            4. Provide me with guidance, support, and training when I need it.
            5. Reward me according to the contribution I make to the business.
-Drag: Forces acting on a solid object moving through a fluid. The force is a frictional force and so acts in the opposite direction of motion, and also is affected by the object's velocity. 
    -Reynolds Number: The ratio of inertial forces (the amount that the fluid resists changes in its motion) to viscous forces. Low Re (<2000) mean the fluid has laminar flow (steady) and high Re (>2000) means the fluid has turbulent flow (unsteady fluctuations).
        -Reynold's number = ((rho)(v)(L))/(mu) = ((v)(r))/(mu); rho = density, v = velocity, L = length, mu = viscosity, and r = radius of pipe if flow is in pipe.
        -In laminar flow, high velocity fluid flows in the center of a pipe and the low velocity fluid remains on the outside, restrained by inertial forces. 
    -Drag Equation: At high velocities, with Re > 1000, the force on a moving object exerted by the fluid:
        - F_(D) = 0.5(rho)(v^2)(C(_d))(A) where rho = density of fluid, v = velocity of object relative to the fluid, C_(d) = drag coefficient, and A = reference area.
            -Drag Coefficient: dimensionless number that quantifies the amount of drag caused by a fluid. 
    -The velocity for an object that begins moving from rest through a non-dense fluid is given by the function:
        -v(t) = sqrt( (2mg)/(rho*A*C_(d)) ) * tanh(t*sqrt( (g*rho*C_(d)*A)/(2m) )) where m = mass of the object and g = acceleration due to gravity (~9.81 m/s/s)
    -Stokes Law: Describes the drag force on spherical objects with very low Re. 
        -F_(d) = 6pi*mu*R*v_(s) where mu = dynamic viscosity, R = radius of the (spherical) object, and v_(s) = object's (settling) velocity
            - Dynamic viscosity:    
-Police Power: (in constitutional law) the capacity of states to regulate the behavior and enforce order within their territory for the betterment of of the general welfare, morals, health, and safety of their inhabitants. 
-Statute of Limitations: the maximum amount of time after an incident that a lawsuit based on said incident can be filed. After the statute is expired, the right to lawsuit is permanently and forever dead, or barred. 
    -Differs per incident/crime as well as per state
    -Common bases:
        -Personal injury: 2 years. 
            -If the injury was not discovered immediately: 1 year after date of discovery
        -Breach of written contract: 4 years from date of breach.
        -Property damage: 3 years after incident occurs.
        -Claims against government agencies: Claims must be filed within 6 months of incident. 
            -If the claim is denied, a lawsuit can then be filed in court.
    -Doesn't apply in cases of fraudulent concealment (of evidence). So if new evidence comes up that was concealed at the time of lawsuit in court, the case can be reopened even if the statute of limitations has expired.
-Objections in court (US law):
    -Ambiguous: Question isn't clear enough for the witness to answer.
    -Arguing the law: Counsel is instructing witness on the law.
    -Argumentative: Question makes an argument rather than actually asking a question.
    -Asked and Answered: When SAME attorney asks a question which has already been answered.
    -Asking Jury to Prejudge: The jury cannot make any decisions during questioning, but the counsel asks them to.
    -Assumes facts not found in evidence
    -Badgering: Antagonizing the witness to provoke a certain response 
        -Mocking the witness or asking rapid fire questions without giving the witness chance to respond
    -Calls for a conclusion: Question asks for an opinion, not facts.
    -Calls for speculation: Question asks witness to guess rather than rely on facts.
    -Compound question: Multiple questions asked in conjunction without giving the witness a chance to respond to each individually.
    -Hearsay: Witness doesn't know the answer but rather, heard it from another source.
    -Incompetent: Witness is not qualified to answer the question.
    -Inflammatory: Question intended to cause prejudice. 
    -Leading question: Question suggests a answer to witness.
        -Only applies in Direct Examination
        -Hostile witness: a witness that is hostile to the questioner's side of the controversy.
            -Hostile witnesses may be asked leading questions during direct examination, the only exception to the rule against leading questions.
            -Leading questions may be asked during cross examination, as the witnesses are considered hostile anyways.
    -Privilege: Witness is protected by the law from answering the question.
    -Irrelevant: (A.K.A. Immaterial) Question is not about issues in the trial.
    -Objections to material evidence:
        -Lack of foundation: evidence lacks proof of its source or authenticity.
        -Fruit of the poisonous tree: Evidence was obtained illegally.
        -Incomplete: Opposing party is only introducing PART of the evidence, taken out of context. 
            -If sustained, the opposing party has the right to bring up additional details.
            -When the witness is presented with such a surprise document, he or she should have the time to study it before answering questions.
        -Best evidence rule: Original evidence must be presented, if available, and supersedes the asking of a witness pertaining to said evidence
    -Objections to answers by a witness:
        -Narrative: Question asks witness to recount a story, instead of stating specific facts.
        -Non-responsive: Witness is answering a question that was not asked, or not answering at all.
        -Nothing pending: Witness continues to speak on something irrelevant to the question; he or she provides extra information in the answer that wasn't specifically asked for in the question. 
-Peremptory challenge: Both litigants have the right to reject certain jurors due to unfavorable bias. 
    -Challenge by cause: Potential jurors may be asked to give a reason for why they have bias; the judge decides if the reason is a legitimate claim for violation of peremptory challenge or not.
-Direct examination: Questioning witnesses called by oneself.
-Cross-examination: Questioning witnesses called by one's opponent. Follows direct examination.
-Affidavit: Written sworn statement of fact voluntarily made by an affiant (ie a legal name for a person who has signed an affidavit) or deponent under oath or affirmation. 
    -Medieval Latin for "he has declared upon oath"
    -Basically, one swears that something is true to one's knowledge.
-Chinese wall: In law, a theoretical barrier that separates two or more groups by restricting the flow of information. It isolates persons who make investment decisions from people privy to information that might influence those decisions.
    -Relies on honor system and is determined by the meticulousness and discretion of the parties involved; it is an ethical barrier created to avoid conflict of interests.
-Purchasing Power Parity: 
    -Purchasing Power: The amount of goods or services one can buy with a UNIT of currency; the currency can be either:
        -Commodity Money: money whose value comes from a commodity (marketable item) from which it is made. E.g. Gold, silver, copper, etc.
        -Fiat Money: Money that derives its value from a set government regulation of law. E.g. US dollars, British pounds, etc.
    -Balassa-Samuelson Effect: Increase in productivity of tradables (Tradable Sector: Industry sectors whose output in terms of goods or services can be traded internationally) relative to non-tradables, if the increase is larger than in other countries, will cause the market exchange rate to appreciate.
        -Based on the Penn Effect: Real income ratios between high and low income countries are thrown off, usually exaggerated, by the GDP conversion at market exchange rates. 
                       Also, consumer price index (changes in the price of household goods and services) is systematically higher in rich countries than in poor countries.
            -Law of One Price: In an efficient market (Markets where all pertinent information is available to all participants) all identical goods have ONE price. Because this means that one good cannot have two prices in the same market, since all consumers would buy the lower price, we can infer that the international market is NOT an efficient market, because this price differences happen.
        -Based on the assumption that productivity varies more (BY COUNTRY) in the traded goods' sector than in other sectors.
    -Relative purchasing power parity: difference in exchange rates
        -E(n) = E(0) * ((1+pi-sub FC)/(1+pi-sub DC))^n
            -Where E(n) = expected exchange rate n periods ahead
                   E(0) = (p-sub FC)/(p-sub DC) = price of foreign currency divided by price of domestic currency = current exchange rate
                   pi-sub FC = rate of inflation in the foreign currency
                   pi-sub DC = rate of inflation in domestic currency
                    -Rate of inflation: change in prices on some price index, usually the consumer price index. Approximately equal to rate of decrease in PPP. Calculation: ((P-sub 0) - (P-sub (-1)))/(P-sub (-1)) * 100% where (P-sub 0) = average price level and (P-sub (-1)) = the price level a year ago
                   n = number of periods ahead
-Cope's Rule: Creatures tend to get bigger over time, by evolution.
    -Example: Horses; their ancestors were the size of dogs.
-Ramjet:  (A.K.A. stovepipe jet or athodyd) As the engine moves forward, incoming air is compressed without a rotary compresser. Relies on high speed to forcefully compress and decelerate the incoming air before combustion.
    -Works best at Mach 3
    -Scramjet: (Supersonic Combustion Ramjet): Type of ramjet air-breathing jet engine in which combustion takes place in a supersonic air flow. 
        -Challenges: Requires near hypersonic flight, which generates immense drag (difference in pressure for air particles above and below (or in front of and behind) the aircraft. At such speeds, the fuel must be combusted within milliseconds.
-Fanjet: (A.K.A. turbofan): air-breathing jet engine widely used in commercial aircraft. Has a gas turbine engine (energy added to gas stream (of air that flows into the engine), fuel mixed with air and ignited. Combustion increases temperature and causes air to expand. Products of combustion forced through nozzle at high pressure over turbine blades. This spins the turbine fan, which drives the suction fans that produce thrust), which converts the combustion of fuel into mechanical energy, and a ducted fan.
    -Like some other jet engines, fanjet uses afterburner ((A.K.A. reheat) Provides an increase to thrust. Injection of additional fuel AFTER the turbine)
-Pulsejet: (A.K.A. pulse jet engine) Combustion occurs in pulses. Low specific impulse (way of describing the efficiency of a rocket, compares the amount of propellant used to velocity)
    -Pulse detonation engine: Uses detonation waves (shock wave supported by trailing exothermic reaction. Wave travels through a highly combustible (or chemically unstable) medium) to combust fuel-oxidizer mixture.
        -Mixture must be renewed between each detonation wave, which is why the engine is a pulsejet.
        -About Mach 5 (avg speed)
-Ion thruster: A form of electric engine for spacecraft that produces thrust by accelerating ions. They are categorized by how they accelerate ions.
-Rocket Engine: Through a de Laval nozzle, propellant (a gas) is ejected at extremely high speeds.
    -Doesn't require use of oxygen. Faster than jet engines but lasts only a few minutes. High acceleration.
-Hydrogen-powered aircraft: Uses hydrogen as power source; either burned in some kind of jet engine, or used to power a fuel cell that provides electrical energy to a propeller. 
    -Normal aircraft store fuel in the wings; liquid hydrogen is typically stored inside the fuselage. 
-Motorjet: (sometimes called "thermojet" but this term refers to a type of pulsejet, completely unrelated) Ordinary piston engine drives a compressor, which drives air into a combustion chamber, where fuel is injected and ignited. 
-BrahMos-2: Cruise missile with a range of 290 km, top speed of Mach 7, fastest cruise missile in the world.
-LGM-30 Minuteman: US nuclear missile (land based ICBM). Travels at an altitude of 700 miles and at a speed of Mach 23 (15,000 mph) 
    -Only land based ICBM used by the US
    -Missile can be launched in about one minute after ordered to (hence the name)
-Most aircraft store fuel in wings.
-X-15 Rocket Powered Aircraft: Travels at a speed of 4500 mph. Fastest manned aircraft in the world.
-Railgun: A long range high powered weapon. Consists of two rails, both parallel and conductive, along which a sliding armature is placed. This armature is accelerated very quickly through the use of electromagnetic forces (Lorentz forces; they also affect the rails but the rails are welded in and so cannot move). The electromagnetic current flows down one rail, into the armature, and back the other rail. 
    -Although solid and metal armatures are usually used, plasma (a.k.a. hybrid) armatures are also used, in which an arc of ionized gas is used to push a solid, non-conducting payload down the rails at high velocity. 
-Coilgun: (A.K.A. Gauss gun) Coils surround a hollow pipe. The coils produce an electromagnetic charge which accelerates the magnetic projectile to high velocity. 
-HeLa Cells: Cells first derived from Henrietta Lacks. Cells which can divide an unlimited amount of times.
-Neurosis: Class of mental disorders involving distress (A state in which a person is unable to adapt to stressors. The resulting stress shows maladaptive behaviors) but NOT delusions or hallucinations.
    -Symptoms: OCD, anxiety neurosis, hysteria, several phobias, pyromania (impulse to start fires for gratification or relief), depression, anger, mental confusion, habitual fantasizing, perfectionism.
-Memory: Two types:
    -Declarative: (A.K.A. explicit memory) memories that can be recalled and pertain to facts, knowledge, and concepts.
        -Episodic Memory: Memory of one's own life, such as associated emotions, times, places, details, concepts, ideas, etc. 
            -Episodic learning: Change in behavior that occurs as a result of an event.
        -Semantic Memory: Memory of meanings, understandings, and conceptual knowledge. 
            -Semantic Learning: Learning new concepts as a result of known concepts.
    -Procedural: (A.K.A. implicit memory) Memory for how to do things. Procedural memories are instantly recalled and integrated into the prefrontal lobe; they play an essential role in motor skills and cognitive ability. Type of long term memory. 
        -OCD patients have exceptional procedural memory.
        -Automaticity: The ability to do things without occupying the mind with details. It becomes an habit (automatic response). Results from learning, and by extension, practice. 
    -Working Memory: (Part of short term memory, not one of the two types) 
-Tunguska event: 7:14am June 30, 1908, Krasnoyarsk, Russia: a large meteoroid or comet collides into the earth, releasing energy on the order of 10-30 megatons of TNT (0.4 - 0.8 times the power of Tsar Bomba). The shockwave measured a 5.0 on the Richter scale and destroyed 2150 square km, destroying 80 million trees. 
    -Largest impact event in recorded history.
    -The air under the meteoroid was compressed and superheated (this is what heats objects in freefall at high speeds up, not air friction) to the extent that the air began undergoing nuclear fusion.
-Chicxulub impact: The impact of a meteor somewhere beneath the Yucatan Peninsula, Mexico, during prehistoric times that wiped out the dinosaurs. The impactor had a diameter of 6 miles and collided with the earth with the energy of 1,000,000 megatons (Tsar Bomba had only 50 megatons yield). The impact caused:
    -Megatsunamis thousands of feet high.
    -Global wildfires from superheated debris displaced by the impactor.
    -An atmosphere largely consisting of dust and debris that creates a pseudo-winter.
        -This blocks of sunlight for an extended period of time, disrupting photosynthesis in the entire biosphere, driving many species, including all dinosaurs, extinct.
-Millennium Development Goals: 193 countries and 23 international organizations have agreed to meet these (8) goals by 2015:
    1. Eradicating extreme poverty and hunger
    2. Universal primary education
    3. Gender equality
    4. Reduction of child mortality rates
    5. Improving maternal health
    6. Combating HIV/AIDS and other diseases
    7. Ensuring environmental sustainability
    8. Development of a global partnership for human development
-Temporal philosophy: A Theory: Events transition from future to present to past.
              B Theory: Events are earlier than or later than one another. 

    -Presentism: Only present objects exist. Think of time not as a line, but as a single point passing through events; the events cease to exist once the point passes through them. (Moving Spotlight view)
        -Great weakness: Appears to be incompatible with general theory of relativity. Because presentism says that, at a given moment in time (the present) all set of objects exist. This contradicts the Principle of Simultaneity.
    -Growing Block Theory: Past and present exists, but future doesn't, and as future events transition to present and then past, the past-present block of existing events continues to grow.
        -Truthmaker Principle: For every true claim one makes about reality, a "truthmaker" must exist to verify it. In most cases, the truthmaker is merely something material or observable or physical.
    -Eternalism: (A.K.A. Block universe, because it refers to all of spacetime as an unchanging, 4 dimensional block) All points in time are equally real. Future events are already there, and there is no objective flow of time.
-Sea squirts: born with a brain and nervous system, but once they find a nice rock to settle on (and they will never, ever, leave this rock), they digest their brain and nervous system for food. Without the need for movement, the brain becomes unnecessary.
    -Clinching evidence that the primary evolutionary function for the brain is to provide advantageous physical movement.
-Kilometers to Miles conversion closely parallels the Fibonacci sequence.
    -3mi = 5km, 5mi = 8km, 8mi = 13km   
    -To quickly convert: Ex: 100 mi = 89+8+3 (all Fibonacci numbers) and the next three numbers in the sequence, respectively, are 144, 13, 5, which adds to 162km. The actualy answer is 161km.
-Buckminster fullerene molecules (carbon alotropes made of 60 carbon atoms)
    -Study in Universite Paris Sud, France: Experimentation on rats involving bucky balls:
        1. Control group of rats 
        2. One group fed olive oil
        3. One group fed olive oil with 0.8 mg/mL concentration of Buckminster fullerene
     Median lifespan (for control and in general): 22 months
     Olive oil group median lifespan: 26 months
     Bucky ball group median lifespan: 42 months
    -Caused by a reduction in oxidative stress (imbalance in cells that contributes to aging).
    -http://gizmodo.com/5902703/bucky-balls-could-double-your-lifespan
 
-Nootropic: Drugs (or supplements, etc) that improve mental functions, i.e. cognition (decision making, problem solving, etc), memory, intelligence, motivation, attention, and concentration. Work in 3 main ways: 1. Alter brain's supply of neurotransmitters.
                                                   2. Vasodilation: Improve brain's oxygen or blood supply.
                                                   3. StimuLating nerve growth.
    -Cognitive enhancers (drugs that enhance concentration and memory) are INCLUSIVE of nootropics (neuroprotectants (offers treatment for the central nervous system disorders, slows neurological disease progression). All nootropics are cognitive enhancers, but NOT vice versa.
    -Little to no side effects. 
        -Racetam Family: Nootropic drugs that share a pyrrolidone nucleus (organic compound consisting of 5-lactam, usually a colorless liquid)
    -Ampakines: Compounds known to expand memory, enhance attention span, and boost the learning curve (inversely the forgetting curve).
    -Analeptic: drug that acts as a stimulant for the central nervous system. 
    -Common examples:
        -Piracetam: Improves memory, improves non-associative learning (See below; "Non-associative learning"), decreases stress.
        -L-Acetylcarnitine: (LAC) Increases IQ, improves memory, effective in treating dementia, lowers frequency of negative feelings and increases motivation. 
            -These favorable effects persist even after LAC is discontinued.
            -No adverse side effects shown in test.
        -Arginine Vasopressin: (Vasopressin or AVP) Neural hormone found in most mammals, plays a role in cerebral cortex for humans. Can enhance memory function.  
    -Dumb drugs: drugs that impair cognitive function
        -Many antidepressants are dumb drugs, as they are anticholinergic (Anticholinergic agent: a nerve agent that inhibits acetylcholine, negatively affecting cognitive processing)
-Associative vs Non-associative learning:
    -Associative learning: association is forged between a stimulus and behavior (or, less commonly, two stimuli). By reinforcing (positively or negatively) or punishing (punishment or extinction)
        -Classical conditioning: (A.K.A. Pavlovian conditioning) Unnatural responses to stimuli are repeated over time, i.e. learned. 
            -Unconditioned Stimulus: (UCS) something that naturally evokes a natural response (e.g. with Pavlov's dogs, meat powder naturally evoked salivation; meat powder is the UCS)
            -Unconditioned Response: (UCR) the natural response (usually also the desired response) to the UCS (e.g. salivation with Pavlov's dogs)
            -Conditioned Stimulus: A stimulus associated with a desired response unnaturally.
            -Conditioned Response: Same as UCR, but is a response to the Conditioned Stimulus, and is therefore unnatural; the response has been learned. 
        -Operant Conditioning: Learning due to natural consequences of actions. Using past responses and consequences of actions to predict future behaviors.  
            -Positive reinforcement: Associating something positive to a response to increase that particular response. Basically, adding something good after one performs a desired action so that, in the future, the desired action happens more often.
            -Negative reinforcement: Associating the elimination of something negative with a desired response. Basically, taking away something bad after a desired action is performed so that, in the future, the desired action happens more often.
            -Punishment: Associating something negative with an UNdesired behavior to decrease that response. Basically, adding something bad after an undesired action is performed so that, in the future, the undesired outcome happens less often.
            -Extinction: Associating the elimination of something positive with an UNdesired behavior. Basically, taking something good away after an undesired response is performed so that, in the future, the undesired response happens less often.
    -Non-associative learning: Learning in which triggers, anchors, or otherwise systems for association of multiple objects are not used.
        -Habituation: Progressive diminution of behavioral response probability with a repeated stimulus. Basically, an organism reacts less and less to a stimulus the more the stimulus is projected, so long as the stimulus is neither rewarding nor harmful.
        -Sensitization: The response to a stimulus gets progressively amplified.
-Fatal familial insomnia: Complete inability to go to sleep; one stays awake until death. Very rare. No known cure.
    -Stages: 1. (About 4 months) Increasing insomnia. Results in panic attacks, phobias, and paranoia.  
         2. (About 5 months) Hallucinations and extreme panic attacks.
         3. (About 3 months) Complete inability to sleep, rapid weight loss. 
         4. (About 6 months) Dementia, then death.
-Suspension of disbelief: (origin: Samuel Taylor Coleridge (poet), 1817) People suspend the actual reality and accept entertainment media as true, suspending disbelief due to inaccuracy, etc. in order to be entertained.
    -People know that when they watch a movie, the reality is that they are viewing a series of moving pictures projected onto a 2 dimensional surface, but they temporarily accept the movie's storyline as actual reality in order to be entertained.

-Archimedes' Principle: Law of physics; States that the buoyant force exerted on an object in submerged in a fluid is equal to the weight of the fluid it displaces. 
    -Buoyancy: Upward force on a body immersed in a fluid caused by the difference in pressure when the body is at the top of a body of fluid and when it is at the bottom; this difference results from the fact that the lower an object is in a fluid, the greater the pressure and the weight from the overlying fluid.
        -Magnitude of the buoyant force is proportional to the difference in pressure at the top and the bottom of the column. 
        -If an object is less dense than the fluid itself, this force can keep the object afloat. 
-Platonic solid: a regular, convex polyhedron with congruent faces, all regular polygons. The same number of faces meet at each vertex. 
    -Five Platonic Solids:
        1. Tetrahedron: Composed of 4 triangular faces, 3 of which meet at each vertex. 6 edges, 4 vertices.
        2. Cube: (A.K.A. Hexahedron) Composed of 6 square faces, 3 of which meet at each vertex. 12 edges, 8 vertices.
        3. Octahedron: Composed of 8 equilateral triangles as faces, 4 of which meet at each vertex. 8 edges ,6 vertices.
            -Looks like two square pyramids joined by the base, one upside down and on right side up.
        4. Dodecahedron: Composed of 12 pentagonal faces, 3 of which meet at each vertex. 20 vertices, 30 edges.
        5. Icosahedron: Composed of 20 equilateral triangles as faces, 5 of which meet at each vertex. 12 vertices, 30 edges.
-Military strategy: Nine principles of military strategy, as defined in United States Army Field Manual of Military Operations FM-3-0, sections 4-32 to 4-39:
    1. Objective: direct all operations towards a single, well defined goal. 
    2. Offensive: Sieze, retain, exploit the initiative.
    3. Mass: Concentrate military force at important areas in the enemy's defense.
    4. Economy of Force: Use as little of the total combat force on secondary objectives. 
    5. Maneuver: Apply combat force so as to displace the enemy's location, especially to a less strategic one.
    6. Unity of Command: Military hierarchy with obedience. Ensure that each objective is carried out under a single officer.
    7. Security: Plan for all of the enemy's possible exploits; allow the enemy to gain no advantage.
    8. Surprise: Concentrate force at areas where the enemy is not prepared to defend or counterattack.
    9. Simplicity: Conclusive and clear orders in the chain of command.     
    -Defending force multiplier: 3 to 1; defensive units can hold off 3 times their own number of attackers.
        -The larger the area of defense, the more the advantage switches to the attacker from the defender, as defending forces become spread thin and sacrifice mobility.
    -Primary weapon of modern conventional warfare: tank, because of its mobility (to permit rapid force concentration) and power (to be effective in concentration).
    -Lanchester's Law: Military force difference (of 2 armies) = a^2 - b^2 where a = units of army 1, b = units of army 2.
        -Ex: A1 = 2 units, A2 = 3 units: Military force difference = 5 units.
        -For armed forces: Advantage to superior force is a by a factor of ((Power1)/ (Power2))^2
            -Ex: 2 tanks versus 1 tank; side with 2 tanks has a superior force by a factor of 4.
    -Force multiplication: Factors that amplify military force. Common force multipliers:
        -Morale
        -Technology
        -Terrain/Climate
        -Training
        -Reputation (particularly fearsome ones)
        -Deception
        -Tactics
-Economic Problems:
    1. Paradox of value: objects' inherent values do not correlate with the economic value given to them by the market (e.g. diamonds have a much higher market price than water despite water being a prerequisite for all life and diamonds having purely aesthetic value; water isn't extremely abundant either: half a billion people die every year from lack of clean water)
    2. Khazzoom-Brookes Postulate: Increased energy efficiency leads to increased energy consumption (e.g. in the 1990s). This is because:
        -Increased energy efficiency -> cheaper energy -> encourages use
        -Increased energy efficiency -> economic growth -> more energy usage
        -Increased energy efficiency allows entities that were previously being restrained by the bottleneck in energy to multiply their use of the energy.
    3. Bounded Rationality: Consumers are supposed to make ideally rational decisions to maximize their utility but are often restrained by information asymmetry, cognitive biases, etc.
    4. Lipstick Effect: During economic downturns, people, instead of cutting down on luxury goods, buy more but cheaper luxury goods (e.g. buying a lot of expensive lipstick instead of a new car due to an economic downturn)
    5. Tragedy of the Commons: It is impossible to conserve a resource in a competitive market because of the Nash equilibrium created: the moment one producer attempts to conserve the resource, his or her competitors use up not only his, now given up, share of the resource, thereby maximizing their own profits, but also use up the resource anyways, defeating the purpose of the conservation and leading to every competitor ignoring conservation until depletion hits. The problem is based fundamentally on a lack of trust.
    6. Tragedy of the Anti-Commons: Too many owners of a product discourages the production of the product due to high costs in both money and time. 
    7. Information Asymmetry: Producers have more information than consumers and the attempt to use that information to minimize risk lowers quality on the market overall.
        -E.g. Defective units of a product whose malfunctioning cannot be traced encourages an average unit price for all units, good or bad condition, and therefore, because even great units are sold for less than worth, no one sells good units and the whole market is filled with only bad units.
        -E.g. Health insurance: Health insurance companies want low-risk customers, but those are the least likely to buy insurance. Companies do not have 100% information of each customer's condition and so try to account for risk of the high-risk customers by raising premiums, which drives away low-risk customers that mitigate risk, driving risk up and, by extension, premiums, and so on until the collapse of the industry.
    8. Cobra Effect: Solution to a problem increases problem.
        -E.g. In order to decrease the snake population in India, the colonial British offered rewards for dead snakes, which caused the Indians to breed more snakes for money.

-Psychopathy: A personality disorder characterized by muted emotions, a lack of empathy, reduced fear, tolerance to stress, egocentricity, superficial charm, manipulativeness, irresponsibility, impulsiveness, disregard for rules, lack of remorse.
    -Not all psychopaths are violent 
    -Most have good attention to detail and high intelligence
    -Genetic disorder
    -Very high emotional intelligence
                        TEMPORAL PARADOXES
-Temporal Paradox: A logical paradox that arises from backwards time travel, usually wherein a backwards time traveler goes back in time and does something to prevent himself from going back in time in the first place.
    -The most basic and first solution proposed is that backwards time travel is just plain impossible.
-Solutions:
    -Time line protection hypothesis: No matter what, the backwards time traveler cannot do anything to affect his going back in time.
        -Ex: While attempting to kill his own grandfather, a time traveler finds misplaces his gun, or kills the wrong man, etc.
    -Novikov self-consistency principle: Anything the time traveler does in the past MUST have been a part of history (i.e. the past) all along. Thus, the time traveler has a causal influence on history, but, according to history, he was supposed to do so, just as much as any other historical event in the past. It is therefore impossible for a backwards time traveler to "change" the present, or even "change" history, because he has already gone back in time.
        -Employs time line protection; if a backwards time traveler tried to kill an ancestor (or do anything else to avert his going back in time) he would inevitably fail.
        -Gives rise to Bootstrap Paradox: Objects or information can exist without ever being created. After information or objects are sent back in time, it is recovered in the present and becomes the very object/information that was initially brought back in time in the first place.
            -Ex: A professor (in the present) reads an equation in a physics journal. He goes back in time and gives the equation to his friend, then a young student. His friend writes the equation up in a physics journal, where the professor reads it in the present, closing the loop.
        -Predestination Paradox: (A.K.A. causal loop, causality loop, closed loop, or closed timelike loop) A time traveler goes back in time and does something to cause himself to go back in time in the first place.
    -Multiple universes hypothesis: There exists a multiverse (an infinite number of parallel universes that exist in conjunction in hyperspace). If a person is travels back in time, all he actually does is end up in a parallel universe that happens to be that amount of time behind is home universe.
        -Timeline corruption hypothesis: Any change in the past, no matter how tiny, will have a domino effect in the future, changing reality in huge ways.
        -Ex: A man who goes back in time and tries to kill his grandfather will only end up going to a parallel universe which is two generations behind his, with everything else being the same, and killing the parallel version of his grandfather.
    
    -Branching universes hypothesis: Backwards time travel would cause time to branch. The effects of a backwards time traveler are felt in an alternate reality, with the parent (and home) reality being unaffected.
    -Temporal Merging Hypothesis:  Each action committed in time travel overlaps reality with one another.
        -Yeah I don't really get this one.
    -Choice timeline hypothesis: History changes the instant the time traveler decides to travel back in time, rendering his actions thereafter to be predestined. 
        -This theory is most consistent with our current understanding of time.
    -Can Not Because Has Not: This theory, assuming that the present is not the most "current" in time (i.e. there exists the future already; we are the past selves of our future selves), and states that backwards time travel is impossible because if it was, we would know about it by now because someone from the future would have brought it back.
        -This theory fails because there are several scenarios in which, in the future, backwards time travel is invented but we still do not know about it in the present.
    -Self Healing Hypothesis: A backwards time traveler can affect the past, but not the present, as a different set of circumstances would simply occur that happen to lead to the exact same present as before.
    -Destruction Resolution: The cause of a paradox due to backwards time travel is completely eradicated; all traces from history are erased and all the effects he had in the past are caused for some other reason. Thus, from a historical viewpoint, the paradox never happened.
        -Ex: A backwards time traveler trying to kill his grandfather is eradicated from history the moment he commits the murder, and the grandfather dies some other way.
    -Temporal Modification Negation Theory: Backwards time travel is possible, but actions that produce temporal paradoxes are impossible. Motives play a huge role. If someone were to go back in time and shoot AT RANDOM five people, one of whom was Hitler's parent, the Holocaust wouldn't happen. Because the backwards time traveler shot AT RANDOM and not with the intention of preventing the holocaust, in the time loop, his motivation for time travel is preserved.
    -Changes Allowed: Not only is backwards time travel possible, but the production of logical paradoxes is possible as well; one can go back in time and kill one's grandfather, and the only change in the present would be that one does not exist, and nor do one's parents and grandfather.
    -Doomed Timeline Theory: The universe has a "fail-safe" by destroying the timeline which has been produced by a change in the past. Any time a backwards time traveler deviates from the stable timeline, he creates an alternate timeline, which is doomed to be destroyed.
-Retrocausality: The question "Can effects precede the cause?". This leads to inquiry into whether the future can affect the present, or the present the past. 
-Internal Combustion Engine: Combustion of a fuel (usually a fossil fuel) occurs with an oxidizer (usually oxygen) in a combustion chamber. 
    -Four-Stroke Engine: Every two revolutions of the engine, four steps are done, which use the expansion of high pressure and high temperature gases (produced by combustion) to apply direct force to a generator, usually the pistons. There needs to be an oxygen-to-gas ratio of about 14:1; 14 parts oxygen to 1 part gasoline.
        1. Intake Stroke: (A.K.A. suction stroke) Piston moves to maximum volume position (meaning it moves as far downwards as it can, thus maximizing the volume of the combustion chamber) and the inlet valve opens (due to the cam lobe pressing down) and lets vaporized mixture of fuel and air into the combustion chamber. This is achieved by the air being sucked into the cylinder; since the cylinder now has more volume, it has less pressure than the surroundings, and the pressure gradient causes air to flow from outside to inside the cylinder, where it mixes with fuel. Fuel is simultaneously injected as the piston moves downward; the fuel injector is typically under the intake valve, so incoming air takes the fuel with it and mixes well. 
        2. Compression Stroke: Both valves are closed. Piston moves into minimum volume position (moving upwards towards the top of the combustion chamber, thus minimizing volume of the combustion chamber), compressing the fuel mixture. Pressure, density, and temperature of the gaseous mixture increase. The purpose of compression is to sufficiently heat the gas (by decreasing volume, pressure increases, which boosts the particle's kinetic energy, which increases temperature) for ignition to be feasible and efficient. 
        3. Power stroke: As the piston approaches top dead center, a spark plug (see Ignition Methods) ignites the mixture (usually at 10 degrees before top dead center). The expansion of the gases (caused by the fuel's ignition) produces power. This expansion also pushes the piston down into the maximum volume position once more. The repeated up-down motion of the piston turns an axle (which is fused to the piston at a point far from its center of mass, so the motion of the piston creates a torque and therefore rotation), which provides energy. 
        4. Exhaust stroke: After the power stoke, the exhaust valve opens. The piston is now in maximum volume position. The exhaust gases (carbon monoxide) escapes into the air, pushed by the piston as it comes back up into the minimum volume position.  
    -Combustion requires oxygen, but injecting nitrous oxide does more of the same combustion and produces more power (thus called nos or nitros) 1
    -Ignition methods: 
        -Spark plug: Delivers electric current to the combustion chamber to ignite the fuel mixture, via electric spark. 
        -Compression Ignition: (A.K.A. Diesel engine) Uses heat of compression to ignite the fuel. Fuel is injected into the combustion chamber. 
            -Has high thermal efficiency due to good compression ratio. 
            -Air is compressed, raising its temperature, and fuel is then injected into the very hot air and is further compressed, causing the fuel to self-ignite.
        -Laser Ignition: Using lasers to ignite the combustion. This would be more fuel efficient as well as cheaper and environmentally friendly. Theoretical at this point.
    -Engine configurations:
        -V Twin: Two cylinders arranged in a V-like pattern, meaning both are angled at equal angles away from the vertical, creating a V shape. This configuration is common in motorcycles.
        -V6: A V Twin arrangement, but scaled up by a factor of 3. This configuration is essentially a row of 3 V Twin arrangements, and consists of of a row of parallel cylinders, angled from the vertical at some angle, and on the other side of the vertical, at the same angle, are 3 opposing cylinders parallel to each other; each cylinder on one side is paired with another on the other side of the vertical. These engines are well-suited for mid-sized cars with moderate to large power usage, and also for sports cars.
        -V8: A V Twin arrangement scaled by a factor of 4; a V6 arrangement with an extra pair of cylinders, also arranged in the V-shape. This is well suited for heavy-duty engines.
        -Flat Four: Four cylinders arranged in a flat arrangement. This can be visualized as having an engine, oriented landscape, in each cell of a 2 by 2 grid.
    -Octane rating of fuel: A fuel's octane rating (seen at gas stations as "Unleaded 87", "Plus 89", and "Premium 93") refers to its resistance to autoignition (when the fuel-air mixture ignites on its own, spontaneously and without any spark). Autoignition damages the engine and wastes fuel. Higher performance engines have higher compression and so require fuels that are more resistant to ignition by compression, so they don't auto-ignite; thus, higher performance engines use fuel with higher octane ratings. 
-External Combustion Engine: Heat engine where an internal working fluid (pressurized gas or liquid that drives a machine) is heated by combustion in an external source. 
    -Closed Cycle: Fluid is cooled and rescued after its expansion has already produced mechanical energy. 
    -Open Cycle: (Less common) Fluid is dumped after use and cool, fresh fluid is pulled in. 
    -Steam Engine: External combustion engine with steam as the working fluid. Water turns to steam in a boiler, reaching high pressure. It is expanded through pistons or turbines, producing mechanical work. The "spent" steam, now with low pressure, is either released as exhaust or  condensed in the boiler for reuse.  
-Laws of Thought: Fundamental axiomatic rules upon which rational discourse is based. The rules guide and underlie all rational thought.
    1. Law of identity: An object is the same as itself. That is, (A if and only if A).
    2. Law of non-contradiction: Contradictory statements cannot both be true. That is, (A is B) and (A is not B) are mutually exclusive. 
    3. Law of excluded middle: For any proposition, either that proposition is true or its negation is true, but not both (by law of non-contradiction). 
____________________________________________________________________________________________________
                        CHAOS THEORY
                ITERATIONS
    -Mapping: Performing a function by inputting a value "x" and receiving "y" as the output. The "y" becomes the new "x". These 2 steps form a mapping of one number onto another. Doing this for all real numbers would be to map all real numbers onto themselves.
        -Iterated mapping: The addition of a third step: taking the new "x" after an initial mapping of one number onto another and using that to yield a new "y"; this new "y" becomes our next "x" value, which yields our next "y" value, which becomes our next "x" value, and so on.
        -N th Iterate: f^n (x) is the n th iterate of our original value "x"; this original value is the number you start with.
    -Orbit: The series of numbers f^1 (x), f^2 (x), f^3 (x), ... , f^n (x)
    -Seed: The original value "x".

        Example: y = x^2 + c and y = x
                 When c = 0, the mapping "f: x -> x^2 +c" yields: 
                            f^n -> |x| > 1 yields Infinity
                                   |x| = 1 yields 1
                                   |x| < 1 yields 0
    -Sinks: (or Attracting Fixed Points) Because all orbits approach either 0 or Infinity, 0 and Infinity are called the sinks (or attracting fixed points)
        -Basically is a value that attracts the orbits of the points around them
    -Sources: (or Repelling Fixed Points) Because only orbits of x = 1 or x = -1 yield 1, 1 is a source (or repelling fixed point)
____________________________________________________________________________________________________
-Medulla Oblongata: Lower half of brainstem, controls respiration, the cardiac center ( and so sympathetic and parasympathetic nervous systems), vasomotor center (regulates blood pressure, homeostatic processes), reflex center (vomiting, coughing, sneezing, swallowing)
    -Small hit to this area can kill
-Human Connectome: Project mapping the functional pathways of human brain.
-Miller v. United States: A supreme court decision maintaining that it is unlawful for law enforcement to arrest a suspect in his own home without first stating their authority and purpose.
-Dysteleology: Philosophical view that existence has no telos (purpose or final cause). 
    -Aggressive, optimistic, science-oriented form of atheism.
-Panglossian Paradigm: Everything that has evolved (through natural selection or not) is suited perfectly for its purpose.
    -Example: We have 5 fingers because 5 fingers is a perfect amount of fingers to have; 4 is too little and 6 too many.
    -Counter: Evolution is the opposite of intelligent design; organs were not made with a specific purpose in mind; they simply arose and stuck if they worked. In short, adaptations must work well enough, not perfectly, to thrive in the gene pool. Even our current organs are imperfect, but work well enough.
-Problem Solving: Most complex of all intellectual functions. Two kinds: Mathematical and personal. 
    -Difficult problems:
        1. Intransparency: lack of clarity 
        2. Polytely: multiple goals, often contradictory or difficult to attain all at same time
        3. Complexity: large amount of variables subject to change and difficult to calculate that must be weighed. 
        4. Dynamics: temporal constraints, sensitivity, unpredictability.
    -Problem solving techniques: 
        1. Abstraction: solving the problem in a model of the system before applying it to the real system
        2. Analogy: using a solution that solves an analogous problem
        3. Brainstorming: (especially among groups of people) suggesting a large number of solutions or ideas and combining and developing them until an optimum is found
        4. Divide and conquer: breaking down a large, complex problem into smaller, solvable problems
        5. Hypothesis testing: assuming a possible explanation to the problem and trying to prove (or, in some contexts, disprove) the assumption
        6. Lateral thinking: approaching solutions indirectly and creatively
        7. Means-ends analysis: choosing an action at each step to move closer to the goal
        8. Method of focal objects: synthesizing seemingly non-matching characteristics of different objects into something new
        9. Morphological analysis: assessing the output and interactions of an entire system
        10. Proof: try to prove that the problem cannot be solved. The point where the proof fails will be the starting point for solving it
        11. Reduction: transforming the problem into another problem for which solutions exist
        12. Research: employing existing ideas or adapting existing solutions to similar problems
        13. Root cause analysis: eliminating the cause of the problem
        14. Trial-and-error: testing possible solutions until the right one is found
-Human capital: knowledge, experience, job skills, education; human capital determines one's wages
-Male-Female income disparity: Women make, on average, $0.75 for every $1.00 a man makes
    -Result NOT of discrimination in labor market, but rather of different choices:
        -Men tend to go into science, business, and engineering; women tend to go into humanities, social sciences, teaching, etc, all jobs that pay less
        -These choices may be the result of sexism in society that guides young girls towards particular professions such as social sciences, teaching, etc, but it is NOT a result of discrimination or sexism in the labor market
    -Expectations: Women expect to take time off in the future to take care of children; men expect to work full time, and this psychological expectation has an effect on choices and wages
        -Women therefore work part time more than men do, which results in lower average income
    -Men and women with same experience, education, and career path are almost on equal ground; the women make $0.98 for every $1.00 a man makes, so the gap has almost disappeared
-Broken Window Fallacy: Fallacy: Destruction means that people spend money to repair, which stimulates the economy.
    -This is a fallacy because the money spent on repairs can't be spent on other things, such as consumption of goods. So while one area of the economy prospers, it is actually at the expense of another.
        -Destruction is therefore bad in general, because not only does it have the same effect as if nothing had happened (think of it as in the status quo, there is an increase in the economy by 1%, and when destruction happens the repair industry grows 4% but the services industry shrinks (because people spend less money here due to spending it on repairs instead) by 3%, still a net increase of 1%) but also leads to loss of property and sometimes lives, as well as time.
-Democracy: all citizens vote, majority policy wins.    
    -Argument against (irrational voters): the vast majority of the populace does not have the knowledge or insight to vote properly; they know so little they may as well be flipping a coin. Therefore, policies, which, by definition of a democracy, are being chosen by a majority, which is basically composed of people who don't know what they're talking about, are chosen in a very poor fashion. 
        -Rebuttal: Miracle of aggregation: Law of large numbers says that, assuming 90% of the population is so irrational their voting patterns are akin to flipping a coin, it should average out to 45% for policy A and policy B, which is a tie, and therefore the remaining 10%, which is composed of well informed people who know what they're doing and choose good policies, are actually in charge and so good policies are still chosen since the well informed are still in charge. Law of large numbers only applies if the sample space is large enough, so miracle of aggregation only holds in societies with voter bases that are very large.
               So policies must appeal to the well informed, not the majority, to be voted in, and therefore democracy DOES lead to good and efficient policies. Presidential candidates must work on the 10% of the population that is well informed since the other 90% may we well flip a coin and guarantees them each 45% vote, and so presidential candidates with good policies and ideals will only win. The errors that bad voters (the majority) make cancel out in a large enough sample.
            -Counter-rebuttal: Miracle of aggregation argument assumes that irrational voters DO flip a coin; they are just as likely to overestimate as they are to underestimate. Irrational voters are easily swayed by easy campaign slogans and such, so policies and political candidates still continue to fight over the majority, not the minority that's well informed. The majority of people, who are irrational, tend to agree on one bad policy. 
-Canadian Health Care: (Since Canada Health Act of 1984) Universal, public health care system that insures all citizens free of charge, keeping quality of care constant through federal standards and regulations. The government doesn't collect any information about the patient (this remains within doctor-patient confidentiality); it just pays for medical costs as specified by federal standards.
    -Cost effectiveness: The system is cost effective because of administrative simplicity: the doctors handle the insurance claim made by the insurance provider in that specific province, and the patient handles nothing.
    -Funding Mechanism: System is paid through income taxes
        -Exception: Province of British Columbia: BC uses an unchanging monthly premium. For those with incomes below a certain line, the premium is free or reduced in price.
        -Maximization of revenue: guaranteed by keeping advertising in this sector low.
        -3 Largest areas of cost: Hospitals (28.2%), Medications and drugs (16.5%), and physicians (13.4%)
    -No deductibles (amount a person agrees to pay in the event of an accident before the insurer intervenes; a higher deductible means a lower monthly premium in most insurance systems) or co-payments (payment made by the person and insurer in conjunction; both parties agree for the person to pay X% and the insurer to pay the rest (100-X %))
    -Coverage: All essential basic health care is covered (eliminating the need for different plans) and everyone receives the same care
    -Preventive care and yearly check ups encouraged, as they cut down overall costs.
    -Public opinion: 86.2% of Canadians favor the current health care system to a for-profit, private based one.
        -82% of Canadians prefer Canadian health care to American health care.
-Scopolamine: (a.k.a. levo-duboisine, hyoscine, devil's breath) can lead to loss of choice; people under the influence do anything they're asked
-Monopsony: Individual buyer buying from multiple sellers; buyer is so large that he controls the prices.
-Quantitative easing: (***INCOMPLETE***)
-P7C3: A drug, similar to latrepirdine (an antihistamine drug, commonly used to treat or prevent allergic reactions), which stimulates growth of neural connections and improves memory and learning. It has potential applications in the treatment of Alzheimer's.
-Boeing YAL-1 Airborne Laser Testbed: chemical oxygen iodine laser, several megawatts in power, designed to intercept enemy TBMs (tactical ballistic missiles). The laser is mounted inside a Boeing 747 and can fire 20 high powered shots or 40 low powered shots (for fragile TBMs) before refueling. 
    -Successful in test runs but discontinued due to low range; in order for range to be practical, a laser 20-30 times the current laser's power is needed, and the current laser would have to fly around Iraq to shoot down Russian missiles.
-Types of missiles:
    -Conventional missiles: 
        1. Air-to-surface missile
        2. Anti-ballistic missile
        3. Anti-satellite weapon
        4. Anti-ship missile
        5. Anti-submarine missile
        6. Anti-tank guided missile
        7. Land-attack missile
        8. Surface-to-air missile (list)
        9. Surface-to-surface missile
        10. Wire-guided missile
        11. Air-to-air missile
    -Cruise missiles: guided conventionally, jet propelled
    -Ballistic missiles: missile that enters space then comes back down before hitting the target (follows a sub-orbital ballistic flight path), delivers one or more warheads
        -Types of ballistic missiles: Classified by range
            1. Tactical ballistic missile: Range 150 km - 300 km
                -Battlefield range ballistic missile: (A.K.A. BRBM) Range < 100 km
            2. Theater ballistic missile: (A.K.A. TBM) Range 300 km - 3500 km
                -Short range ballistic missile: (A.K.A. SRBM) Range < 1000 km
                -Medium range ballistic missile: (A.K.A. MRBM) Range 1000 km - 3500 km
            3. Immediate range ballistic missile: (A.K.A. IRBM or long range ballistic missile (A.K.A. LRBM)) Range 3500 km - 5500 km
            4. Intercontinental ballistic missile: (A.K.A. ICBM) Range > 5500 km
            5. Submarine launched ballistic missile: (A.K.A. SLBM) Range > 5500 km
-Missile defense systems: Technology to intercept or detect hostile missiles. US, Russia, France, India, and Israel have developed the technology.
    -Types:
        1. Strategic missile defense: Targets ICBMs (which travel at 7 km/s = 15700 mph)
        2. Theater missile defense: Targets medium range missile (which travel at 3 km/s = 6700 mph)
        3. Tactical missile defense: Targets tactical ballistic missiles (which travel at 1.5 km/s = 3400 mph)
    -Time of interception: The stage in the missile's payload delivery trajectory that it is intercepted during.
        1. Boost phase: Interception occurs during launch.
            -Advantages: Easy to detect (from rocket sound and smoke), makes decoys impossible
            -Disadvantages: Difficult to set up a system near enemy territory, short window for interception (3 minutes at most)
        2. Mid-course phase: Interception occurs in outer space when rocket thrusters are no longer operating.
            -Advantages: Large window for interception (up to 20 minutes), covers maximum area
            -Disadvantages: Deals with decoys, requires anti ballistic missiles, difficult/expensive to launch into space, requires powerful radar
        3. Terminal phase: Interception occurs after missile enters atmosphere.
            -Advantages: Small anti ballistic missile required, impossible to use decoys, less powerful radar required
            -Disadvantages: Tiny window for interception (30 seconds), smaller area covered, hazardous materials may still fall onto some areas
    -Anti ballistic missiles: Missile designed specifically to intercept ballistic missiles.
    -Process: 1. Infrared satellites detect very fast heat signatures and track the missile's heat signature as it makes its ascent.
        2. When the missile reaches outer space, its engines are deactivated, and the satellites can no longer track the heat signature. Multiple, geographically separated ground or sea systems use very powerful radar to track the missile.
            -The various stations are always communicating with each other to create a tracking system of maximum accuracy.
        3. An anti ballistic missile, usually a kinetic missile that relies on pure momentum, is launched. Calculations factor in the hostile missile's location, velocity, and trajectory as well as the kinetic anti ballistic missile's projected flight path to calculate the point of interception.
        4. If any remaining missiles or missiles failed to be destroyed enter the atmosphere, smaller anti ballistic missiles are launched for interception. The calculation of interception point and radar tracking is a lot easier.
        -In November of 2011, NATO tested such a system, using international systems working together, and destroyed a target missile in 5 minutes.
-Newcomb's problem: (A.K.A. the predictor paradox) Chooser has a choice between taking both A, containing $1000, and B, containing either $1000000 or $0 depending on the predictor's decision, or just taking B. The predictor knows with perfect accuracy what decision chooser makes, and determines the contents of B with this matrix: (1) IF Chooser chooses [A+B] THEN B has $0 and (2) IF Chooser chooses [B] THEN B has $1000000.
    -Payoff matrix: Predicted choice       Actual Choice       Payout
                    [A+B]                  [A+B]               $1000
                    [A+B]                  [B]                 $0
                    [B]                    [A+B]               $1001000
                    [B]                    [B]                 $1000000
    -Arguments: Choosing [A+B]:
   When the predictor puts money in the box, it is already there and cannot be changed, no matter what the Chooser does. So, if the predictor predicts [A+B] then Chooser should choose [A+B], since the choice is now $1000 or $0. If predictor predicts [B], then Chooser should choose [A+B] as the choice is between $1001000 and $1000000.
        -Supported by Dominance principle in game theory
                -Choosing [B]: Dominance principle application above assumes that it is possible for the predictor to be wrong, which contradicts assumptions. If predictor predicts [B] then the alternative of [A+B] is null, as it is now impossible to choose [A+B]. The predictor knows that Chooser will do, so if Chooser chooses [A+B] then  he will get the free $1000, but if he chooses [B] then he will get $1000000.
        -Supported by Expected Utility Hypothesis
        -Problems involving perfect predictors create timelike loops of information, since their knowledge of future events affects their present; therefore, the future has an effect on the past.
    -Allows Past Events to affect Future Events: The prediction, which is 100% correct but hasn't happened yet, makes the choice the decision maker makes equivalent to the contents of Box B. However, the contents of Box B have been pre determined whereas the choice happens in the present, and so the past and future affect each other, not in a unilateral direction as the should.
        -Fatalism: Philosophy that all actions are subjugated to fate. It posits that we are powerless to make decisions that alter from a set order of events.
        -Idle argument: If something is fated to occur, it is useless to make any effort at all with respect to the event, even though your actions in the present will have an effect on the fated event itself.
            -Example: It is fated that you get a certain grade on the test. Therefore, it is useless to study for the test because you will get a singular grade regardless. However, your lack of studying is the main factor in the grade you get, even though the grade you got, which was in the future, caused you not to study. In this way, the future can affect the past.
-Svalbard Global Seed Vault: Safeguard that holds over 500,000 of the earth's plant species in case of apocalypse. 
    -390 feet under a Nordic mountain, 620 miles South of North Pole and surrounded by thousands of miles of ocean, tundra, and polar bears. Resistant to nuclear holocaust and magnitude 10 earthquakes. 430 feet above sea level. 4 heavy steel doors.
-Tupper's Self-Referential Formula: when graphed in 2 dimensions, the graph is the equation itself.
    - 1/2 < [mod([y/17]2^(-17[x] - mod([y], 17)), 2)] where [x] = floor function and mod = modulo operation (operation such that for two numbers, the output is the remainder when the numbers are divided (e.g. 5 mod 2 = 1)). 
    -The formula is actually a representation of every bit within the covered rectangle and so plots every possible pattern of pixels within that rectangle at various vertical heights.
-Relational psychology test:
    -Read the following questions, imagining the scenes in your mind, and write down the FIRST thing that you visualize. Do not think about the questions excessively.
        1. You are walking in the woods. Who are you walking with?
        2. You are walking in the woods. You see an animal. What kind of animal is it?
        3. What interaction takes place between you and the animal?
        4. You walk deeper into the woods. You enter a clearing and before you is your dream house. Describe its size.
        5. Is your dream house open, or surrounded by a fence?
        6. You enter the house. You walk to the dining area and see the dining room table. Describe what you see on and around the table.
        7. You exit the house through the back door. Lying in the grass is a cup. What material is the cup made of (ceramic, glass, paper, etc.)?
        8. What do you do with the cup?
        9. You walk to the edge of the property, where you find yourself standing at the edge of a body of water. What type of body of water is it ?
        10. How will you cross the water?
    -Answers:
        1. The person who you are walking with is the most important person in your life.
        2. The size of the animal is representative of your perception of the size of your problems.
        3. The severity of the interaction you have with the animal is representative of how you deal with your problems (passive, aggressive).
        4. The size of your dream home is representative of the size of your ambition to resolve your problems.
        5. No fence is indicative of an open personality. People are welcome at all times. The presence of a fence is more indicative of a closed personality. You'd prefer people to not drop by unannounced.
        6. If your answer did not include food, people, or flowers then you are generally unhappy.
        7. The durability of the material with which the cup is made is representative of the perceived durability of your relationship with the person from number 1. For example, Styrofoam, plastic, and paper are disposable; Styrofoam, paper, and glass (ceramics) are not durable; and metal and plastic are durable.
        8. Your disposition of the cup is representative of your attitude toward the person in number 1.
        9. The size of the body of water is representative of the size of your sexual desire.
        10. How wet you get in crossing the water is indicative of the relative importance of your sex life.
-Anatoli Bugorski: stuck his head into a particle accelerator and had his face burned off.
____________________________________________________________________________________________________
                        LINEAR ALGEBRA
Linearly Independent Vectors: Given a set of vectors in vector space V as {v_(1), v_(2), ..., v_(n1)}, each vector having components a_(1), a(2), ..., a_(n2), that set of vectors is linearly independent if the only solution to the equation (a_(1))(v_(1)) + (a_(2))(v_(2)) + ... + (a_(n))(v_(n)) = 0 is when the scalars a_(i) = 0 vector.
    -Informally, every vector in the set of vectors is not a multiple of any other vector. Similar to prime numbers in number theory. 
    -Vectors are always in one column form, anything higher is a rank-2 tensor or up.
    -To solve the above equation, add all the vectors into an augmented matrix and put it in reduced row echelon form through row reduction. Then multiply each component by a the value you want (many will cancel from the 0s and many will be simplified from the 1s) and solve.
        -Row Echelon Form: The matrix satisfies the following conditions:
            1. All nonzero rows are above any all zero rows.
            2. The leading coefficient (first nonzero element) is to directly in the immediately leftward position compared to the leading coefficient of the row under it.
        -Reduced Row Echelon Form: A matrix that is the result of Gauss-Jordan elimination (using row reduction to put matrices in row echelon form). They must satisfy all of the above conditions plus two more:
            3. Every leading coefficient = 1 
            4. The leading coefficient is the only nonzero entry in that row.
    -Linear transformations: T is a linear transformation if (assuming u and v are vectors)
        1. T(u + v) = T(u) + T(v) 
        2. T(c * u) = c*T(u) where c = constant.
    -Properties of linearly independent vectors:
        -Any set with only one vector is linearly independent if and only if that vector is not the zero vector.
        -A set containing vectors is linearly independent if and only if any one vector is NOT a multiple of any other vector.
        -A set containing only one-column vectors with the same rows  (that is, all vectors are in the form "n x 1") is linearly dependent if the number of vectors is greater than the number of rows in each vector
        -If a set is linearly independent, then the property of linear independence of that set is unaffected by the removals of any arbitrary vectors.
        -If a set contains only distinct unit vectors, then the set has linear independence.
Bases of Vector Spaces: The set of vectors {v_(1), v_(2), ..., v_(n)} is the basis for a vector space if: 1. All vectors in the above set span the vector space.
                                                      2. All vectors in the above set are linearly independent.
    -Span: The span of a set of vectors {v_(1), v_(2), ..., v_(n)} is given by the set of all linear combinations of the set, where a linear combination is of the form (c_(1))(v_(1)) + (c_(2))(v_(2)) + ... + (c_(n))(v_(n)), where each c_(i) is a real number scalar.
        -Any two vectors in R^2 that are not linear multiples of each other will span all of R^2. That is, any vector in R^2 can be written as a linear combination of the two specific vectors mentioned before.
    -The basis of a set of vectors can be seen as the building blocks of that set; any vector in the set can be written as a combination of its basis vectors. 
    -To check if any set of vectors form a basis for a vector space: 1. Make sure the vectors are linearly independent. 
                                     2. Find constants, in terms of x, y, and z such that when those constants are multiplied to the vectors in the set of vectors for which we are forming a basis for a vector space, produces any vector [x]. 
    -Two vectors form a basis for a third vector if those two vectors can be linearly combined (multiplied by scalars and then added) to produce the third vector.                                                                                                                                      [y]
        -Any two vectors in R^2 that are not scalar multiples of each other will span all of R^2.                                                                                                                                   [z]         
-Determinants: Values used in calcuLating linear algebra.
    -For a 2 by 2 matrix [a   b], the determinant is given by ad - bc.
              [c   d]
    -For higher matrices: Select an element and multiply that element by the determinant of the resulting matrix when the row and column to which the element belongs is crossed out. Multiply the determinant of the small matrix by the element chosen and add or subtract with the next element in that row, all the way down the row; adding or subtracting depends on the element's position; starting at the first element (row 1, column 1) being positive, the elements from there alternate signs as one proceeds down the rows.
    -Applications of Determinants:
        -Finding the area of a polygon: If there exists a polygon in R^2 with vertices (x_(1), y_(1)), (x_(2), y_(2)), ..., (x_(n), y_(n)) listed counter clockwise around the perimeter of the polygon, the area of that polygon is given by 0.5(determinant([x_(1)   x_(2)]) + determinant([x_(2)   x_(3)]) + ... + determinant([x_(n)   x_(1)])).
-Inverse Matrices: A matrix A has the inverse matrix A^(-1) is A * A^(-1) = Identity matrix. To find the inverse:
    - A^(-1) = (1/(determinant(A)))(transpose(adjugate matrix))
        -Adjugate matrix: (A.K.A. matrix of cofactors) A matrix with elements computed from the determinants of the matrix with the remaining numbers when rows and columns of the corresponding element of the original matrix are crossed out.
        -Transpose: The transpose of a matrix is given by the matrix with elements computed by inverting the elements of the original matrix over the diagonal of the original matrix.
            -The transpose of [x1   y1   z1] is given by [x1   x2   x3]
                              [x2   y2   z2]             [y1   y2   y3]
                              [x3   y3   z3]             [z1   z2   z3]                                                                                         
-Cramer's Rule: To solve a system of equations with the same number of equations as variables, use Cramer's Rule as follows:
    1. Separate all the variables on the left side of each equation and the constant on the right side, for every equation. Put all the variables in the same order for each equation.
    2. Create a matrix with the coefficients of every variable, with each element following the order of the equations. Find the determinant.
    3. Replace the first column of the matrix created in (2) with the column of constants on the right side of every equation and find the determinant. Divide this determinant by the determinant found in (2) to find the value of the first variable.
    4. Replace the second column of the matrix created in (2) with the column of constants on the right side of every equation and find the determinant. Divide this determinant by the determinant found in (2) to find the first variable.
        -Repeat: Replace the n-th column with the column of constants that serve as the answers in the original system of equations and divide by the original determinant to find the value of the n-th variable.
-Gauss-Jordan elimination: Perform row reduction on the augmented matrix of the coefficients of the variables of each equation (with each variable in the same order per equation) and the constants on the right to create an identity matrix with the values for each variable on the right.
    -Using row reduction, convert the matrix [a1   b1   c1   ... | C1] to [1   0   0   ... | C4] where the constants C4, C5, C6, ... are the values of the corresponding variables.
                         [a2   b2   c2   ... | C2]    [0   1   0   ... | C5]
                         [a3   b3   c3   ... | C3]    [0   0   1   ... | C6]
                         [ .    .    .   ... | . ]    [.   .   .   ... | . ]
                         [ .    .    .   ... | . ]    [.   .   .   ... | . ]
-Component of vector A along vector B = (A * B)/magnitude(A)
    -Where the magnitude(A) = sqrt(sum of each squared component)
-Projection of vector A on vector B = ((A * B)/(mag(A))^2)(A)                       
-Unit vector: vector with same direction as initial vector but with magnitude of 1.
    -To find a unit vector, divide the initial vector by its magnitude.
-Homogeneous systems: system of linear equations all equaling 0; a homogeneous system equals the matrix A(matrix(x)) = 0
____________________________________________________________________________________________________
____________________________________________________________________________________________________
                        ABSTRACT ALGEBRA
-Abstract algebra: The study of algebraic structures (e.g. rings, groups, vector spaces, etc.); taking sets of special elements and using operations within those sets.
    -Operators: Process to produce a new value from one or more inputs. 
        -Unary operations: Require only one input value (e.g. trigonometric functions)
        -Binary operations: Require two input values (e.g. addition)
            1. Associative law: (of addition) a + b = b + a
                    (of multiplication) ab = ba
            2. Commutative law: (of addition) (a + b) + c = a + (b + c)
                    (of multiplication) (ab)(c) = (a)(bc)
            3. Distributive law: (a + b)(c) = ac + bc
            4. Identity element: (of addition) 0 + a = a  
                     (of multiplication) (1)(a) = a
-Set: An algebraic structure of a collection of well defined and distinct objects. 
-Group: A group G is a set with a binary operator such that:
    1. If a and b are both elements in the set, then a*b is also an element in the set; G is closed under the binary operator *.
    2. a*(b*c) = (a*b)*c
    3. There exists an element e such that ae = ea = a; e is the identity in G.
        -The identity of Group G is always unique; there is only ever a single identity.
    4. If a is an element in the set, there exists an element a' such that a*a' = a*'a = e; a' is the inverse of a.
    -Note that the binary operator * can be defined in any way, and should not be assumed to mean traditional multiplication.
-Function: Defining a function f: A -> B means that for every element in A there is a corresponding element in b.
    -A function in which every element in A maps to at least one element in B is "onto" or "surjective".
        -For every a in A there exists b in B such that f(b) = a.
    -A function in which every element in A maps to exactly one element in B is "one-to-one" or "injective".
        -If f(a) = f(b) then a = b.
    -Two sets have the same number of elements in them if and only if there exists both a surjective and an injective function between them.
        -A function that is both injective and surjective is bijective.
        -Bijections: Bijective functions are precisely those functions that have an inverse. A function has an inverse only if it is both injective and surjective, ie it always maps different elements in the domain to different elements in the range, and it "uses" every element in the domain. To see examples of why this is true:
            -Consider the function f(x) = x^2. This function is surjective but not injective, and therefore has no inverse. The elements 2 and -2, though different, are mapped to the same number, 4. So, when the inverse mapping tries to map the number 4 to a value, it ends up simultaneously mapping 4 to 2 and -2, which is impossible; a functional value cannot be two different values at once.
            -Consider the function from the natural numbers to itself defined by f(n) = 2n. This function is injective but not surjective, and so has no inverse. This is because certain elements in the range are not "used" by the function, ie the function doesn't map any elements to them, so when the inverse mapping tries to map those elements back into the domain of f, it cannot find any values. In this example, the odd numbers, such as 5, have no integer n such that 2n = 5. So, these values cannot be mapped at all.
    -Homomorphism: A function in two groups G and F, such that G is defined with the binary operator * and F is defined with the binary operator #, where f(a*b) = f(a)#f(b).
        -a and b are elements in G and f(a) and f(b) are elements in F.
        -If such a homomorphism exists, the binary operators in the two groups behave similarly; this is important in proving that two groups are identical.
        -The operators are always preserved.
    -Isomorphism: A homomorphism that is bijective.
        -If there exists an isomorphism between G and F then G and F and isomorphic. Isomorphic groups are algebraically identical.
-Ring: An abelian group with a binary operation that is both associative and distributive over the entire group; every element in such a group has an additive inverse. 
    -Identity element: 0 is always the identity.
    -Operators: The operations of addition and multiplication are associative and commutative, and multiplication distributes over addition.
    -Elements in a ring: 
        -Left zero-divisor: a is a left zero divisor within ring R if and only if there exists a non-zero element b such that ab = 0
            -Right zero divisor: Same as left zero divisor, but of the form ba = 0
        -Nilpotent element: a is a nilpotent element within ring R if and only if there exists a positive integer b such that a^b = 0
        -Idempotent element: a is an idempotent of a set if and only if there exists a binary operation * and x*x = x
            -An idempotent element of a ring is an element a such that a^2 = a
    -Abelian group: A ring in which operations on two group elements is independent of order (law of commutativity)
    -Field: An abelian group under multiplication; informally, a specific type of Abelian group in which each nonzero element has a multiplicative inverse. 
        -Finite fields: (A.K.A. Galois field) Field that contains a finite number of elements. 
            -Finite prime fields:

-Modular arithmetic: A form of arithmetic over integers such that numbers cycle, or "wrap around" a modulus. 
    - a (triple bar) b mod (n) if and only if the value (a-b) is an integer multiple of n. 
        -Equivalently, a and b must have the same remainder.
        -The denotation (triple bar) is represented as an equal sign but with three bars, not two.
-Cayley graph: A visual connection of points that encodes the entirety of the mathematical and abstract structure of a group. See below for appropriate background and an explanation.
    -Background:
        -Group theory background: A group is a set of elements, paired with some associative binary operation under which the elements of the set are closed, have inverses, and contain an identity element (relative to the operator). A generating set of a group is a subset of the group whose elements completely span the group under the operation of the group, ie the set X is a generating set of the group <B, *> (the set of elements in B under multiplication) if every element in B can be obtained by some product of elements in X. 
        -Graph theory background: A graph is a set of points (A.K.A. nodes or vertices); any pair of two points may or may not be connected by a line (called an edge). The important information contained in the graph depends on which nodes are connected to which. The placement of the nodes, the shape and curvature of the edges, and intersections of edges are irrelevant to the mathematical meaning of a graph; all that matters is which points are used and how they are connected to each other. Thus, the same graph can be drawn in multiple ways. 
            -Directed graph: A graph whose edges have direction, ie when two nodes are connected, one can move from one to the other but not back from the second to the first.     
            -Applications: Graphs can be used to represent moves in a game, with each node representing the state of the game. Then, to solve the problem, simply start at the initial state and follow the edges until you reach the solved state; the path one takes over the edges to get from the initial to solved states represents the moves that must be made, in order, to solve the problem. A common example is the Rubik's cube. 
    -Cayley graph: If a graph is constructed with some positions or states are nodes and edges as moves, then those positions (that we're using as nodes) form a group if no moves are blocked no matter what path taken or position is reached, if moves can always be undone, and if all the nodes are different. In fact, the nodes of the graph form a generating set for a group under whatever operation the graph's edges are interpreted to represent. Then, the graph in question is the Cayley graph for the group it is associated with.
        -Formally, a Cayley graph is a colored and directed graph; to construct a Cayley graph of a group given a generating set for the group: 
            1. Assign each element of the group a vertex.
            2. Assign each element of the generating set a color.
            3. For a given element g in the group and a given element s in the generating set, draw a directed edge from g to gs with the color associated with s. 
        -Connectivity: A graph is connected if every pair of nodes is connected. For a Cayley graph, a Cayley graph is connected if and only if the colors used (ie the generating elements used) generate the entire group. 
        -Metric space: We can make any Cayley graph a metric space by defining some measure of distance between nodes for the graph. For a Cayley graph, the distance between nodes is defined as the smallest number of edges to traverse to get from one node to the other. 
-Primitive polynomial: The minimal polynomial for the primitive element in some finite field. Put another way, if the finite field GF(p^m) (this is another way of writing "any finite field"; the "GF" stands for "Galois field" and the "p^m" refers to the order, which is always a prime power for any finite field) has primitive element a, then f(x), with coefficients in GF(p^m), is the minimal polynomial of a if f(a) = 0 and GF(p^m) is generated by a, up to (p^m - 2), ie GF(p^m) = {0, 1, a, a^2, a^3, a^4, ..., a^(p^m - 3), a^(p^m - 2)}. 
    -Primitive polynomials are irreducible over GF(p^m). 
    -Background in field theory:
        -Field extension: Field extensions are a mathematical concept that is extremely prevalent in the study of field theory. A field extension of a given field is another field which includes the given field, but is "extended" in the sense that it is larger, more general, and satisfies additional properties. Formally, a field A is a field extension of a field B is B is a subfield of A, ie B is a subset of A (every element in B is also contained in A) and both fields follow the same binary operations. Since B is a subset of A, it makes sense to say that A "extends" B, since it contains all the elements in B, but additionally contains more elements according to some property; both fields also follow the same binary operations and are closed under them (and have inverses).
            -Notation: If A is a field extension of B, then it is written that "A/B" is a field extension, specifically that A extends B. 
            -Intermediate field: (A.K.A. sub-extension) A field that is extended by one field and also extends another. Formally, B is an intermediate field of the field extension A/C is A/B is a field extension and B/C is a field extension. 
                -Simple extension: A field extension that is created by the adjugation of a single element to the subfield. Therefore, a field E is a simple extension of field F if: (1) E is a field extension of F, and (2) E contains a particular element to be specified, and (3) E is the smallest possible such field. The "particular element" referenced in condition (2) is called the primitive element of the extension.
                    -Adjugation: For fields E and F, let E be a field extension of the field F. For a set of elements A in E, "F(A)" denotes the smallest possible field extension (more specifically, an intermediate extension (ie a sub-extension)) of F that contains the set of elements A. To clarify, F(A) is a field extension of F and E is a field extension of F(A), and F(A) has the property that it is the smallest possible such sub-extension that still contains the set of elements A (which is also contained in E) as a subset. This defines the process of adjugation; it is said that (A) is constructed by adjugation of the elements in A to F. It is also said that F(A) is generated by the set A.
            -If A is a field extension of B, then A can be interpreted as a vector space over the elements of B. This means that the elements in A are the "vectors" (and follow all the axioms for the definition of a vector space), and the elements of B are the "scalars" over which the operation of scalar multiplication is defined (under whatever the multiplicative operation of the field happens to be). 
                -Degree of a field extension: Using the above interpretation, the degree of a field extension is the dimension of the corresponding vector space, ie the number of elements in any given basis of vectors for the vector space.
            -Examples: The complex numbers under addition and multiplication form a field extension of the real numbers, and the real numbers form a field extension over the rational numbers (note that, unrelatedly, this implies that C/Q is also a field extension and R/Q is a sub-extension of C/Q). The degree of the field extension C/R is 2, since when we interpret C/R as a vector space with elements of C as vectors and elements in R as scalars, and we define the two appropriate binary operations, the dimension is 2, since the set {1, i} forms a basis for C/R. 
            -Fundamental theorem of field theory: Let F be a field and f(x) be a non-constant polynomial in F[x] (ie the ring of polynomials in variable x that have coefficients in the field F). Then there exists a field E such that E is a field extension of F and f(x) has a 0 in E, (ie there exists an a in E such that f(a) = 0). Note that the zero of f(x) could itself be, trivially, in F. However, in the case that it isn't, this theorem has more use (as it guarantees the existence of a field extension which, unlike F, DOES contain a zero of f(x)).
        -Quotient group: A (mathematical) group that is created by taking another group and partitioning it by defining an equivalence relation on its elements. The definition of an equivalence relation partitions the group because elements that are distinct in the first group may be considered equivalent and therefore the same in the second. The most common example of a quotient group is the integers modulo an integer n.
            -Notation for integers modulo n: The group of integers modulo n is denoted "Z/nZ". This notation arises from the fact that the group is a quotient group. It makes intuitive sense because "nZ" intuitively and informally refers to multiplying the elements of Z by n, obtaining the set of multiples of n, and dividing the integers by the multiples of n produces the integers modulo n.   
        -Finite field: (A.K.A. Galois field) A field with a finite number of elements. Finite fields are interesting because they can only exist if the order of the field is a prime power (ie a prime number raised to an integer power). Every possible prime power (ie 2^1, 2^2, 2^3, ..., 3^1, 3^2, 3^3, ..., 5^1, 5^2, 5^3, ...) has an associated family of finite fields; all of these finite fields (which shares the characteristic of size) are isomorphic. 
            -Characteristic: The characteristic of a finite field is the prime number that is the base of the prime power that represents the field's size. Formally, if a field F has order p^k (any finite field must have an order of this form), where p is a prime number and k is an integer, the characteristic of F is p. Thus, adding p copies of any particular elements to itself (equivalently, multiplying any element by p) will equal 0.
                -Proof that the characteristic of a finite field is prime: Consider a field F under addition and multiplication. F is finite by assumption and closed under addition by the definition of a field, which means that if we keep adding elements together (take the smallest non-zero element, 1, for simplicity's sake), we will eventually "loop" back onto some element in the sequence created by our adding 1's. Put another way, if we keep adding 1's, we form the sequence (1, 1 + 1, 1 + 1 + 1, 1 + 1 + 1 + 1, ...), and since F is finite and closed, it is guaranteed that the sequence will eventually reach a value that has already appeared in the sequence (ie the sequence has "looped" on itself; this is extremely common and expected with modular arithmetic). So, there exists some number m such that m is the smallest possible number of times we have to add 1 to itself before we loop through the entire field. This number, m, is called the characteristic of F; since 1 is not equal to 0, clearly m 
                                               cannot be equal to 0 or 1. Therefore, m >= 2. To prove that m is prime, use proof by contradiction. We assume that m is composite. Then, there exist two numbers m_1 and m_2 such that (1) m = m_1 * m_2 and (2) m_1, m_2 > 1. Since m = m_1 * m_2, it follows that 1 + ... + 1 (m times) = (1 + ... + 1 (m_1 times)) * (1 + ... + 1 (m_2 times)), since multiplication is equivalent to repeated addition over the natural numbers. However, since 1 + ... + 1 (m times) = 0 by definition, we have: (1 + ... + 1 (m_1 times)) * (1 + ... + 1 (m_2 times)) = 0. This means that one of the two terms on the left hand side must be 0, which implies that m_1 = 0 or m_2 = 0. Both of these conclusions violate our assumption that m_1, m_2 > 1. Thus, there do not exist such integers m_1 and m_2 such that m = m_1 * m_2. Therefore, m is prime.
                -Although the characteristic is prime, prime powers are also viable candidates for the size of a finite field. 
        -Minimal polynomial: Suppose field E extends field F. Let a denote a particular element in E. Then, the minimal polynomial of a is the (monic; ie leading coefficient is 1) polynomial in F[x] (the ring of polynomials of a variable x with coefficients in F) of smallest possible degree such that a is a root. Put more symbolically, for an element a in E, a polynomial f(x) in F[x] is the minimal polynomial of a if f(a) = 0 and f(x) is the polynomial with the smallest possible degree that still satisfies the prior conditions. It should be noted that such a polynomial is irreducible over F. 
            -Example: The real numbers extend the rational numbers. Consider the number sqrt(2). The minimal polynomial in Q[x] of sqrt(2) is f(x) = x^2 - 2. However, consider the real numbers, which we can interpret as extending themselves in some trivial sense of the word. Then, the minimal polynomial in R[x] for sqrt(2) is simply f(x) = x - sqrt(2). This is why the notion of a field extension is involved; the subfield determines the coefficients of the polynomial and therefore restricts our choices for it.
        -Division ring: A field in which multiplication is not commutative.
-Lattice: A lattice is, informally, a repeating arrangement of points in n-dimensional space. More formally, a lattice is a discrete (the elements are not continuous and occur in set, isolated jumps) subgroup of R^n such that that lattice spans the vector space R^n; the lattice is then said to have dimension n. Every possible lattice in R^n can be generated by taking all Z-linear combinations (a linear combination in which only integer scaling is permitted; the Z-linear span of a set of vectors {v_1, ..., v_n} is any vector x of the form x = m_1 * v_1 + ... + m_n * v_n, where m_i are integers for i = 1, 2, ..., n) of the basis vectors for the vector space R^n. This basis does not need to be the basis for R^n, however, and lattices can exist as structures that are (mostly) independent from R^n. The set of vectors mentioned before, {v_1, ..., v_n} is the basis for the lattice. An n-dimensional lattice is, essentially, the same thing as n-dimensional space R^n except instead of all possible n-tuples being 
      included in the structure, as they are in the (continuous) R^n, only Z-linear points are permitted (forming a discrete structure that also gives rise to the repeating and tesselLating arrangement of points). 
    -Hypervolume: (A.K.A. volume or discriminant) The hypervolume of a lattice is the determinant of the matrix formed by using each vector in the basis of the lattice as a column.
    -Important computational problems: The two most important (and very difficult) problems in lattice theory are the Shortest Vector Problem and the Closest Vector Problem. 
        -Shortest Vector Problem: For a lattice L, find the shortest non-zero vector in L.
        -Closest Vector Problem: Let x denote a vector in R^n that is not in lattice L; find a vector in L that is closest to x out of all vectors in L.
-Cosets: A coset of a group g is defined as follows. Let H be a subgroup of group G and let g be an element in G; the left coset of H in G with respect to g = gH = {gh | h is in H}; the right coset of H in G with respect to g = Hg = {hg | h is in H}. 
-Types of Numbers, Number Theory:
    -Divisor Function (lower case sigma, or s(n)) The sum of all of the divisors of the argument "n" raised to the subscript "k"th power (e.g. s-sub(0)(n) gives the number of factors n has because all of the factors reduce to 1 when raised to the 0th power.
        -Harmonic divisor number: (A.K.A. Ore number) number whose divisors have an integer harmonic mean.
            -Harmonic mean: (A.K.A. sub-contrary mean) Reciprocal of the arithmetic mean of the reciprocals of the data set.
        -Perfect Number: Positive integer with the sum of its factors being itself (i.e. 6 = 1 + 2 + 3). The first few are: 6, 28, 496, 8128, 33550336, ... (the first four were known to the Greeks; the definition was first posited by Euclid in his Elements). There are 48 known perfect numbers (up to (2^(57885160))*(2^57885161 - 1)). It is unknown whether or not there are infinitely many perfect numbers.
            -Euclid-Euler theorem: All even perfect numbers can be expressed in the form (2^(p - 1))*(2^p - 1), where 2^p - 1 is a prime number (specifically, a Mersenne prime). In other words, there is a one-to-one correspondence between the Mersenne primes and the perfect numbers
                -Proof: Consider the number (2^(p - 1))(2^p - 1) for prime p. We will show that this is a perfect number by showing that the sum of its proper factors is itself. Now we must figure out what all the factors of (2^(p - 1))(2^p - 1) are. 2^(p - 1) is just a power of two, so every power of two between 2^0 and 2^(p - 1) is a factor. (2^p - 1) is a Mersenne prime and so its only factor is 1 and itself. Lastly, any of of the factors of 2^(p - 1) multiplied by any of the factors of (2^p - 1), which are of course only 1 and (2^p - 1), are also factors of our original number (2^(p - 1))(2^p - 1). Thus the sum of all the factors is: (sum from i = 0 to i = p - 1 of (2^i)) + (1)(2^p - 1) + (2)(2^p - 1) + (4)(2^p - 1) + ... + (2^(p - 1))(2^p - 1) = (sum from i = 0 to i = p - 1 of (2^i)) + (sum from i = 0 to i = p - 1 of (2^i))(2^p - 1) = (sum from i = 0 to i = p - 1 of (2^i))(2^p). Since the first term in the product is a geometric series, we can find its summation using the formula for a geometric series. Recall that
                    for a geometric series S = (sum from i = 0 to i = m of (r^i)) for real number r != 1, S = (1 - r^n)/(1 - r). Therefore, (sum from i = 0 to i = p - 1 of (2^i)) = (1 - 2^p)/(1 - 2) = (2^p - 1). Therefore, simplifying our expression for the sum of factors of (2^(p - 1))(2^p - 1), we arrive at: (2^p - 1)(2^p) = 2*((2^(p - 1))(2^p - 1)). Therefore, the sum of divisors of (2^(p - 1))(2^p - 1) = sigma((2^(p - 1))(2^p - 1)) = 2*((2^(p - 1))(2^p - 1)), and therefore (2^(p - 1))(2^p - 1) is always a perfect number for Mersenne prime 2^(p - 1).
                -Note that for (2^n - 1) to be a prime number, n must also be prime.
            -Odd perfect numbers: An odd perfect number has never been found in existence, and yet has not been proven that odd perfect numbers cannot exist. Finding an odd perfect number or disproving their existence is an open problem in number theory.
                -It has been proven that any odd perfect number, should it exist, must exceed 10^300.
            -Perfect numbers always have a digital root of 1.
                -Digital root: The digital root of a number is a single digit obtained through an iterative process: (1) sum the digits of the number, (2) If the resulting number has more than one digit, repeat, (3) Once you reach a number with only one digit (ie a number between 1 and 9), stop.
            -Even perfect numbers always end in either 6 or 28.
            -Sum of reciprocals of non-trivial factors is 1.
            -A perfect number, expressed as 2^(n - 1)*(2^n - 1), is equal to the sum of the first 2^((n - 1)/2) (which is just the square root of 2^(n - 1), which is guaranteed to be a perfect square since n is an odd prime, making (n - 1) even) odd numbers, each cubed. For example, 8128 = (2^(7 - 1))*(2^7 - 1) = 1^3 + 3^3 + 5^3 + 7^3 + 9^3 + 11^3 + 13^3 + 15^3.
        -Superperfect number: A number n is superperfect if sigma(sigma(n)) = 2n.
    -Pseudo-perfect Number: Same as perfect number, but allows the number to be a sum of SOME of its factors
    -Abundant Number: A number n for which the sum of the factors of n (including n itself) > 2n
    -Wierd Number: Abundant number that is NOT pseudo-perfect. That is, a number whose integer divisors, not including itself, sum to greater than the number, but no subset of divisors adds to the number itself.
    -Mersenne Prime: Prime number that is one less than a power of two (i.e. 2^x - 1)
    -Quaternion: Number system extending the complex numbers. A quaternion is the quotient of two vectors; the product of two quaternions is therefore non-commutative (order in which the quaternions are multiplied yields different results). Can be represented as a scalar plus a vector.
    -Hypercomplex number: An element (a scalar, tensor, etc.) of an algebra over a field (a vector space that is the bilinear product (the matrix multiplication of two vector spaces)) in which the bilinear vector spaces are a real number vector space and a complex number space   
    -Narcissistic number: A number that is the sum of each of its digits raised to the number of digits the number has. It depends on the base of the number system used.
        -Base 10: The first few base-10 narcissistic numbers are: 0-9, 153, 370, 371, 407, 1634, 8208, 9474, ...
____________________________________________________________________________________________________
____________________________________________________________________________________________________
                    PSYCHOLOGY
1. Psychological Research:
-Psychology: study of human cognitions, emotions, and behavior. 5 goals of psychology:
    1. Descriptions: Compilation of descriptive data.
    2. Theory: Creating frameworks to explain trends in the data; why did the subject behave the way he or she did?
    3. Predict: Extrapolate further trends.
    4. Act: Use knowledge to control, influence, or change society for the better.
    5. Improve and repeat
-Research biases: favoring certain outcomes over others; negatively affects integrity of the data. In order to compensate for biases, data must be collected from randomly assigned, double-blind (neither the subjects nor the researcher knows who is receiving which treatment) samples 
    -Selection bias: Errors in the grouping of subjects (e.g. non-random grouping affects data trends)
    -Placebo effect: Subject's belief and/or prediction of results is confounded with experimental treatment.
    -Experimenter bias: Bias in the actual collection of data on the part of the researcher.
-Types of Research: 
    -Naturalistic observation: Observing behavior in natural environment.
    -Case study: Following a single case over a long period of time.
    -Survey: Polling a sample of subjects randomly in order to obtain data about the entire population. Only applies to data concerning opinions or characteristics of subjects themselves, as actual questions are ase
    -Correlative studies: Testing for association between two variables.
    -Psychological testing: Performing an experiment to obtain data about an individual or set of individuals.
2. Biopsychology:
-Neuron: specialized cell that receives and transmits information to other cells. Most people have between 10 and 100 billion neurons; about 10,000 die per day and throughout our lifetime we lost about 2%.
    -Structure: Dendrites (long tendrils at one end) receive information. The cell body (a.k.a. soma, contains nucleus and maintains cell's life support) processes information and sends it to the axon (a long string like section cut into sections; they have terminal bulbs at the ends that pass the information to muscles, glands, etc.)
    -Neurotransmitters: Chemicals in the brain that have profound effects on mental health and behavior.
        -Acetylcholine: voluntary movement, learning, memory, and sleep. Too much can cause depression and too little (only in the hypothalamus) can cause dementia.
        -Dopamine: movement, attention, learning. Reward chemical. Too much can lead to schizophrenia, too little can cause depression.
        -Norepinephrine: Eating, alertness. Too much can cause schizophrenia, too little is associated with depression.
        -Epinephrine: energy, glucose metabolism. Too little associated with depression.
        -Serotonin: involved in mood, sleep, aggression, appetite. Too little can lead to depression, some anxiety disorders (e.g. OCD).
        -Gamma-Amino Butyric Acid: (A.K.A. GABA) inhibits excitation and anxiety. Too little can lead to anxiety.
        -Endorphins: Pain relief, pleasure. 
    -Neural structure: contains two main systems: Central Nervous System (CNS) and Peripheral Nervous System (PNS). 
        -Central Nervous System: brain and spinal cord. Cerebral Cortex (higher cognition, emotion, some sensory or motor functions). Four lobes of brain:
            1. Frontal lobe: Motor cortex. Motor behavior, expression (especially through language), higher level cognition, orientation with respect to spacetime and situation.
            2. Parietal lobe: Somatosensory cortex. Processes touch, pressure, temperature, pain. Controls thermoception and nocioception. 
            3. occipital lobe: Visual cortex. Visual interpretation through eyes and visual imagination.
            4. Temporal lobe: Auditory cortex. Understanding language, understanding emotion, memory.
         Spinal cord works with brain but also has functions of its own. Usually not able to be consciously controlled. Consists of:
            -Brainstem: life sustaining functions (e.g. breathing, heartbeat).
                -Medulla oblongata: controls heartbeat, breathing, blood pressure, digestion. 
                -Reticular activating system: Sexual arousal, attention, sleep, reflexes. 
                -Pons: regulates arousal, involved in dreams.
         Other parts of the CNS:h.
            -Cerebellum: Balance, posture.
            -Thalamus: Relays information from the senses to the brain. Exception: smell.
            -Hypothalamus: Maintains homeostasis (internal body temperature, metabolism, appetite). Interprets extreme emotion, instigates physical responses.
            -Limbic system: Emotional expression, affects emotions in memories, relates behavior with emotion, involved with motivation.
            -Amygdala: Processes information and attaches emotional significance to it. Mediates defensive behavior as well as aggression.
            -Hippocampus: Large role in memory, converts short term memory to long term memory.
        -Peripheral Nervous System: involved in threat response, sub divided into two systems.
            -Somatic Nervous System: Regulates skeletal muscles and voluntary activity. 
            -Autonomic Nervous System: involuntary activity similar to that regulated by brainstem. This activity involuntary changes with the processing of information about the environment (e.g. seeing a lion drives blood pressure up). Further sub divided:
                -Sympathetic Nervous System: Controls Fight-or-Flight response. 
                    -3 Stages: 1. Alert: possible threat recognized. Heart rate quickens, muscles tense, breathing rate quickens, pupils dilate (to improve vision), certain biological processes temporarily ceased to divert energy for immediate use (e.g. digestion, fingernail and toenail growth, immune response, etc); body prepares itself for action. 
                           2. Response: Body selects course of action to deal with threat; this can either be Fight or Flight, as the name implies.
                           3. Fatigue: Body reels from sudden decrease in energy; biological processes return to normal.
                -Parasympathetic Nervous System: Deals with stage 3 of Fight-or-Flight response. 
3. Development of the Personality:
-Motor development:
    -2 months: able to lift head by himself or herself.
    -3 months: able to roll over.
    -4 months: able to maintain rudimentary balance (e.g. sitting with support)
    -6 months: sense of balance further develops (e.g. sitting without support)
    -7 months: sense of balance further develops (e.g. standing with support)
    -9 months: sense of balance further develops (e.g. walking with support)
    -10 months: sense of balance further develops (e.g. standing without support)
    -12 months: sense of balance further develops (e.g. walking without support)
    -14 months: Equilibrioception development complete (e.g. can walk backward without support)
    -18 months: able to manipulate objects (e.g. kicking a soccer ball)
-Cognitive development: Jean Piaget's theory divides development into 4 stages:
    1. Sensorimotor stage: (0-2 years) Learning to manipulate objects. 
        -Object permanency: ability to understand that objects continue to exist even when out of sensory reach.
    2. Preoperational stage: (2-7 years) Development of language accelerates throughout rapidly. Heightened interaction with environment. 
        -Egocentrism: Belief that everyone sees the world as the child does. Child fails to understand that perceptions vary. 
        -Conservation: Child is only able to focus on one dimension or aspect of an object; inability to understand reversibility (e.g. children think that water in a wide beaker, even when poured directly from a taller yet thinner beaker in front of them, contains more liquid).
    3. Concrete operations stage: (7-12 years) Centrism disappears and concept of reversibility develops. Imagined objects still remain mystical.
        -Grouping: children understand the concept of groups despite differences in direct characteristics (e.g. quarters and pennies are both coins despite size and color difference).
    4. Formal operations stage: (12+ years) Abstract thinking develops. Able to develop personal theories about world. Failure to reach this stage correlated with low intelligence. 
-Erikson's Stages of Psychosocial Development: 
    1. Trusting: (0-1 years) Children learn to trust, though the trust is based on consistency of care provided. Child gains confidence and security about world. Failure to complete stage results in lifelong trust issues.
    2. Independence: (1-3 years) Children let go of caregivers (e.g. walking away from mother). Confidence in one's ability to survive increases. Failure to complete stage (caused by criticized, overly controlled, etc.) results in lifelong shame and self doubt.
    3. Autonomy: (3-6 years) Children assert themselves. They take the initiative with planning things. Failure to complete stage (caused by criticism or over control) leads to lifelong inability to lead.
    4. Confidence: (6 years-puberty) Pride in accomplishments develops. Children become more outgoing. Failure to complete stage has same consequences as those of failing to complete third stage.
    5. Self Identity: (Puberty) Transition from childhood to adulthood. Children look ahead in life for long term goals (e.g. college, career, family life, etc). Experimentation and exploration with the self is common. 
    6. Intimacy: (Young adulthood) Ability to become intimate with others. Relationships with others explored. Fosters sense of commitment, caring, and safety.
    7. Productivity: (Adulthood) Life settles down; careers established, long term relationship established, etc. People begin to give back to society and see themselves as part of the bigger picture. 
    8. Ego integrity: (Old age) Explore life as retired. Contemplate life accomplishments and experiences. Failure to complete this stage results in lifelong regret and despair. 
-Freud's Stages of Psychosexual Development:
    1. Oral stage (0-18 months) Child focuses on oral pleasures; sucking. Too little can result in Oral Fixation and too much can lead to habits such as drinking, smoking, etc. 
    2. Anal stage: (18 months-3 years) Pleasure shifts to elimination of feces. Child must learn to control this pleasure. Too much control can cause Anal Fixation, leading to obsession with cleanliness and organization (anal retentive).
    3. Phallic stage: (3-6 years) Pleasure switches to genitalia. Oedipus Complex (Electra Complex in girls): Children develop unconscious sexual attractions towards the parent of the opposite sex and grow distant to the parent of the same sex, whom they see as a competitor. Children must learn to suppress their sexual desires and to identify with their "competitive parent".
    4. Latency stage: (6 years-puberty) All sexual urges repressed; children interact with same sex peers.
    5. Genital stage: (puberty+) Pleasure shifted to genitalia, sexual urges reawakened.
-Freud's Structural Model: Model to describe the brain and its cognition.
    -Structural model: The persona is split up into three parts: the Id, the Ego, and the Superego. 
        -Id: Part of personality most dominant in early years - almost completely so in early years of life - and based on pleasure centers. Focuses on basic instinctive desires regardless of higher level processing of the environment; when the Id is hungry it demands food. Purely selfish portion of the persona that only takes into account personal basic needs.
            -If the Id gets too strong, the person will act purely on impulses and immediate self gratification. 
        -Ego: Develops in middle of childhood as children interact with their peers and learn how society and civilization are structured. Ego mediates and regulates the Id, understanding that meeting the Id's needs is important but not ultimately so, and acting on pure selfish base instinct can be harmful in the long run. 
        -Superego: Develops around the end of the phallic stage. Develops as a result of moral and ethical restraints placed on children; when society tells children that he or she cannot hurt others for their benefit (e.g. moral restraints) the superego develops to adapt to the non-Darwinian environment. 
            -If the Superego gets too strong the person will adhere to strict moral guidelines regardless of circumstances and would be unbending in situations of the world.
    -Topographical model: Underlying emotions, beliefs, and impulses are available to all on the conscious level. Most of what drives human behavior and decision making is stored in the the unconscious;  the unconscious is inaccessible and buried. 
        -Preconscious: (A.K.A. subconscious) Below the surface but still accessible (e.g. childhood memories).
-Ego Defense Mechanisms: Ego's way of maintaining the balance between the Id and Superego, or even giving one or the other the upper hand, all depending on circumstances.
    -Denial: Convincing oneself that the anxiety provoking stimulus simply doesn't exist.
    -Displacement: Venting emotional impulses of the Id on a less threatening target (e.g. slamming a door when upset)
    -Intellectualization: Focusing on the minutiae of an anxiety provoking stimulus so as to banish the (unpleasant) emotions from the conscious (e.g. spending time on the logistics of a funeral instead of on mourning)
    -Projection: Simply projecting impulses onto someone else, especially when provoked (e.g. the use of ad homenim in an argument)
    -Rationalization: Supplying a logical explanation for an anxiety provoking stimulus. 
        -Techniques of neutralization: How people who commit immoral acts try to deal with the pressure. They are only present in delinquents, not actual criminals who do not struggle with moral ambiguities, such as psychopaths.
            -Denial of responsibility: Perpetrator posits that events weren't in his or her locus of control.
            -Denial of injury: Perpetrator denies the negative impact of the actions he or she took. 
            -Denial of victim: Perpetrator justifies his or her act against a victim through quid pro quo or other moral rationalizations. Commonly used is dehumanization of the victim.
            -Condemnation of condemners: Perpetrator attacks (often with ad homenim) his or her accusers.
            -Greater Good: Perpetrator justifies his or her actions by arguing that they had positive impacts in the long run. 
    -Regression: Returning to a prior stage of development (e.g. crying in a corner when sad, taking solace in a childhood toy)
    -Repression: Pulling memories of the anxiety provoking stimulus into the unconscious.
    -Sublimation: Taking out socially unacceptable impulses (of the Id) in a socially acceptable way (e.g. aggressive and violent people become boxers, people who have an impulse to cut become surgeons).
-Kohlberg's Stages of Moral Development: Breaks down development of personal ethics into three stages.
    1. Pre-conventional stage: (0-9 years) Avoiding pain and moving towards reward is moral; morality synonymous with personal hedonism. Children see the rules set by their parents as defining morality. 
    2. Conventional stage: (9 years-end of puberty) Children understand societal expectations, and achieving these standards is morality. Fulfilling obligations is moral.
    3. Post-conventional stage: (adulthood) People understand differences in perception and moral creeds. Morality is seen as following not base instincts, but upholding higher level concepts of basic instinct (e.g. upholding instinct for self preservation by believing it is moral to save another's life). Upholding values of one's culture is moral. People can judge themselves and create their own senses of morality. 
4. Human Learning and Behavior:
-Learning: Compiling and using data to reinterpret the environment and subsequently alter behavior. John B. Watson, founder of Behaviorism (main principle: only observable behaviors are worthy of research because thoughts are subjective) 
-Classical conditioning: (Pavlovian learning) The association of two stimuli with each other. Ivan Pavlov, while doing gastrointestinal research, rung a bell every time he fed some dogs, and observed first that the dogs' natural response to the food was salivation and second that after some time the dogs would salivate just after hearing the bell but without receiving any food; they had associated the bell to food. 
    -Unconditioned stimulus: The natural stimulus (food in Pavlov's dogs).
    -Unconditioned Response: The natural response (salivation in Pavlov's dogs).
    -Conditioned stimulus: The stimulus anchored to the unconditioned stimulus (the bell in Pavlov's dogs).
    -Conditioned response: The same response as the unconditioned response but occurring following that conditioned stimulus, not the unconditioned stimulus (salivation following the bell in Pavlov's dogs).
-Operant conditioning: Reinforcing behavior by associating it to outcomes.
    -Positive reinforcement: Adding a positive to increase the frequency of a behavior.
    -Negative reinforcement: Imposing a negative that is removed after the desired behavior is performed in order to increase the frequency of that behavior.
        -E.g. nagging someone until they finish a task.
    -Punishment: Adding a negative following an undesired behavior in order to decrease the frequency of that behavior.
    -Extinction: Taking away a positive following an undesired behavior in order to decrease the frequency of that behavior.
-Reinforcement Schedules: Govern the classifications of situations as matched to conditioning techniques.
    -Continuous schedule: applying reinforcement every time the trigger behavior occurs. Ideal for punishment. Two types:
        -Fixed ratio: Applying reinforcement after a set number of behaviors. 
            -Problems: Subjects adapt to the ratio; behavior doesn't change until the preset number of triggers.
        -Fixed interval: Applying reinforcement after a fixed duration of time.
            -Problems: Subjects adapt to interval; behavior is only altered around the time of reinforcement.
    -Variable schedule: Reinforcement applied on an irregular basis.
        -Variable ratio: Applying reinforcement after a variable number of trigger behaviors. 
            -Usually works the best.
        -Variable interval: Applying reinforcement after variable durations of time. 
            -Works well, but not for punishment.
5. Sensation and Perception:
-Sensation: Process of gathering and processing information.
    -Absolute threshold: Brightline at which sensory motivation is noticeable to the human senses.
    -Difference threshold: The range required for a difference in sensation to be perceived. 
    -Signal detection theory: Selectively separating certain sensory stimuli from others, such as holding a conversation in a crowded and noisy room.
    -Sensory adaption: Sensory stimuli become redundant after extended exposure without being linked to an outcome. 
-Perception: Processing of information through the senses.
    -Gestalt principles of grouping: Our brain naturally groups objects together (e.g. five dots are seen as a one group of dots, not "dot, dot, dot, dot ,dot") to save time and resources. Four principles:
        1. Similarity: Grouping based on shared physical characteristics.
        2. Proximity: Relative closeness tends to effect grouping.
        3. Continuity: Pattern recognition, even when patterns aren't there, causes the perception of things as parts of a continuous pattern.
        4. Closure: Tendency to complete objects with gaps in them.
6. Memory, Intelligence, and States of Mind: 
-Process of creating a memory:
    1. Encoding: Taking in information and processing it so as to transform it to a state that can be stored.
    2. Storage: The process of recording the memory.
    3. Retrieval: Bringing the memory out of storage and reversing the encoding process to access the memory's content.
-Sensory memory: The content received by our senses; lasts only a couple seconds. It allows us to process and navigate the environment around us.
-Short term memory: Memories that are transferred from sensory memory to the conscious or aware sphere of mind; a window of about 30 seconds. Short term memory is estimated to allow 5-9 objects stored at once.
    -Working memory: Memory designed to hold multiple changing variables at once; allows us to concentrate on a complex task longer than the window of short term memory would allow. 
    -Displacement: New information has priority over old information in terms of what is remembered.
-Long term memory: Permanent and hypothesized to be infinite in storage capability. 
    -Declarative memory: Memory of facts, details, ideas, and concepts.
        -Semantic memory: Factual knowledge, especially that of languages, word meanings, conceptual ideas, and mathematical skill.
        -Episodic memory: Situations, events, life stories, etc.
    -Non-declarative memory: (A.K.A. implicit memory) Memory of conditioned habits and practiced movements or ideas; using previously stored memory over and over again.
-Memory categorization: The way the brain prioritizes and remembers information.
    -Primacy effect: Information received first is remembered well. 
    -Recency effect: Information most recently received is remembered well, probably due to displacement.
    -Distinctiveness: Information in stark contrast to commonly perceived information or to the information around it is remembered well.
    -Frequency effect: Rehearsed memories or actions are remembered well; practice greatly improves the storage of a memory.
    -Associations: Linking pieces of information together makes both easier to remember; alternatively, linking new information to already well memorized information makes both easier to remember.
    -Reconstruction: Missing parts of a memory; holes in a memory; can be recreated by the brain, often erroneously.
    -Significance: Significant information is automatically transferred from short term to long term memory.
-Retention loss: (A.K.A. forgetting) Many memories are destroyed by the brain, though rarely completely eradicated. Information not used for long periods of time decays. Some information is still stored but cannot be retrieved. Four types of forgetting:
    1. Repressive erasure: Removing every association or link to a memory can cause the memory to be forgotten. Governments around the world can attempt to remove every artifact that reminds a person of a certain memory to make them forget it.
    2. New identity: Forgetting past information about oneself to better live with a new self.  
    3. Structural amnesia: People only remember other significant people.
    4. Annulment: Forgetting things because there is too much information.

7. Motivation and Emotion
-Motivation: Motivation is the energy and drive that causes a person to go to lengths in order to achieve something. The amount of motivation that people have varies greatly from person to person, and there are five major theories of motivation in modern psychology.
    -Drive reduction theory: This theory proposes that people are motivated because they are averse to certain drives and internal emotions and so are motivated to extinguish them. For example, the same way people are averse to hunger and are therefore motivated to eat, people are averse to states of stagnation, tension, or arousal, and so are motivated to reduce these states
    -Arousal theory: This is similar to the drive reduction theory, and states that people need to maintain some baseline level of arousal in order to feel comfortable, and are thusly motivated to keep their internal state at that level. 
    -Psychoanalytic theory: This is a theory similar to instinct theory and is Freudian in nature. Freud believed that two main forces shaped every one of our decisions: Eros and Thanatos, of the life and death drives, respectively. The desire to survive and prevent our destruction are the two main forces that drive everything we do, but unlike instinct theory, psychoanalytic theory posits that most of these decisions are made in the unconscious mind. 
    -Humanistic theory: This is the most well known and common model for motivation, and states that humans are innately driven to attain their maximum potential, and will always push towards this goal unless obstacles are placed in the way, such as hunger, thirst, financial difficulties, danger and crime, etc. This is the theory that gives rise to Maslow's hierarchy of needs, a pyramid, the layers of which are, from bottom to top: physiological needs (eg food, water, air, shelter, sleep, etc.), safety needs (general safety from danger, injury, and death due to violence or predation), belonging and love needs (eg the feeling of being accepted, loved, belonging in a society or group, etc.), esteem needs (the need for achievement and praise, social status, to accomplish goals and self-improvement, competence and respect), and lastly, the need for self-actualization (ie the need to realize one's full potential as a person). According to the humanist theory, human lives are spent climbing towards the top of the pyramid, conquering a layer once they obstacles it entails no longer pose a threat. No one ever reaches the top of the pyramid, but our lives are spent trying to get as close as possible. 
-Emotion: The idea of emotion is well known by people, and yet a comprehensive and precise definition does not exist. There are many theories, five main ones, as to why people experience emotion.
    -James-Lange theory: This theory argues that an event is aroused by a physiological event, and the emotion that results is our minds interpreting the event. 
    -Cannon-Bard theory: This theory is similar to the James-Lange theory, but instead of an event triggering a psychological arousal which then triggers an emotion, the arousal and emotion are immediately triggered in parallel by an event. 
    -Schachter-Singer theory: This is the same theory as the James-Lange theory, but rather than our minds interpreted the psychological arousal that comes after an event, our minds identify a reason for the arousal and then label the reason as an emotion.
    -Lazarus theory: This theory asserts that before an emotion can present itself r be felt, a thought must first occur in the brain, and so when an event occurs, one thinks about the event and its ramifications before experiencing an emotion.
    -Facial feedback theory: This theory posits that events cause reflexive changes in the position of our facial muscles, such as smiling, frowning, clenching teeth, etc., and our brain interprets these responses as different emotions.

8. Social Psychology
-Humans are primarily social creatures, and so our behavior in society and in any group of humans is influenced by the people around us. There are a few different theories concerning attribution (how we interpret people around us and their behavior) and attraction (what we look for in romantic or sexual partners).
-Attribution theory: This theory posits that humans view others' behavior in terms of causes, and form opinions and judge behavior based on what they think the cause of the behavior is. There are two types of causes: situational (ie external) and dispositional (ie internal); the former explains behavior as a consequence of environmental or uncontrollable circumstances whereas the latter explains behavior as intrinsic to a person. Although we weigh a variety of factors in interpreting behavior, the central idea is what caused it. The theory also suggests two errors that humans make as a result of the process by which they view other peoples' behavior.
    -Fundamental attribution error: This is the tendency to overestimate the internal factors in behavior while underestimating the external factors. 
    -Self-serving bias: This explains why humans tend to assign internal causes to successes but external causes to failures. For people with inferiority complexes, the opposite is true.
-Attraction: Although it may seem as though we choose those we associate with and who our friends are, there are actually rather predictable factors that influence friendship. This is to be expected, as humans are inherently social animals and so would not evolve to be extremely picky in who they associate with.
    -Factors: These factors correlate highly to friendships and even to romantic relationships.
        -Proximity: A huge factor in deciding friendships is basic geographic proximity. The overwhelming majority of friendships take place between people who see each other often and live close by. Friendships develop after two people get to know each other, usually disregarding most other factors, and this closeness is best facilitated through physical proximity. 
        -Association: This is a relatively minor factor but does play a role, and simply states that we associate opinions about our current environment with the people we meet in that environment. 
        -Similarity: This is the best known factor in determining friendship. Put simply, when two people share hobbies, dislikes, and other characteristics about themselves, they are more likely to be friends or romantic partners.
        -Reciprocal liking: This is a simple principle - we tend to enjoy the companies of those who already like us. 
        -Physical attractiveness: Lastly, how physically attractive a person is plays a role in their friendships, though much more so in their romantic relationships. The latter is due to evolutionary reasons (attractive characteristics coincide with characteristics of fertile, healthy, and biologically fit characteristics), and the former may be due to the link between physical attractiveness and social status. We also tend to choose people who we view as close in physical attractiveness to our own.
-Behavior in groups: 
    -Social facilitation: The principle that when in the presence of others, even when we don't know them, we perform better at learned tasks but worse on new tasks that we are not familiar with. This is because when we are alone, we are "ourselves"; when in the presence of others, we become more self-conscious and watch our own behavior, inducing a state of heightened self-awareness. This heightened perception of our surroundings can make tasks that we are already good at easier, but when learning a new task, the prospect of failure in front of others is too big a an obstacle to our focus for us to perform well.
    -Group think: Group think is the phenomenon in which when in a group, our preferred and expected outcome is instinctively group harmony, when it comes to agreeing or disagreeing on a subject. When everyone in the group agrees, everyone is more at peace, and also more likely to voice opinions that they normally would restrain or second guess, if they think that opinion is aligned with the group sentiment. Similarly, any sign of dissent from the group mindset is stifled and discouraged. Therefore, what ends up happening is impulsive and outlandish decisions that any one individual might not make, focused in a singular direction, such as the Ku Klux Klan or lynch mobs. The concept is closely related to the concept of mob mentality.
    -Social loafing: This is the observation that the contribution that an individual (in a group) makes to the group's well being varies inversely with the size of the group; in other words, the more people there are in a group, the less any given individual will contribute to the group. This is due to the shift in distribution of responsibility and lack of accountability that comes with rising group sizes.
    -Bystander effect: It has been observed that in situations where an individual would certainly be compelled to intervene, such as a mugging in broad daylight (most people would cal 911 as a reaction), if there are a large number of people observing the incident and therefore a large number of potential people who can intervene, the probability of any one person intervening diminishes. This is due to the ambiguity that arises with many people observing the incident; any one person feels less responsibility to intervene, since others might do it anyways, and does not feel the same urge to help when there are many others who could do so and are expected to do so. When every observer thinks this way, the result is a total lack of intervention, even though, paradoxically, the fewer observers there are, the more likely that someone will intervene.

9. Psychopathology
(***INCOMPLETE***)

10. Psychotherapy
(***INCOMPLETE***)
____________________________________________________________________________________________________

-Dilatant material: Non-Newtonian fluids used for body armor. Kevlar stops a bullet by (1) distributing the energy of impact over a large area, and (2) stretching the fibers of Kevlar to further use energy and slow the bullet down. It takes 20-40 layers of Kevlar to stop a bullet.
    -Shear-Thickening fluid: A liquid that behaves like a solid when it encounters shear (mechanical stress). This fluid is a colloid of particles suspended in the liquid, too big to dissolve, and too electromagnetically repulsive to settle together or sink to the bottom. A sudden shear force temporarily overcomes these repulsive forces and forms hydroclusters (clusters of these particles temporarily formed in response to a shear force and that behaves as a solid). When the impact energy dissipates, the repulsive forces form a colloidal solution again.
        -Example: Oobleck - A solution of equal parts of cornstarch and water. Such a solution solidifies when struck, can be molded into a ball that falls apart when pressure is released, and can be walked on very quickly but causes an object or organism to sink if it stops moving (exerting force on the solution).
        -In body armor, the solution is one of silica (component of sand and quartz) particles suspended in polyethylene glycol (used in laxatives and lubricants). Production process:
        1. Solution is created.
        2. Fluid is diluted in ethanol.
        3. The saturated fluid is heated to evaporate the ethanol. 
        4. The fluid permeates the Kevlar, and the Kevlar strands hold the fluid in place.  
        -STF dipped Kevlar has a 2:7 strength ratio with normal Kevlar yet has no difference in flexibility. The former also has less tensile fibers due to the fluid, so bullets have less penetration. 
    -Magnetorheological fluid: Oils filled with iron particles (typically at 20%-40% of the total volume and 3-10 microns each), often with surfactants (materials that lower the surface tension of a fluid) to keep particles suspended in fluid. The iron particles line up and behave like a solid when exposed to a magnetic field over a time interval of about a 20,000th of a second. This method requires a constant magnetic field, and the proposal of running wires with electric current (that would produce a magnetic field) requires electricity. 
-Wait Calculation: The optimal amount of time one should wait before making long distance (i.e. interstellar) trips, due to the fact that during the travel time, travelers back at the origin (i.e. earth) would be able to later, perhaps thousands of years later, overtake the original traveler who left thousands of years ago, through his or her utilization of faster velocities.  
    -Assuming that maximum velocity attained is exponentially growing, 
        (T_(now))/(T_(t)) = (1 + r)^(0.5t) where r = annual rate of increase in world power production. 
-Impact depth: Depth to which a projectile traveling at high velocity will embed itself into a surface
    -Newton's approximation: A cylinder shaped object has an impact depth approximately equal to its length multiplied by its relative density with respect to the target material.
        -Relative density: (A.K.A. specific gravity) The ratio of the density of one object to another. 
        -The approximation is independent of velocity so long as it is sufficiently high; as high enough velocities, all objects begin to behave as fluids.
        -Assumes that the velocity at which the target material is being pushed aside by the projectile is equal to the velocity of the projectile.
-Theory of mind: Cognitive self-awareness; the ability to understand that one has beliefs, intents, desires, etc. and that others also have these mental states, and that these mental states differ from one's own. It is apparent in some great apes, though not as much as in humans; in humans it develops at age 4, and never in autistic kids.
-Bateman's Principle: Females invest more energy and resources in their offspring than males do, and so males compete over females; females are a limited resource in high demand.
    -Females can have a limited number of offspring but males can have infinite. 
-Scarcity principle: People put higher value on things that are scarce  and lower value on things that are widely abundant. That which we cannot have makes us desire it more.
-Superfunction: Mathematical construct. A regular function (algebraic, etc.) but iterated upon itself. S(z; x) = f(f(...f(x)...) is a superfunction of x valued z times (i.e. at z iterations).
    -The notation S(z; x) is only valid when the domain of z is positive integers.
    -Hyperoperation: An infinite sequence of arithmetic operators. 
        -Hyper 0: b + 1; 1 + 1 + 1 + ... + 1 with b 1's.
        -Hyper 1: a + b; a + 1 + 1 + ... + 1 with b 1's.
            -A.K.A. addition
        -Hyper 2: ab; a + a + a + ... a with b a's.
            -A.K.A. multiplication
        -Hyper 3: a^b; (a)(a)(...)(a) with b a's.
            -A.K.A. exponentiation
        -Hyper 4: a (2-arrow) b; a^(a^(a^(...)^(a))) with b a's.
            -A.K.A. tetration
        -Hyper 5: a (3-arrow) b; a (2-arrow) (a (2-arrow) (a (2-arrow) ((...) (2-arrow) a))) with b a's.        
            -A.K.A. pentation
        -Hyper 6: a (4-arrow) b
            -A.K.A. hexation
    -Knuth's up-arrow notation: n-arrows means (a ((n-1) up-arrows) b), where b is the argument (i.e. a (up-arrow, up-arrow, ..., up-arrow) b, with n up-arrows).
        -Example: 10 (triple-arrow) 3 = 10 (double-arrow) (10 (double-arrow) 10)    
-Leidenfrost effect: When a liquid comes into immediate contact with a mass much hotter than its boiling point (the Leidenfrost point), the outer molecules of the liquid evaporate so fast that the remaining liquid is protected from the heat by a cushion of vapor, and thus lasts much longer as a liquid than it would otherwise. 
    -Explains why water sprinkled on pans at very high temperature form droplets that skid around the pan.
    -Leads to the experiment in which a man dips a finger in water and then molten lead and pulls it out unharmed.
-Gun suppressor: The main sound from a gun comes from the expanding hot gases that explode and propel the bullet. To suppress this sound, a suppressor has a relatively huge volume with many compartments, called baffles, that allow the hot gases to expand and expend pressure over more time, thus reducing the sound. 
    -Bullets traveling at supersonic speeds produce a sonic boom that suppressors can do nothing about; many suppressors therefore reduce the velocity of the bullet.
-Card counting: Keeping track of which type of cards are dealt and when in order to formulate a betting strategy that maximizes payoff.
    -When many low cards are dealt, there is a positive count because there is a higher concentration of high cards, favorable to the player, in the deck; thus the player bets larger amounts.
        -Positive count is good because it (1) causes the dealer the bust more often, (2) produces better (higher) starting hands, and (3) players are dealt more blackjacks.
    -Balanced counting system: values are assigned to cards such that the entire deck has a net count of 0.
     Unbalanced counting system: net count is not 0.
    -Hi-Lo system: cards 2-6 are assigned a value of +1, cards 7-9 are assigned a value of 0, and cards 10, J, Q, K, A are assigned a value of -1.
    -Hi-Opt I: (A.K.A. high optimum 1) same as hi-lo system, but 2s and aces carry a value of 0. The number of aces played is counted separately, often with the aid of external tools such as fingers or feet.
    -Hi-Opt II: (A.K.A. high optimum 2) Cards 2-3 and 6-7 have a value of +1, cards 4-5 have a value of +2, cards 10, J, Q, K have a value of -2, and cards 8-9 and aces have a value of 0. Ace count is kept separately as in the Hi-Opt I system.
    -Running count: The running total of the count values so far.
     True count: The running count divided by the number of decks remaining (which can be found by subtracted the number of decks in the discard pile from the starting number). 
        -When the true count is a non-integer, round, truncate, or floor (rounding down always).
-Introspection: Understanding why one believes what one believes, and entertaining the possibility that one is wrong, and if evidence to support this were brought to light, the acceptance that one was wrong and the subsequent changes to one's position will be made.
-Epicureanism: One should not worry about death, because when one is alive death is of no concern, and when one is dead, they no longer exist to mourn it.
    -Tetrapharmakos: "Don't fear god, Don't worry about death; What is good is easy to get, and What is terrible is easy to endure" 
        -Don't fear god: God is not a personal god, merely a concept; pantheism or deism
        -Don't worry about death: epicureanism
        -What is good is easy to get: Basic necessities, such as food, clothing, shelter, etc., are easy to obtain and are all that is necessary to be happy; seeking more requires additional effort but shouldn't. Similar to Buddhism.
        -What is terrible is easy to endure: Pain is either intense or chronic, but rarely both, so it is easy to endure.
-Heuristic: Experience based problem solving
-Phorid fly: A type of parasitoid (like a parasite but they kill their host rather than keeping it alive) that preys on red imported fire ant. Just a dozen ants can take down a nest.
    -Hunting method: The flies hover over the ants and lock onto one, injecting fly eggs into the thorax. The eggs eventually hatch into instar larvae, which burrow their way to the ant's head, where they consume its brain, muscular tissue, and bodily fluids for energy to grow. Eventually, the larvae release and enzyme that causes the membrane holding the ant's head to its body to dissolve, and the head falls off. The larvae pupate in the ant's decapitated skull and fly out through the mouth.
    -Use in pest control: Phorid flies are very effective in controlling red imported fire ants. They prey only on these ants, ignoring other species of ants, other animals, people, human food, feces, etc; they only attack red imported fire ants.
-Toxoplasma gondii: The most common human parasite in the world (1/3 of the global population is infected). It produces mild flu-like symptoms when first contracted but generally doesn't show any additional symptoms thereafter.
    -Main prey: They can reproduce asexually in any warm-blooded animal, but their definitive host is any member of the cat family (a felid). In felids, they can reproduce sexually in the cat's small intestine.
        -Alteration of behavior: They can infect other animals, such as mice, and make them attracted to, rather then repulsed by, the smell of a felid; if the mouse is eaten, they can reproduce within the cat.
-Spherical coordinates: Any points in 3 dimensions can be described with three coordinates: rho, phi, and theta.
    -Rho: Distance from the point to the origin (0,0,0); given by sqrt(x^2 + y^2 + z^2).
        -The locus of points with a constant rho-coordinate is a sphere with radius rho.
    -Phi: Angle between the positive z-axis and the vector from the origin to a given point.
        -The locus of points with a constant phi-coordinate is a cone.
        -1. To find phi: Construct the right triangle with vertices at the origin, the point (x, y), and the point (x, y, z). 
         2. The bottom leg can be found using sqrt(x^2 + y^2), as it is the distance from the origin to the point (x, y). 
         3. The other leg, parallel to the z-axis, is given by the z-coordinate of the point (x, y, z). 
         4. Phi is the complementary angle to the angle opposite the leg with length z and adjacent to the leg with length sqrt(x^2 + y^2).
    -Theta: Usual rectangular (or polarO coordinates; angle between the x-axis and (x, y) for a given point (x, y, z); found by using tan(theta) = y/x
        -The locus of points with a constant theta is a plane.
    -To convert rho, phi, and theta to (x, y, z): 1. z = (rho)(cos(phi))
                             Setting up the right triangle with vertices at the origin, the point (x, y, z), and the point (0, 0, z) produces the angle phi adjacent to the leg with length z and the hypotenuse with length rho. Solve for z.
                              2. x = (rho)(sin(phi))(cos(theta))
                             Setting up the right triangle with vertices at the origin, the point (x, y, z), and the point (x, y, 0) produces a right triangle with the angle theta adjacent to r, the polar coordinate (r = sqrt(x^2 + y^2)), and opposite the leg with length z. 
                             Solving for r produces r = (rho)(sin(phi)). Using the formula x = (r)(cos(theta)), we can derive the above equation.
                              3. y = (rho)(sin(phi))(sin(theta))
                                 Same as (2) but use the formula y = (r)(sin(theta))
    -Azimuth: The angle between the vector from the origin, at the observer, and point (x, y) of an object located at (x, y, z) and a reference vector, usually the North vector in navigation.
    -Altitude: The angle between the vector from the origin, at the observer, and object located at point (x, y, z), and the vector from the origin to the point (x, y).
-Hermitian Matrix: A square (order of n x n) and complex (entries of the form a + bi) matrix such that the matrix is equal to its conjugate transpose (reflecting the entries across the main diagonal from the bottom left to the top tight and then taking the conjugate (negating the imaginary part of "a + bi").
    -Skew-Hermitian matrix: (A.K.A. anti-Hermitian matrix) A matrix such as the above that is equal to the negative of its conjugate transpose. 
-Bivector: A 3-Dimensional vector with complex coordinates. 
    -Standard form: [iv       w+ix], a 2 x 2 complex matrix that represents the bivector q = vi + wj + xk = <v, w, x>.
            [-w + ix   -iv]
    -Bivectors are skew-Hermitian.
-Hyperbolic functions: Counterparts to the (circular; based on the unit circle) trigonometric functions; hyperbolic functions are based on the unit hyperbola, a hyperbola centered at the origin and with equivalent transverse and conjugate axes of 1 (x^2 + y^2 = 1). 
    -The points (cosh(t), sinh(t)) form the unit hyperbola, where t is the hyperbolic angle; sinh(x) = (1/2)(e^x - e^(-x) and cosh(x) = (1/2)(e^x + e^(-x)) 
    -Intuition as imaginary trigonometric functions: The hyperbolic functions are nothing more than the trigonometric functions with imaginary angles.
        1. First note that, from Euler's identity, e^(i*t) = cos(t) + i*sin(t) and therefore e^(-i*t) = cos(t) - i*sin(t) since cosine is even and sine is odd. Adding the two equations, cos(t) = (1/2)(e^(i*t) + e^(-i*t)) and sin(t) = (1/2)(e^(i*t) - e^(-i*t)). 
        2. For cos(t), let theta = i*p. Then cos(i*p) = (1/2)(e^(i*(i*p)) + e^(-i*(i*p))) = (1/2)(e^p + e^(-p)) = cosh(t) and sin(i*p) = (1/2)(e^(i*(i*p)) - e^(-i*(i*p))) = (1/2)(e^p - e^(-p)) = sinh(t).
        3. When solving homogeneous linear differential equations, one can derive a characteristic polynomial (a function of lambda), and using the polynomials roots, derive a general solution. If the roots are imaginary, say i*r and -i*r, then the solution is of the form y(t) = c_1*e^(i*r*t) + c_2*e^(-i*r*t), which reduces to the form y(t) = c_1*cos(r*t) + c_2*sin(r*t). Thus, when the characteristic polynomial of the differential equation has imaginary roots, the solution has deep connections to the trigonometric functions. Now, consider when the characteristic polynomial has real roots, say r and -r. Then the solution is of the form y(t) = c_1*e^(r*t) + c_2 * e^(-r*t).
           Consider: y(t) = c_1*e^(r*t) + c_2*e^(-r*t) = (1/2)c_1*e^(r*t) + (1/2)c_1*e^(r*t) + (1/2)c_2*e^(-r*t) + (1/2)c_2*e^(-r*t) = (1/2)c_1*e^(r*t) + (1/2)c_1*e^(r*t) + (1/2)c_2*e^(-r*t) + (1/2)c_2*e^(-r*t) + ( (1/2)c_1*e^(r*t) - (1/2)c_1*e^(r*t) ) + ( (1/2)c_1*e^(-r*t) - (1/2)c_1*e^(-r*t) ) = (1/2)(c_1*e^(r*t) + c_1*e^(-r*t) + c_2*e^(r*t) + c_2*e^)(-r*t)) + (1/2)(c_1*e^(r*t) - c_1*e^(-r*t) - c_2*e^(r*t) + c_2*e^)(-r*t)) = (c_1 + c_2)( (e^(r*t) + e^(-r*t)) / 2 ) + (c_1 - c_2)( (e^(r*t) - e^(-r*t)) / 2 ) = c_1*cosh(r*t) + c_2*sinh(r*t).
           Thus, when the characteristic polynomial has imaginary roots, the function is trigonometric, but when the characteristic polynomial has real roots, the function is hyperbolic trigonometric.
    -Hyperbolic angle: Parallels the relationship of an angle to a circle, but with a hyperbola. It is the argument of the hyperbolic function.
        -Standard form: The angle between the vector from (0, 0) and (1, 1) and the vector from (0, 0) to (x, 1/x)
        -Magnitude: The magnitude of the hyperbolic angle "a" is twice the area of the region bounded by the x-axis, the unit hyperbola (with points on it of the form (cosh(t), sinh(t))), and the line from (0, 0) to (cosh(a), sinh(a)).
            -The hyperbolic angle is equal to the area of the corresponding hyperbolic sector (the sector bounded by a hyperbola and the vectors <a, 1/a> and <b, 1/b>) of the hyperbola xy = 1. 
    -Inverse hyperbolic functions: Provide a hyperbolic angle for a given hyperbolic function. They are also called area hyperbolic functions, as they provide a hyperbolic angle that produces a hyperbolic sector with a given area. 
        -Arsinh is the inverse hyperbolic sine function, and so on.
-Digits of pi: If there is a though experiment such that a big ball with mass A has some initial velocity and hits a little ball with mass B, and the little ball is accelerated, then hits a wall and bounces back and hits the big ball again, which has now moved (though with less velocity since it gave some to the little ball) and repeats its journey, again taking some momentum from the big ball. Eventually, the big ball will come to a stop and after that, the collision with the little ball with cause the big ball to move in the opposite direction.
    -If the mass of the big ball A is related to the mass of the little ball B by the equation A = (B)(16)(100)^N where N is any integer, solving the number of collisions taken to reverse the big ball's movement in the other direction at a constant of N in the equation given, the number of collisions is equal to the first N + 1 digits of pi.
    -Ex: Assume the little ball has mass 1 kg. If the big ball has mass 16 kg, it takes 3 collisions.
                               If the big ball has mass 1,600 kg, it takes 31 collisions.
                           If the big ball has mass 160,000 kg, it takes 314 collisions.
                           If the big ball has mass 16,000,000 kg, it takes 3141 collisions.
     And so on.
-Controlled Substances Act: US drug policy. Classification:
    -Schedule V Drug: (A) Low potential for abuse compared to Schedule IV drugs, (B) There is a currently accepted medical use for it, (C) Abuse of the drug may lead to limited psychological dependence
    -Schedule IV Drug: (A) Low potential for abuse compared to Schedule III drugs, (B) There is a currently accepted medical use for it, (C) Abuse of the drug may lead to psychological dependence
    -Schedule III Drug: (A) Lower potential for abuse compared to Schedule II drugs, but probable and can be harmful, (B) There is a currently accepted medical use for it, (C) Abuse may lead to low or high psychological dependence
    -Schedule II Drug: (A) Drug has a high potential for abuse, (B) There is a currently accepted medical use for it but only under severely restricted conditions, (C) Abuse may lead to severe psychological or physical dependence
    -Schedule I Drug: (A) High potential for abuse, (B) No currently accepted medical use for it, (C) No way to safely use the drug
-Westermark effect: People remember the scent of people they grew up with and become less physically attracted to them than they would be to strangers.
-Bone Wars: (A.K.A. Great Dinosaur Rush) In the late 1800s, two paleontologists sparked a rivalry between their work: Edward Drinker Cope and Othniel Charles Marsh. They stole fossils, manufactured them, used bribery, destroyed evidence, and used other such unethical methods to defame the other. They tried to get each other's funding cut off in an attempt to get ahead. Each was wealthy and there was able to launch a flurry of fossil expeditions in a period from 1877 to 1892. The rivalry continued until both men exhausted their funds. 
    -The rivalry was enormously successful for the field of paleontology, although it socially devastated both men as unscientific, immoral, and petty. It produced the discoveries of 142 new species and was the main reason the general lay population got interested in paleontology.
    -Statistically, Marsh won the Bone Wars; he discovered 80 new species to Cope's 56, because Marsh had more manpower and funding available.
    -Cope was a general paleontologist while Marsh almost exclusively focused on fossilized reptiles and mammals.
    -Both men used dynamite to excavate earth, and probably destroyed fossils in the process. It was also environmentally damaging.
-Lazarus Syndrome: After resuscitation of a clinically dead patient is halted due to ineffectiveness, the Lazarus syndrome may occur, in which the flow of blood is spontaneously sparked (auto-resuscitation). The syndrome is extremely rare and not well understood. 
    -One theory is that the repetitive cardiopulmonary compressions lead to a built up pressure in the heart, and so when CPR is ceased, the pressure breaks forward and causes the heart to resume beating and triggering the heart's electrical impulses. 
    -Extremely high concentrations of epinephrine (adrenaline), norepinephrine, and cortisol have been found in such cases.
-Broken Window Theory: A theory in criminology that posits a cause and multiplier of crime to be the appearance of society and the message it conveys to potential criminals. Vandalism, especially graffiti, and other urban disorder is generally overlooked in high crime areas due to a focus of resources on more serious crimes, but the state of urban chaos and graffiti is a root cause of more serious crimes: it tells potential criminals that it is ok to commit crimes and that the police force is weak. It creates a positive feedback mechanism in which an out of the ordinary crime, even a small one, when left unattended, goes from out of the ordinary to ordinary
               and thus creates the new sense of "normal", or the zeitgeist, as a society of moderate crime, and the cycle continues, this time espousing more serious crime, and so on.
    -Milgram's experiment: Two identical cars with no license plates and with their hoods up are each placed in Palo Alto and in the Bronx. In the Bronx, the car is vandalized within minutes when a family takes the radiator and battery. Less than two days later, everything of value was gone from the car. Within a week, the windows were smashed and the inner upholstery destroyed, and children used it as a playground. In an area with low crime, e.g. Palo Alto, the car was untouched for the duration of the experiment.
-Green Revolution: A time during the 1940s to the 1970s of intense research and development in the field of agriculture that transformed the industry from one of sustenance to one of commercialization.
    -Norman Borlaug: Father of the Green Revolution. A famous scientist in the field of agronomy (science of producing plants for food, fuel, etc.) that used his knowledge of plant genetics and pathology to create extremely high yielding crops that were extremely resistant to disease. He is credited with saving over a billion lives with his work and won the Nobel Peace Prize.
    -Technology: Advanced irrigation, pesticides, artificial fertilizer (usually nitrogen based), genetic engineering, etc.
    -Production of cereal doubled and that of agricultural in general steadily rose. The ratio of crops produced to energy input has decreased. 
    -It is estimated that without the revolution, widespread famine would have occurred. A fraction of the four billion people born during the time would have survived without the Green Revolution. The average person's average caloric intake increased 25%.
-Two-Stage Light Gas Guns: Conventional guns can fire projectiles up to 2 km/s; gas guns use compressed hydrogen to accelerate projectiles to over 7.5 km/s
        -Stage 1: Conventional but smokeless gunpowder is ignited using an electrical spark. The rapidly expanding gas drives a piston forward at great velocity, compressing light hydrogen gas very quickly. 
        -Stage 2: The hydrogen gas is rapidly pressurized. The projectile is enclosed by a sabot (a commonly used material to bridge the gap between projectile and gas chamber); the pressurized chamber with hydrogen gas is separated from the projectile with a burst disc that keep the volume constant, disallowing the gas to escape and allowing pressure to rapidly build.
              When pressure reaches a sufficient level, the burst disc automatically fails and the gas rushes all at once through the muzzle, propelling the projectile very quickly.
-Fermi problems: A rapid form of estimation that uses only general knowledge and builds on dimensional analysis to approximate the solutions to seemingly insurmountable problems. The problem is first broken up into many sub problems that, when put together, yield the answer, and the solutions to these smaller sub problems are approximated using general knowledge and powers of 10. 
    -Theory: Without any consistent bias in estimation and with considerable general knowledge, overestimations and underestimations tend to cancel each other out, yielding an answer that is usually within a couple orders of magnitude as the real answer.
    -Example: To rapidly estimate an answer to the question "How many piano tuners are there in Chicago?" use the Fermi method. 
          Split the question into: Number(piano tuners) = Number(piano tunings/year)/Amount(piano tunings performed by 1 piano tuner). Number(piano tunings/year) = (Population(Chicago) in people)/(Number of people per household in persons/household) * (Number of pianos per household in pianos/household)(Amount of times a piano is tuned in tunings/year) = number of piano tunings in Chicago per year
                                                                           Amount(piano tunings/piano tuner) = (Amount of work weeks in a year in weeks/year)(Amount of work days in a week in days/week)(Amount of work hours/day in hours/day)/(Amount of time to tune a piano for 1 piano tuner in hours/piano tuner)
          Applying the following estimations: 1. Population of Chicago = 5 million, 2. There are, on average, 2 people to one household, 3. About 5% of households have pianos, 4. Pianos are tuned about once a year, 5. It takes a piano tuner a total of 2 hours to tune 1 piano, 6. Piano tuners work 8 hours a day for 5 days a wee for 50 weeks a year. Plugging in the estimations yeilds an answer of 125 piano tuners. The verified answer is 81, which is on the same order of magnitude.
-Tsiolkovsky Rocket Equation: Equation that, for rockets, relates the change in speed a vehicle will experience if it expels a certain amount of mass at a certain velocity. 
    -The equation: delta-v = v_(e)*ln((m_(0)/m_(1)) where delta-v = change in velocity, v_(e) = effective exhaust velocity, m_(0) = initial mass, m_(1) = final mass.
        -The delta-v represents the "energy cost" of the trip and is often given in km/s, i.e. the velocity needed to break gravitional forces.
    -Rocket: A vehicle that transports itself by expelling a part of its mass at high velocity and exploiting the conservation of momentum to more forward. 
    -Specific impulse: Describes the efficiency of a rocket or jet engine. It relates the speeds with which a particular fuel leaves the rocket, and so varies per fuel (liquid propellants are the most efficient). It is denoted by the dot product of the thrust force and the amount of propellant used over time. Higher specific impulse means lower propellant flow rate to achieve the same thrust.
        -Effective exhaust velocity: Velocity a propellant would need to be ejected to give a vehicle the same thrust. Can also be thought of as the energy available in a propellant.
            -Common propellants and their specific impulses (converted from energy to velocity in km/s): 
                Solid Rocket: 3 
                Kerosene-Oxygen: 3.1
                Hypergols: 3.2 
                    -Hypergols are used in rocket propellants and are two materials that spontaneously ignite when they contact each other. Common hypergols are dinitrogen tetroxide and hydrazine, or monomethyl hydrazine and unsymmetrical dimethylhydrazine.
                        -Hypergolic propellant: Propellants spontaneously ignite with one another; usually consists of a fuel and an oxidizer. They are typically stored as liquids, though very hard to handle. Since no ignition system is needed, propellants can be fired by simple opening and closing access routes (as this initiates and cuts off contact between the propellants).
                            -Hard start: A condition of overpressure in a rocket engine during the start of ignition; the wost case scenario involves the rocket exploding.
                Methane-Oxygen: 4.5
                Hydrogen-Oxygen: Most energetic chemical reaction for use in a human rocket known.              
        -Actual exhaust velocity: Average speed the propellant experimentally leaves with. When the only forces considered are those of the above mentioned astrodynamics, effective exhaust velocity equals actual exhaust velocity, but many engines use other mechanisms to accelerate even more, e.g. jet engines that compress and expel air for additional thrust.
    -Most rockets are well above 80% propellant.
-Wrongful Dismissal: Situation in which an employee's contract of employment has been terminated in such a way to breach the contract itself or breach a statute of some sort. Common cases:
    -Discrimination: Termination on the grounds of certain irrelevant differences applied to people such as religion, sex, nationality, etc.
    -Retaliation: Termination occurs with the purpose of being vengeful or punitive.
    -Refusal to commit crime: Termination because the employee was disobedient when asked to commit an act that was at the time illegal.
    -Wrong procedures: Proper termination procedures as delineated by the employee handbook or company policy have not been followed.
-Small world experiment: Individuals were randomly chosen and were mailed a set of instructions that told them to send the mailed packet to someone that they estimated would have the highest probability of being able to continue the procedure until it reached a target in some other known city, such as New York.
             The average number of individuals connecting any two randomly selected people, suspected to be over 100, was 6. This is known as the 6 degrees of separation.
    -Conducted by Stanley Milgram.
-Milgram experiment: An experiment designed to see how much a person is willing to defy their own conscience and morality if instructed to do so by an authority figure; prompted by the Nuremberg defense.
    -The experiment: Subjects were told they were participating in an experiment on memory. A teacher administered the test, an experimenter supervised it (the authority figure), and a learner (actually part of the experiment; an actor) learned. The selection of teacher or learner appeared to be random but was always guaranteed so that the subject would be the teacher.
             The subject and learner are separated visually, but can hear each other. Each time the learner got a question wrong, the subject was told to press a button that would administer a painful electric shock to the learner, with the shocks climbing in intensity as more questions were "missed". The learner was told to scream out in pain and complain of a heart condition, arousing
             the teacher's empathy and testing if he would halt the experiment. If the subject did ask to stop, the experimenter would first say "Please continue.", and on the second request to stop, "The experiment requires that you continue.", on the third "It is absolutely essential that you continue.", and on the fourth "You have no other choice, you must go on." Subjects were allowed
             to leave on their fifth request. 
        -Maximum shock administered: 450 volts. After the maximum shock was administered 3 times and the subject still hadn't asked more than 4 times to leave the experiment, it was concluded.
        -Polls showed that it was predicted that 3% of subjects, at most, would make it to the maximum shock, and the majority would stop after the first request from the learner to do so.
        -Results: 65% went on to the final shock and concluded the experiment without stopping. 
            -All subjects questioned the experiment at some point but still continued. 
            -Subjects were sweating, trembling, stuttering, biting their lips, groaning, digging their fingernails into their skin, and some were even having nervous laughing fits or seizures.
-Stanford Prison Experiment: An experiment to measure the effect that being placed in a position of power of position of submission has on people. Out of 24 randomly selected males deemed to be of high psychological stability and with high moral values, 12 are each assigned randomly to be either guards to prisoners in a mock prison.
    -The guards quickly turned the prison from a corrective facility to a strict authoritarian regime, brutalizing the prisoners and psychologically torturing them. Prisoners passively accepted the abuse and many helped the guards abuse the prisoners if asked to do so.
    -On the second day, prisoners blockaded their cells with their beds. To suppress the revolt, guards attacked them with fire extinguishers. The guards used psychological methods to control the prisoners, such as rewarding good behavior and punishing bad behavior.
    -Torture: Guards forced prisoners to urinate and defecate in a bucket and restricted access to restrooms, didn't allow them to empty the buckets, permitted them to live in unsanitary conditions, confiscated their mattresses and forced them to sleep on concrete, stripped them naked, and put them in solitary confinement. The guards were diagnosed as genuinely sadistic, and most were upset when the experiment finished.
        -The conditions got so bad that the experiment, scheduled for 2 weeks, ended after 6 days. 
    -Learned helplessness: A condition in animals (including humans) where the subject learns to behave as if it is helpless, even when it isn't, following the perception that a subject has no control over an unpleasant situation.
        -If an animal is constantly caused pain, and it can do nothing to escape it, then after a while it stops resisting the pain and simply accepts it. Even when an escape route is offered, it isn't taken.
        -Diagnosed as a mental illness.
-Mach number: The ratio of the velocity of a vehicle moving through some fluid to the speed of sound in that fluid; M = v/v_(sound)
    -Critical Mach number: The lowest Mach number for which fluid around the vehicle can reach the speed of sound.
    -Transonic: A condition of a vehicle capable of flight in which the fluid around it, usually air, is moving at variable velocities, all of which are either below, at or above the speed of sound, usually give or take 0.2 Mach.
-Equation of a normal distribution: (A.K.A. normal distribution) f(x) = (1/(sigma*sqrt(2pi)))(e^(-(x-mu)^2/(2*(sigma^2)))); mu = mean, sigma = standard deviation
    -Intuition: The motivation for the function's form is presented in three steps: First, the motivation behind the general form of the function as e^(-u^2), where u is a function of x. Second, we explain the actual function in the exponent, i.e. (x - mu)^2 / (2*sigma^2). Third, we explain the coefficient 1/(sigma*sqrt(2pi)).
        1. Graphically, probability can be expressed on the Cartesian plane with specific outcomes being points and the entire plane being the sample space. Then, to find the probability of a range of values, we will need to total collection of points in a region, i.e. the area. The simplest region is a rectangle with width delta-x and height delta-y. The probability of obtaining a point on the thin horizontal strip given by delta-x is f(x)*(delta-x), and the probability of obtaining a point on the thin vertical strip given by delta-y is f(y)*(delta-y), where f is the probability distribution function (to be determined). These probabilities are a result of the definition of probability and the assumption that the probability of hitting a region is proportional to the size of the region. The probability of the rectangle is thus the probabilities times the area, or f(x)*f(y)*(delta-x)*(delta-y). Consider the same representation of the point in polar coordinates, radius r and angle theta. Then the probability of the rectangle is some function of r, g(r),
            which represents the probability distribution function, times the area: g(r)*(delta-x)*(delta-y); note that there isn't a function of theta involved because an assumption in probabilitity is that errors are isotropic, and therefore direction is irrelevant. Therefore, g(r)*(delta-x)*(delta-y) = f(x)*f(y)*(delta-x)*(delta-y) and therefore g(r) = f(x)*f(y). Now differentiate with respect to theta to obtain a differential equation whose solution is the probability distribution function we've been looking for: dg/d(theta) = 0 = f(x)*(df(y)/d(theta)) + f(y)*(df(x)/d(theta). Note that df(y)/d(theta) = df/dy * dy/d(theta), and dy/d(theta) = r*cos(theta) since y = r*sin(theta) by definition. Applying the chain rule to both terms, we have 0 = f(x)*f '(y)*x - f(y)*f '(x)*y, which implies that f '(x) / (f(x)*x) = f '(y) / (f(y)*y); this is only true if both terms are constant. Set the x term equal to a constant C (it doesn't matter which term we choose since the variables are dummy variables anyways). Then f '(x) / f(x) = C*x. This is the differential equation alluded to earlier. Integrating both sides,
            ln(f(x)) = (C/2)*x^2 + D, or f(x) = A*exp(C/2 * x^2), where A = e^D. This proves the general format of the Gaussian function.
        2. To find what the coefficient A is, use the definition that the sum of all probabilities in f(x) must add to 1, and therefore the sum from negative infinity to infinity of A*exp( C/2*x^2 ) = 1. Since the function is symmetric, simplify by only taking the right half of the function and dividing by A: integral from 0 to infinity of exp(C/2*x^2) = 1/(2A). To integrate, we must use polar coordinates. First, note that 1/(2A) = sqrt( (integral from 0 to infinity of exp(C/2*x^2))^2 ) = sqrt( (integral from 0 to infinity of exp(C/2*x^2))*(integral from 0 to infinity of exp(C/2*y^2)) ), since y is just a dummy variable. This is equal to sqrt( integral from 0 to infinity of exp(C/2*(x^2 + y^2)) ). Applying a polar transformation and evaluating the integral, we find that 1/(2A) = sqrt(-pi/(2C)), and therefore A = sqrt(- c/(2pi) ).
        3. To figure out what C is, completing the picture, note the definition of variance: sigma^2 = integral from negative infinity to positive infinity of x^2*f(x) dx, assuming the mean is 0. Substituting what we already know about f(x) and again focusing only on the right half of the function, this yields sigma^2 = 2*sqrt(-c/(2pi))*(integral from 0 to infinity of x^2*exp(C/2*x^2)). Integrating by parts (let u = x and dv = x*e^(C/2*x^2)) yields sigma^2 = (-1/C)( (sqrt(2pi))/(2*sqrt(-C)) ), which simplifies to C = -1/(sigma^2) when multiplied by the constant term on the outside of the integral. Before we substitute this into our formula, note that we assumed the mean (mu) was 0; if this is not the case, simply plug in (x - mu) instead of x to shift the distribution over. This completes the equation of the distribution.
    -Central limit theorem: A random variable follows a probability distribution; this distribution can be any possible distribution and is NOT necessarily Gaussian. If a number of independent random variables (each with a well-defined though not necessarily equal mean and variance) is taken, their composite means and variances can be computed. If the same number of random variables is again run, new composite means and variances can be computed; these will probably differ slightly from the first mean and variance. The distribution of all possible means of a number of independent random variables' means will follow an approximate Gaussian distribution, and as the number of random variables increases, the approximation becomes closer and closer to Gaussian. In other words, the distribution of means of random samples approaches a Normal distribution as the sample size increases.
        -Intuition: The law of large numbers (covered far below) states that the mean value of a number of independent random variables approaches the random variable's mean as the number of iterations of that variable increase. All of the different values for means for a number of independent random variables must create some kind of distribution. According to the Principle of Maximum Entropy, we should choose the distribution with maximum entropy that has the same mean and variance as the random variable, and this is the Gaussian distribution.
            -The distribution of all the distributions of the variables, even if those variables are non-Normally distributed, will be Normally distributed, assuming each variable has the same mean and variance.
            -Principle of maximum entropy: The distribution with the most entropy should be chosen to best model a phenomenon.
    -Log-normal distribution: A distribution used to describe variables that cannot be negative but have unlimited positive potential (eg stock prices, oil reserve's, etc.). The distribution is very right skewed with its peak being close to the vertical axis. The logarithm of a random variable with a log-normal distribution, by definition, is normally distributed, ie if X is log-normally distributed, then log(x) is normally distributed.
-Shannon entropy: Entropy is a measure of the uncertainty in a situation. It can be calculated as follows: H(X) = sum over all possible outcomes in X of (p(x_i) * I(x_i)), where p(x_i) refers to the probability of the ith outcome of X and I(x_i) refers to the amount of information contained by that outcome. 
    -I(x_i) = log_2 (1/(p(x_i)). The reason a logarithm is used follows from a set of conditions that only the logarithmic function meets: 
        1. I(p1*p2*...*pn) = I(p1) + I(p2) + ... + I(pn) because the product of p1 through pn is nothing more than the outcome that all events 1 through n occur, and the total information from that event ought to equal the sum of informations from each individual event.
        2. I(1) = 0; this is because for a certain event, no new information is gained; an event's probability is inversely proportional to the amount of information we gain if that event occurs, since we are more and more "surprised" if an unlikely event occurs.
        3. I must be continuous.
     The reason a logarithm with base 2 is used stems from the outcome's units: binary. Since information is referred to with units of bits, i.e. binary digits, which can only take on two possible values, the logarithm is base 2. The argument of the logarithm is 1/(p(x_i)) because the amount of information gained is inversely related to the probability.
-Survival analysis: Discusses the frequency and prevalence of failure (i.e. "death") in some population; can deal with biological organisms and death or mechanical systems and failure. What constitutes "lifetime", "death", and "failure" must be defined for a given system.    
    -Survival function: If S(t) is defined as the survival function, S(t) = P(x > t) where x is the random variable in question and t is time; S(t) = probability that x survives t units of time.
        -The axiom that S(0) = 1 implies that instantaneous death is not recognized. 
        -S(t) is non-increasing; S(t+1) < S(t), and the limit as t approaches infinity of S(t) = 0 (unless immortality is defined for the system)
        -A plot of S(t) is a density curve, and so by definition the integral from -infinity to infinity = 1
    -Lifetime Distribution Function: The complement of the survival function; outputs the likelihood that x lives to a certain age of t units of time.
        -F(t) = 1 - S(t) = P(x </= t)
        -f(t) = F'(t) = rate of death per unit time
            -Therefore, S(t) = integral from t to infinity of f(u) = 1 - F(t)
-Ricin: Extremely toxic substance; lethal dose of 1.78 mg (equivalent to a few grains of salt in mass) for a human, with LD_(50) = 22 microg/kg, if entered directly into bloodstream (e.g. by injection). Consumption by ingestion or inhalation has a much higher lethal dose of LD_(50) = 20 millig/kg - 30 millig/kg. Death takes a few days and appears to be of natural causes.
    -Median lethal dos: (A.K.A. LD_(50), or lethal dose 50%) The amount of a substance required to kill 50% of a population. Expressed as unit mass of substance per unit mass of host (e.g. 22 micrograms of ricin per 1 kilogram of unit human mass = 22 microg/kg)
    -Gy: (Grey) SI unit for amount of a dosage that is absorbed, usually of (ionizing) radiation. 1 Gy = 1 Joule of energy absorbed per 1 kilogram of matter.
        -1 rad = 0.01 Gy
    -Derived from castor beans.
-Thermoplastic: (A.K.A. thermosoftening plastic) Any polymer that, upon heating, increases in pliability and malleability, but solidifies with cooling. Most are characterized by long chains held together by Van der Waals forces; the specific temperature of spontaneous pliability is the temperature at which these intermolecular forces are broken; below the temperature, they reform.
    -Glass transition: A reversible transition of an amorphous solid (a solid that doesn't have ordered crystalline structure over long ranges of its molecular constituents), where it goes from hard and brittle to soft and rubbery. Solids that undergo this transition are called glasses. The glass transition temperature is always lower than the melting point for any glass. 
-Tardigrade: (A.K.A. water bear, moss piglets) One of the most complex extremophiles. 1 mm long when fully grown. Can survive temperature ranging from just above absolute zero to hundreds of degrees Celsius. They can survive pressures many times greater than those at the bottom of Earth's oceans. They can survive the vacuum of space and can go without food or water for 120 years by dehydrating, releasing 97% of their body's water, and waiting conditions out by entering a state of cryptobiosis. Can survive without oxygen with this method as well. Can survive doses of radiation thousands of times the lethal dose for humans.
-Mantis shrimp: An organism that can see in the infrared and the x-ray spectra; they have 16 color cones, compared to humans' 3 and dogs' 2, and so can see a much wider spectrum of color. 
    -Live in shallow aquatic and warm regions. Between 6-12 inches.
    -Uses two raptoral appendages, similar to bone claws, that it can accelerate to the speed of a bullet and can thus crack rocks and any prey.
        -They can also be used for cavitation, and can be used to create shock waves and temperatures on the order of 15,000 K. This also emits tiny bursts of light (sonoluminescence: light produced when an underwater bubble is collapsed by a soundwave).
        -The claws are so strong and resilient that they can be used for body armor.
-Laser: Emits high energy light by amplifying electromagnetic radiation; LASER = light amplification by the stimulated emission of radiation. All light emitted is coherent (in phase with each other; no interference, constructive, destructive, or otherwise).
    -A laser that produces light itself is not an optical amplifier; it is an optical oscillator.
    -Construction: Consists of 3 main components: (1) Energy source, (2) Gain medium, (3) Optical resonator
        -Energy source: Type of source depends on the gain medium; the energy is used to emit the light. Common energy sources are electrical discharges, flashlights, another laser, chemical reactions, explosives.
        -Gain medium: The source of the optical gain in the laser that allows the laser output to exist; draws power from the stimulated emission of electromagnetic radiation, which is usually induced through electronic or molecular transitions from higher to lower energy states. Determines the wavelength of the light emitted.
            -Optical gain: The ability of a circuit to increase the power to an outputting device by pumping energy into the circuit; the energy must be first converted from a power supply. Can be defined as the average ratio of signal output to input.
                -In terms of power, gain = (10)(log(A/B)) where A = output power, B = input power. Measured in dB.
                -In terms of voltage, gain = (10)(log(A/B)^2) where A = output voltage, B = input volatage. Measured in dB. 
            -Common gain media: Helium-neon, argon, krypton, xenon ion (uses ionizing gas as the medium), nitrogen, carbon dioxide, excimer (uses UV rays and a combination of halogens and inert gases)
        -Optical resonator: 2+ mirrors are used to create an optical cavity that forms standing waves (waves that remain in a constant position). One mirror is usually highly reflective while the other is only partially reflective; this system allows for continuous output of light. The light produced from the energy source is reflected into the gain medium hundreds of times before it exits the optical cavity.
            -Superluminescent lasers: Lasers  that do not use optical resonators; instead, they utilize extremely high optical gain to produce a continuous stream of light.
    -Chemical laser: Laser that uses a chemical reaction as a power source. 
    -HERCULES laser: Laser constructed at the University of Michigan; so power that it is the most powerful known source of light in the known universe. It has a greater power output optical gain than a laser that concentrated all of the sun's rays onto a single grain of sand would. Emission is sustained for 30 femtoseconds (E(-15) s), with a pulse every 10 seconds.
        -Has possible applications in "vacuum boiling" (the concentrating of energy in a vacuum to spontaneously create matter).
        -Power output of 300 TW (terawatts)
-Ionizing radiation: Radiation with sufficient energy to free an electron from an atom.
-Radar: RADAR = radio detection and ranging. Emits radio waves and records orientation of the reflected radiation, as well as time passed, intensity, etc. to estimate the position and velocity of an object. Radar signals are reflected best by good electrical conductors; otherwise, they are usually scattered. 
-Magnetism: The forces that arise out of moving electric currents and fundamental magnetic moments of elementary particles, mostly the electron. These fundamental magnetic moments are caused by particles' nonzero spin (the measure of angular momentum and algebraic orientation that creates a pseudo moving charge). When these spins are aligned, a magnet is formed. The spin can only point up or down, and so there are only two orientations for a magnetic field; in a normal material, the magnetic field orientations cancel out if the electron spins are not aligned. Because of the Pauli exclusion principle, filled electron shells have equal numbers
        of spin up and spin down electrons, so only atoms with partially filled valence electron shells can have a net magnetic moment.
    -Electromagnetism: An ordinary metal wire is electrically neutral, because there are equal amounts of positive and negative charge as well as equal charge densities. Even if there is current in the wire, the charge densities and amount of charge do not change, so even an externally charged object, (e.g. a proton) would feel no force. If this object were moving, then it does feel a force. This is explained in 2 cases:
        1. If the object is moving in the same direction as the electrons, then in its frame of reference, the protons are moving faster than the electrons are. A principle of special relativity is length contraction, i.e. faster objects contract in length. So, from the object's frame of reference, the protons contract in length, while the electrons expand relative to their resting position. The positive charge density now outweighs the negative, and so the object feels a force. Magnetism in this way is simply the electric field viewed from separate reference frames.
        2. If the object is moving in the opposite direction as the electrons, then in its frame of reference, the protons are moving much slower than the electrons, and so the principle of length contraction, by the same logic as above, causes an increased negative charge density in the object's frame of reference, so it feels a force.
    -Curie temperature: The critical point of a magnetized substance where its permanent magnetism is converted to induced magnetism, and so the magnetic moment reverses direction. This is why temperature is inversely proportional to the strength of an induced magnetic field; spontaneous magnetism is only possible below the Curie temperature.
        -Magnetic susceptibility: The ratio of the magnetization of a material to a magnetic field's strength (usually measured in amps/m). Gives an idea for the degree to which a substance is magnetized due to a magnetic field.
            -Curie-Weiss law: X = C/(T-T_(C)) where X = magnetic susceptibility, C = curie constant of the material in question, T = absolute temperature, T_(C) = Curie temperature for that material.
        -Curie's law: The magnetization of a material is about directly proportional to that of an applied magnetic field. As temperature rises, the proportionality constant decreases.
            -M = C (dot) B/T where M = (resulting) magnetization, C = Curie constant for that material, B = magnetic field, T = absolute temperature
    -Ferromagnetism: Strongest type of magnetism; the only type that can cause physically significant changes. It can permanently magnetize materials (though ferrimagnets can as well). 
        -Common ferromagnetic substances are iron, nickel, cobalt, lodestone, and some rare earth metals.   
        -Heusler alloy: A ferromagnetic alloy of two or more materials such that none of the materials are ferromagnets alone.They are based on the Heusler phase (a particular configuration of the alloy's lattice structure involving a cubic crystal system (a crystal system in the shape of a cube; very simple and the most common)).
    -Ferrimagnetism: A material with opposing magnetic moments that still has a net magnetic moment because the opposing moments are not equal.  
        -Most common when two or more ionic substances are fused.
    -Spin glass: A disordered magnet with a random (stochastic) distribution of electron spins 
    -Antiferromagnetism: Materials with electrons that alternate in spin direction. 
    -Paramagnetism: Materials that are not magnetized yet are attracted to an external magnetic field.
        -Diamagnetism: Opposite of paramagnetism; the material is repelled by an external magnetic field.
    -Dipole: An object with a dipole moment (a vector). Types of dipoles: (1) Electric dipoles: The separation of positive and negative charges; electret = a permanent electric dipole. (2) Magnetic dipole: A closed circulation of an electric current that creates a magnetic field. (3) Flow dipole: In fluid mechanics and flux, the separation of a sink and a source. (4) Acoustic dipole
        -Physical dipole: Two equal and opposite charges; two poles. The dipole moment is the vector representing the force field lines from the negative to the positive charge.
        -Molecular dipole: Molecules with dipole moments, usually due to a characteristic distribution of electrons.
            -Permanent dipoles: Different atoms in a molecule have substantial discrepancies in electronegativity, and so one atom attracts the electrons more than the other. The molecule has a permanent and fixed dipole moment, and is therefore polar.
            -Instantaneous dipoles: If electrons and randomly distributed, by chance, in such a way for there to be a non-uniform distribution of electrons with a significantly higher concentration in one area than the rest, there is a temporary dipole moment.
            -Induced dipole: A permanent dipole affects the electron distribution of another molecule (i.e. positive dipoles attract the electrons towards one concentration, negative dipoles repel them away), polarizing the molecule and inducing a dipole moment.
        -Field strength of a dipole moment: B = ((mu_(0)*m)/(4pi*r&3))(sqrt(1 + (sin (lambda))^3)) where r = distance from center, lambda = magnetic latitude, m = dipole moment, mu_(0) = vacuum permeability
         For an electric dipole, psi = (1/(4pi*epsilon_(0))((p (dot) r)/r^2) where epsilon_(0) = vacuum permittivity, r = unit vector in the direction of r', p = dipole moment
        -Units: Dipole moments are measured in "debye"s, in honor of Peter J.W. Debye, who first extensively studied dipoles. It is the dipole moment resulting from two charges with an equal magnitude of 10^(-10) e.s.u. (electrostatic units)
-Hexadecimal: A (positionally based) system of numbering with a radix, or base unit, of 16. 
    -Useful in computer science, programming, and mathematics as a shorthand for writing out and inputting binary values.
    -Just as the decimal system works by multiplying the units' digit by 10^0 or 1, the tens' digit by 10^1 or 10, the hundreds' by 10^2 or 100, and so on, the hexadecimal system works with a base of 16 instead of 10.
        -The digits 0-9 represent themselves (the first 9 digits) and the letters A-F represent 10, 11, 12, 13, 14, and 15.
        -The number 10 = 16 in the decimal system, because in hexadecimal 10 = (0)(16^0) + (1)(16^1) = 16
         C9A1 = (1)(16^0) + (10)(16^1) + (9)(16^2) + (12)(16^3) = 51,617 in decimal 
-Dynamic programming: A problem solving tactic that involves breaking a problem down into smaller, more manageable sub-problems. Closely related to Fermi problems.
-Definitions of money supply:
    -M0: The total amount of currency in the world; it is approximately valued at an aggregate of just over 5 trillion USD.
    -M1: Money created due to fractional-reserve banking; approximately 25 trillion USD.
        -Fractional reserve banking: The most common form of banking worldwide. In this method, a bank only retains a fraction of its customers' deposits as on-hand reserve's; the rest is used to lend to other customers (at interest rates to make a profit) or to invest. Thus, the amount of money in existence slowly grows. To prevent bank runs (when a huge proportion of the customers try to withdraw their deposits at once and because of fractional banking their money isn't there), most governments regulate and watch over these banks.
            -Creation of money: Money multiplier (the maximum amount of money a deposit can be expanded to become) formula: m = 1/R where R = the reserve requirement (the ratio of a deposit that a bank must (by law) or does keep at the ready)
    -M2: The total value of non-liquid assets and money that cannot yet be spent (e.g. pensions, savings accounts, etc); approximately valued at 60 trillion USD
    -M3: The most broad definition of money, it includes everything up to large time deposits, institutional money and market funds, agreements and contracts, stocks, etc.; approximately valued at 75 trillion USD. This measure is no longer used by the Federal Reserve because of how abstract it is.
-Tinkerbell effect: The phenomenon in which something (immaterial or material, though it is almost always immaterial, e.g. an idea) exists only because people believe in it or believe it exists.  
    -Named after Tinkerbell from the children's story of Peter Pan, who is revived because children believe in fairies.
    -Examples: Fiat currency, laws and civil society, private property
    -Reverse Tinkerbell effect: When something becomes less prevalent as more people believe in it
        -Examples: The idea that driving is safe, the idea that one's vote counts, etc.
-Clostridium botulinum: Bacteria that produce what is indisputably the more poisonous substance, natural or otherwise, on earth. They also cause botulism. The LD_(50) for the botulinum toxin is 1 nanogram; an amount equivalent in volume to that of a grain of sand can kill 9600 people.
    -Thrives in low oxygen environments. The toxin is only produced by active cells. The bacteria release a deadly neurotoxin as a by-product of their metabolism, and this neurotoxin inhibits the production of acetylcholine, which is required for muscle contraction; the victim dies of asphyxiation as the lungs, diaphragm, and heart fail. 
-Van der Waals force: The total sum of all forces between molecules, factoring in attractive and repulsive forces for the net force. These forces do not encompass those due to covalent bonds, hydrogen bonds, or ionic/electrostatic interactions. 
    -Keesom force: Force between two permanent dipoles
    -Debye force: Force between a permanent dipole and the induced dipole it has just induced
    -London dispersion force: Force between two instantaneously induced dipoles.
        -Instantaneously induced dipole: When electrons in an atom, by chance, end up unevenly distributed around an atom and clump to one side, creating a very small dipole. This resulting (tiny) electrostatic force increases the chances of other nearby instantaneously induced dipoles forming, thus starting a chain reaction.
-Technological readiness level: A scale from 1 to 9 used to rate the maturity of a newly created technology, from basic conceptualization and abstraction to real world system implication.
    -The scale: TLR 1: The idea is created and its basic principles are recorded. Research begins to become experimentation.
            TLR 2: Speculative practical applications are invented, though they may not have much proof behind them.
            TLR 3: Actual research and development of the technology begins. Validation of applications of the technology begins.
            TLR 4: The basic technology now exists but they cannot yet work together; integration of all technological components has not reached the experimentation phase.
            TLR 5: Basic technological components can be sufficiently integrated such that they can work together well enough to be tested in a simulation.
            TLR 6: Bugs are removed and a prototype is created. It is tested in either a high-fidelity lab environment or a simulated operational environment.
            TLR 7: The prototype can perform functionally in an operational environment.
            TLR 8: The technology can work at maximum efficiency in all operational conditions with a minimal fail rate; all bugs are removed.
            TLR 9: Technology is applied to the real world.
-Retrograde analysis: A problem solving method that solves positions by working backwards from a known solution. 
    -Known as backward induction in game theory: Reasoning backwards in time to find the best action.
-Redout: The exit of one's blood through the eyes, nose, mouth, and ears due to a sufficient negative g-force. It is caused by the difference in inertia between fluids and solids; the skeleton and heavy organs are accelerated downwards but the blood tends to stay where it is "more" than these solids do, resulting in all the blood rushing to the brain a subsequent redout.
    -Can cause severe retinal damage or lead to a stroke.
    -Positive g-forces result in blackouts when there isn't enough blood in the brain.
        -Greyout: A precursor to the blackout caused by a sharp decrease in blood pressure; it results in temporary dimming of vision and a temporary loss of peripheral vision. 
-Regelation: When pressure, not heat, causes a material to melt; the material re-solidifies when the pressure is removed. Works best with substances with high thermal conductivity because the pressure transfers latent heat of fusion best.
    -Discovered by Faraday
    -Can be demonstrated when a thin wire (usually copper) slices through a block of ice without cutting it and with the ice remaining intact and solid.
-Heat of fusion: The change in the total internal energy of a thermodynamic system (i.e. enthalpy) required to cause a substance to undergo a phase change from solid to liquid.
 Heat of vaporization: The enthalpy change required to change a substance from liquid to gaseous.
-Stefan-Boltzmann law: Describes the power radiated by a true black body (doesn't exist; merely an ideal approximation) with respect to its temperature.
    - J = (sigma)(T)^4 where J = Total energy radiated per unit surface area per unit time (across all wavelengths in watts/m^2), sigma = Stefan-Boltzmann constant = (2(pi^5)(k_(b))^4)/(15(h^3)(c^2)) (where k_(b) = Boltzmann constant, h = Planck's constant, c = speed of light) = 5.670373 E(-8), T = absolute temperature
    -Total power radiated by a black body = (J)(A) where J = total energy radiated per surface area per time, A = surface area
-Prandtl-Meyer expansion fan: The pattern of shockwave expansion in supersonic media; it occurs when the flow turns around a convex corner. Because the second law of thermodynamics prevents the entire shock wave from turning the corner all at once, as one wave, an expansion fan consisting of an infinite number of Mach waves (a wave of pressure traveling at the speed of sound), gradually turning the flow. Because of the change in direction, the flow and its Mach number accelerate and increase, respectively, while (static) pressure, density, and temperature decrease. 
-Van Allen radiation belts: Three layers of energized plasma that surround the earth, held together in their form by the earth's magnetic field, and so are a part of the inner region of the earth's magnetosphere. They exist from 1000 to 60,000 km above the crust; the radiation levels are not uniform. They consist mainly of electrons, but also some alpha particles; these particles can damage satellites. The third belt barely exists following a powerful shockwave from the sun that nearly wiped it out.
-Semantic satiation: The phenomenon in which repetition of a word in a short period of time causes it to lose its meaning as a word and degrade to a mere sequence of sound, psychologically.
-SI Metric Prefixes: 20 prefixes affixed to SI units to denote orders of magnitude.
    -Greater than base unit prefixes:
     Yotta: (Y) 10^24 = 1 septillion
     Zetta: (Z) 10^21 = 1 sextillion
     Exa: (E) 10^18 = 1 quintillion
     Peta: (P) 10^15 = 1 quadrillion
     Tera: (T) 10^12 = 1 trillion
     Giga: (G) 10^9 = 1 billion
     Mega: (M) 10^6 = 1 million
     Kilo: (k) 10^3 = 1 thousand
     Hecto: (h) 10^2 = 1 hundred
     Deca: (da) 10^1 = 10
    -Less than base unit Prefixes:
     Deci: (d) 10^(-1) = 1 tenth
     Centi: (c) = 10^(-2) = 1 hundredth
     Milli: (m) 10^(-3) = 1 thousandth
     Micro: (mu (Greek symbol)) 10^(-6) = 1 millionth
     Nano: (n) 10^(-9) = 1 billionth
     Pico: (p) 10^(-12) = 1 trillionth
     Femto: (f) 10^(-15) = 1 quadrillionth
     Atto: (a) 10^(-18) = 1 quintillionth
     Zepto: (z) 10^(-21) = 1 sextillionth
     Yocto: (y) 10^(-24) = 1 septillionth
-Root mean square: (A.K.A. quadratic mean) A type of mean useful when observations in the set are both positive and negative, but the quantity to be expressed is raw magnitude.
    -For a set {x1, x2, ... , xn}: x_(rms) = sqrt((1/n)((x1)^2 + (x2)^2 + (x3)^2 + ... + (xn)^2))
-Anti-bubble: A sphere of liquid surrounded by a gas (the opposite of a bubble: a gas trapped by a liquid film). When a liquid flows into another liquid, turbulently, anti-bubbles can form. Anti-bubbles that merely skim across the surface of water but are not submerged are water globules.
    -Held together by surface tension; they are elastic and can bounce of solids
    -Very short lifetime of less than a few seconds. To make it last longer, the voltage drop (electric potential) between the fluid contained within the anti-bubble and the surrounding fluid is equalized.
-Ferrofluid: A magnetized liquid. This property comes from the makeup of a ferrofluid; it is a solution with colloidally suspended magnetic, usually ferromagnetic, particles. The particles must be coated with a surfactant so that they don't clump and form a dense precipitate.
    -SUrfactant: A substance that lowers the surface tension of a liquid by absorbing the interface (the boundary between two phases) between the gas and liquid, or between two liquids. They are amphiphilic (hydrophobic on the inside and hydrophilic on the outside)
-ALNICO V: The most common form of commercially sold permanent magnets containing a mix of ALuminium, NIckel, and CObalt.
    -Neodymium magnets: Most widely used rare earth magnet. It is an alloy of neodymium, iron, and boron, and forms a crystalline structure of Nd_(2)Fe_(14)B. They are the strongest permanent magnets to be constructed. They have extremely high uniaxial magneto-crystalline (magnetic) anisotropy (the phenomenon in which a substance's constituents spontaneously align themselves along an easy axis (an energetically favorable position that allows for spontaneous magnetization)).
        -Causes of magnetic isotropy: Crystalline structure itself causes electron spins to align (due to the principle of least action, it creates a path of least resistance in that structure), imperfect spheres (which create disparities in the demagnetizing field), magneto-elastic anisotropy (tension within a linear object creates a magnetic moment by forming a temporary yet sustainable easy axis), or exchange anisotropy (the interaction of anti-ferromagnetic and ferromagnetic substances).
-Snake Island: An island along the coast of Malaysia that is home to hundreds of species of snakes. Many species of water snakes come ashore to breed.
-Sinc function: In engineering, sinc(x) = ( sin(x) )/x or ( sin(pi*x) )/(pi * x)
-Non invasive education: Developed by Indian physicist Sugata Mitra, who believes that even children in extreme poverty can become computer literate extremely quickly if they are only allowed free access to information.
    -Hole in the wall experiment: A computer, with sustainable power and fast Internet connection, was placed in the middle of a slum in India; anyone in the village could use it. Very little English was spoken and no one had any knowledge of computers, but kids agd 6 - 12 began using the computer and learned about it very quickly. Within a few months, they could operate it well. These elementary and middle school kids were able to work together, with the use of this computer, to solve college level calculus and physics problems.
-Night vision technology: Technology that aids the eye in vision in low light environments. 
    -They work in 3 main ways:
        1. Image intensification: Magnify the amount of light already present (e.g. from stars, the moon, etc.)
        2. Active illumination: A source of light, very low intensity, is first released. This technology is then combined with image intensification.
        3. Thermal imaging: Using temperature differences and infrared radiation given off to see.
    -Biological night vision: Compared to many animals, human night vision is quite poor. Many biologists speculate that this is due to the human eye's lack of tapetum lucidum (a tissue layer in the eyeball situated directly behind the retina whose purpose is to reflect incoming light back through the retina a second time, after it has already entered the retina. At the cost of blurring images, the tissue increases the amount of light available for photoreceptors and thus increases night vision).
-Geneva conventions: 4 treaties and 3 protocols signed and put forth by UN countries following World War 2. They lay the foundation for international law, specifically with respect to the rules of war.
    -First Geneva convention: Humane treatment of wounded and ill soldiers that are out of battle; they are free from being killed, injured, tortured, or experimented upon. Medical units must be respected, including in particular the International Red Cross. Though wounded and sick soldiers can become prisoners of war, they must be cared for humanely. The identity of the wounded that have been captured should be revealed to the opposing sides. 
    -Second Geneva convention: All parties must care for the wounded, sick, or shipwrecked. Neutral vessels cannot be captured, and appeals can be made for neutral vessels to help collect or care for the wounded. Religious and medical personnel have protection, even on enemy ships. Hospital ships specifically designed for medical treatment cannot be used otherwise in any way for the military. A hospital ship's medical staff cannot be captured, though the wounded, sick, and shipwrecked can be held as POWs.
    -Third Geneva convention: Clarifies what is defined as a prisoner of war (POW). POWs are the responsibility of the state, not those who capture them. They must be treated humanely and without discrimination; medical needs must be met. POWs cannot be subjected to physical or mental torture. Specific restrictions on treatment of POWs and what can be done to them or what they can be made to do, taking into account sex, race, etc. are made clear.
    -Fourth Geneva convention: Civilians are protected.
-Hague Conventions: Two international treaties, one in 1899 and the other in 1907, that delineate war crimes and other related rules of war.
    -Hague convention of 1899: Consists of 3 sections and 3 declarations. It was led by Nicholas II, the Russian Tsar.
        -Section I: The creation of a Permanent Court of Arbitration (an international body that resolves disputes amongst states)
        -Section II: This section is about the rules of war on land. Treatment of POWs is specified; the wounded must be given medical attention, and poison may not be used. It forbids the killing of people that have already surrendered. Captured troops may not be forced to fight against their own country. It also forbids collective punishment (punishment of a group of people for the actions of one individual or a subgroup).
        -Section III: This section is about rules of marine warfare. Marked hospital ships have protection. Hospital ships must treat the wounded from all belligerents, not just one side.
        -Declaration I: In the first 5 years of a war, projectiles or explosives cannot be launched from balloons.
        -Declaration II: Projectiles with the sole purpose of spreading gases that suffocate or otherwise injure. 
        -Declaration III: Expanding bullets (bullets that expand on impact, usually because of a hole in the penetrating end through which flesh is propelled on impact, slowing it down and expanding it. They increase in diameter, limit penetration, and cause a larger impact wound) and flattening bullets cannot be used.
    -Hague convention of 1907: Few decisions were made in this conference, and it is regarded as a failure.
-Project Orion: A project studying the idea of launching and propelling spacecraft by detonating nuclear weapons beneath it. It has the advantage of extremely high thrust and extremely high specific impulse (or propellant efficiency). Most other rocket engines have a trade-off in thrust and efficiency; chemical rockets tend to produce a large amount of thrust inefficiently, while electric ion engines produce small thrust very efficiently.
    -Operation Plumbbob: A series of nuclear tests in the US. In one such test, a manhole was directly atop an air vent for an underground nuclear explosion. A high speed camera captured the manhole in only 1 frame, which puts the lower bound of the manhole's velocity at 66 km/s = 41 mi/s = 147,000 mph.
-Heat engine: An engine that converts heat or thermal energy to work. It uses a hot reservoir, i.e. the heat source, to energize a working body to a higher temperature state; the excess heat is lost in a cold reservoir. The properties of the heated working body are exploited to do work.
    -Carnot's theorem: The maximum efficiency of any heat engine is given by 1 - T_(C)/T_(H) where T_(C) = absolute temperature of the cold reservoir and T_(H) = absolute temperature of the hot reservoir.
-Angle of attack: (represented in fluid dynamics by alpha) The angle between a reference line (the chord line passing through the leading and trailing edges of the airfoil (shape of a wing)) and the velocity vector of the body with respect to the fluid through which it travels. 
    -Airfoil terminology:
        -Leading edge: The point at the front of the airfoil that maximizes curvature.
        -Trailing edge: The point of maximum curvature at the rear of the airfoil.
    -Lift coefficient: A quantity reLating the lift generated by a moving body inversely with the dynamic pressure of the fluid (q = 0.5(rho)(v^2) where rho = fluid density and v = fluid velocity) and planform area. C_(L) = lift coefficient = L/(qS) where L = lift, q = dynamic pressure, S = planform area. Increasing the angle of attack increases the lift coefficient until it hits the maximum lift coefficient, after which it decreases. The critical angle of attack is the unique angle that produces the maximum lift coefficient.
        -Lift: The component of the surface force generated by a fluid acting on a moving body perpendicular to the direction of oncoming flow; lift can be upwards, downwards (i.e. downforce), horizontal (as in sailboats), etc. There are several ways to explain lift, as well as several misconceptions about lift. The two main explanations are derived from either Newton's laws or Bernoulli's laws, though neither scientist ever attempted to explain airflow in itself.
            -Newton's explanation: As air flows around an airfoil, it is deflected (i.e. the fluid is turned). Therefore, the body must be exerting a force on the air, and by Newton's third law, the air must in return be generating an equal and opposite force on the body known as lift. Airplane wings are designed to allow air to flow quickly above then sharply downwards for a long distance, deflecting it downwards. The downward deflection of air causes an upward force on the wing, resisting gravity. Newton's theory is technically correct but cannot concretely explain fluid deflection at its core, and nor is it detailed enough to support the 
                           calculations necessary for engineering applications.
            -Bernoulli's explanation: The fluid particles all have different velocities and are at different locations with respect to the airfoil. Bernoulli's equations relate a fluid's velocity field to its pressure, so there exist a number of pressure variations around any airfoil. Adding up (integrating) these pressure variations and multiplying by the airfoil's surface area yields the net force; lift is the component perpendicular to the direction of oncoming flow (drag is the component parallel). The pressure variations are not, as many believe, caused by the wing's changing the fluid's velocities. The positive angle of attack accelerates the airflow.
                          Bernoulli's equation can be simplified, with caution, to a simple law of continuity: In a moving stream of fluid, the product of the density, cross-sectional area, and fluid velocity is constant. Thus, a body curved as a properly shaped wing with an appropriate angle of attack has a reduced cross-sectional area on the top than on the bottom; the reduction in area creates an increase in velocity, and so decreased pressure above the wing, and so a net upward force - lift. The principle can be derived from either the conservation of energy or Newton's second law. The latter derivation asserts that as a fluid moves to a region of
                              lower pressure, it must, be definition, be coming from a region of higher pressure. This creates a net external force that accelerates the fluid.
                -Bernoulli's principle: The total energy in a steadily flowing fluid system is constant along the flow path. An increase in fluid velocity comes at the price of a decreased internal pressure. Airfoils generating lift must have a pressure imbalance in the surrounding fluid.
                    -Bernoulli's equation: The equation is valid for incompressible flows or compressible flows moving at low Mach numbers; for compressible fluids moving at high Mach numbers, further derivations are required. The equation: (v^2)/2 + gz + p/(rho) = constant, where v = fluid velocity, g = acceleration due to gravity (9.81 m/s/s on earth), z = elevation above a reference plane, p = pressure, and rho = fluid density.
            -L = lift = (0.5)(rho)(C_(L))(v^2) where rho = fluid density (e.g. number of air molecules per unit volume; standard density of air = 1.2754 kg/m^3), C_(L) = coefficient of lift, v = velocity
        -D = drag coefficient = (0.5)(rho)(C_(D))(v^2); varies with angle of attack proportionally, because an increased angle of attack means the wings are "more perpendicular" to the fluid flow.
    -Air flows in streamtubes, or long tubes of air in which air cannot exit into another streamtube. They can be visualized as rectangles of air flowing. A wing passes in the middle of 2 streamtubes: the upper (which passes over the wing) and the lower (which passes under the wing). Because of conservation of energy, the amount of air flowing into a streamtube at some velocity must be equal to the amount of air flowing out of the streamtube, and at the same velocity. So if there is a constriction in a streamtube, the air must speed up to maintain the same amount of air passing through per unit time. The wing constricts the upper streamtube but not the lower streamtube, so there is faster moving air on the top of the wing than on the
     bottom. Faster air exerts less pressure, so the pressure difference pushes the wing up. 
    -Camber: Some wings employ camber, the asymmetry between the top and bottom parts of a wing; wings without camber are thus referred to as symmetric. The shape of the camber allows the wing to produce lift at a zero angle of attack.
-Pascal's principle: Intuitively, the mathematical form of this principle is explained by the idea that the pressure difference at two elevations is equal to the weight of the fluid in between. Mathematically, delta(P) = weight of fluid = (mass of fluid)(g) = (rho)(delta(h))(g), where rho = volumetric density of the fluid, delta(h) = elevation difference in the endpoints of the fluid column, g = acceleration due to gravity. 
-Navier-Stokes equations: Equations that describe how the pressure, temperature, and density of a moving fluid are related. They have their root in the application of Newton's second law to fluid mechanics. In almost all but the simplest of situations, the equations are nonlinear PDEs, making them very difficult to solve and giving rise to conditions of turbulence (chaotic time-dependent behavior in fluid flow).
    -General form of the equation: (rho)((dv/dt) + (v (dot) curl(v) ) = -curl(p) + (del (dot) T) + f, where rho = fluid density, v = fluid velocity, p = pressure, T = component of the total stress tensor, and f = body forces acting on the fluid. Note that (v (dot) curl(v) ) is representative of convective acceleration, or the effect of time-independent acceleration of a fluid with respect to space. This does not imply that individual particles do not experience time-dependent acceleration; they do.
-Mechanical stress: A quantity concerning the internal forces that the particles of a material exert on each other; microscopically, it is the average force of all intermolecular forces and forces from collisions.
-Flux tube: (quantum chromodynamics): The existence of two quarks interacting with each other creates a tube in the gluon field in which zero point fluctuations do not exist (i.e. truly empty space); note that this usually takes energy. These energy fluctuations the quarks interact with make up about 99% of a proton's mass; the other 1% comes from the 3 main quarks and the constantly pair produced quark-anti quark pairs.
-Coastline paradox: The apparent paradox resulting from inaccuracies in the measurement of fractal shapes. As a classic example, if one were to measure the coastline of Australia, which is essentially a very jagged coast with thousands of inlands and cliffs, using a 1 km unit stick, the sum is 12,500 km, but using a 1 m stick, it's 25,700 km. This is because the more accurate measuring instrument measures more of the inlands and missed perimeter. So as measuring devices become more and more accurate, the total perimeter must approach infinity. 
    -The paradox applies to all fractals. Consider a triangle with sides of length 1. In the center of each side, put another equilateral triangle with the base of 1/3 the larger triangles base. Add triangles forever, and the perimeter must approach infinity, even though the infinite triangles enclose a finite area.
    -The "paradox" never made sense to me because it only highlights the inaccuracies of approximations; with an infinitely accurate tool that measured each individual atom of the coastline, the true finite value would emerge. The paradox results from the confusion involved with convergent infinite series; that an infinite amount of terms can add or multiply to a finite sum/product.
-Magnus force: There is a force, due to an object moving through a fluid, perpendicular to the direction of rotation of a moving body, e.g. a ball that has been thrown with spin. Objects with topspin, i.e. rotating clockwise in the same direction as the object's velocity, have an upward force perpendicular to velocity. Objects rotating against the direction of their velocity have forces acting on them downwards.
    -The origin of the force is the behavior of fluids when the interact with the moving object. When an object is rotating in the same direction as its velocity, the fluid (usually air) is moving in the same direction of rotation at the bottom of the ball, and so is deflected upwards by friction. At the top of the ball, the object is rotating against the fluid flow, causing the air to slow down. The net effect is an upward force. Objects rotating counterclockwise to the direction of velocity experience the same effect but in reverse and are deflected downwards. Objects spinning on their sides also experience Magnus forces to the left of right.
    -Seams: When a ball has seams, another aerodynamic force acts. If the ball's seams (i.e. simply an area of roughness or protruding material, such as stitching) are on one side of the ball or are not aligned with the ball's direction of motion, the smooth side of the ball allows laminar air flow to quickly pass over it, but the side with seams disrupts the air flow and makes it turbulent and stick to the ball, and the air is then deflected away from that side of the ball as it finally does leave the ball's surface. The quick deflection towards the smoother side of the ball means the air is exerting less pressure on that side of the ball, and therefore the ball moves in that direction (i.e. the direction the seams are on). The ball doesn't necessarily need to have seams either; all it needs is a rough side and a smooth side. This is why cricket players often polish one side of the cricket ball. 
-Frozen light: In 2003, a group of American physicists managed to lower the speed of light to 0 m/s for a few milliseconds, though it can, in theory, be sustained for longer. 
-Center of pressure: The point at which the sum of all pressure fields are acting on a body, producing a net force (on that point). This force is found by integrating the pressure field interpreted as a vector field. It is the average location of the pressure on the object. 
-Overclocking: Modding a computer or computer like device to operate faster than its manufacturer designated clock speed. Overclocking usually comes at the price of increased power usage and increased heat output.
    -Clock rate: The frequency the computer processing unit (CPU) runs at, measured in cycles/second = Hz. Specifically, the clock rate refers to the frequency of the oscillator crystal (A circuit that produces an oscilLating electronic signal; crystal oscillators use the resonance of vibrating crystals to create electronic signals with precise frequencies), which typically runs as a sine wave.
-Schadenfreude: Pleasure gained by misfortune befalling other people.
-Cherenkov radiation: Electromagnetic radiation, usually in the blue visible spectrum, that is emitted when a particle exceeds the speed of light in some non-vacuum medium. The velocity of the particle must exceed the phase velocity of light in that medium, and the medium is therefore dielectric (an electrical insulator that can become polarized when under the influence of an electric field). This is what causes nuclear reactors to glow blue. 
    -Phase velocity: The velocity of the phase of a wave as the wave propagates through space. v_(p) = phase velocity = lambda/T, lambda = wavelength and T = period
     Group velocity: The velocity the amplitudes of a wave have as the wave propagates through space.
    -Mechanism: A charged particle traveling at relative superluminous speeds in some medium disturbs the local electromagnetic field, displacing the electrons of the atoms making up the medium and polarizing the atoms. These electrons return to equilibrium afterwards, which involves a return to a ground state. This emits photons. This phenomenon always exists to some extent when a particle of any velocity passes through any medium, but only at a speed greater than the phase velocity of light in that medium do the photons constructively interfere, rather than destructively, as they do otherwise.
-Sonic boom: A shock wave of enormous energy produced when an object exceeds the speed of sound while traveling through air. When any object moves through a fluid, commonly air, it creates a series of pressure waves behind and in front of it. At some critical speed, i.e. the speed of sound (Mach 1), these pressure waves are produced so quickly that they do not dissipate to allow the next wave to pass through, causing them to become forced together in one shock wave. 
-Mirror neuron: A neuron that is activated when the host observes another animal performing the action coded by that neuron, e.g. laughing when seeing someone else laughing. These neurons are found in the premoto cortex and primary somatosensory cortex. These neurons are instrumental in learning through observation, and heavily influence a human's sense of empathy. They evolved in humans in order to facilitate the construction and organization of large communities, which were a far more formidable force to be reckoned with by predators than any single human.
-Coanda effect: A stream of fluid tends to be attracted to curved surfaces immersed in the fluid or nearby the fluid. It occurs due to friction; as the fluid flows by a surface, friction between the surface and the fluid's particles slows the fluid down by producing a force that acts opposite the direction of motion. Because the surface is curved, the vector of the frictional force acts not only in the horizontal direction, but it also has a slight vertical component that pulls the fluid towards the surface. This vertical component is strong enough for fluid to bend around corners or cylinders in a circular path.
-Venturi effect: Fluid pressure decreases when it flows from a pipe with a certain cross sectional area to a constriction in the pipe with a smaller cross sectional surface area.
-Pedigree collapse: The observed phenomenon in which as a population grows larger, individuals that share relatively recent common ancestors will mate at the same frequency as those without, causing the family tree to shrink as the number of unique ancestors is cut down; some ancestors are "used twice". If pedigree collapse didn't exist, then family trees would grow exponentially; it implies, for example, that a mere 50 human generations ago (approximately 1000 years) there existed 2^50 = 1,125,899,906,842,624 = about 1 quadrillion people, far greater than the world population at the time. Because of pedigree collapse, this isn't the case. Each person's ancestry tree is not composed of unique individuals; many individuals crop up in multiple places for many peoples' family trees. The most famous of these was a fluke: Genghis Khan, who is estimated to be the ancestor of roughly 8% of all people of Asian descent. Moreover, in the past it has been customary to marry, and therefore mate, within the family so as to preserve and prevent the dilution of status, wealth, and power. 
    -Because of pedigree collapse, although one's family tree does grow pseudo-exponentially as one goes back generations, the tree actually collapses and starts to shrink at some point, resulting in a diamond shape rather than an exponential tree shape; for most people, this point is around 1200 AD. 
-Stages of death: Following the cessation of brain activity and any other biological processes required to sustain life, human corpses undergo a series of stages due to natural causes.
    -Stage 1: Pallor mortis: The skin pales very quickly after death (30 minutes after at maximum). This is a result of the lack of blood circulation.
    -Stage 2: Algor mortis: The body temperature of a corpse drops after death in a steady linear path until it is in thermal equilibrium with the environment, i.e. the same temperature as its surroundings (the ambient temperature). 
        -Glaister equation: An estimation of the time elapsed, in hours, of a corpse found with a particular rectal temperature. The equation: Number of hours elapsed = (98.4 - rectal temperature)/1.5, where the temperatures are given in Fahrenheit. The equation is merely an approximation.
    -Stage 3: Rigor mortis: Once the chemicals and functions going on in the muscular system cease, a corpse's limbs stiffen and are difficult to induce locomotion in. The process begins at 3-4 hours after death and stabilizes at maximum stiffness at 12 hours after death. It dissipates 48-60 hours after death.
    -Stage 4: Livor mortis: The purple-red discoloration of the skin of a corpse caused by the lack of uniform distribution of blood in the body. As the blood sinks to the feet or the bottom of the body (if the body is lying horizontally), the skin appears discolored. It begins 20 minutes - 3 hours after death and maximimizes/stabilizes at 6-12 hours after death.
    -Stage 5: Putrefaction: The decomposition of the proteins that leads to the breakdown of of cohesion between body tissues. Eventually, the organs liquefy. Putrefaction is caused primarily by bacteria.
    -Stage 6: Decomposition: The matter making up the body decomposes into simpler compounds.
    -Stage 7: Skeletonization: All tissues have decomposed or dried up, leaving only the skeleton. 
-Format of a scientific paper: The scientific format is highly structured and rigid, and differs greatly from writing in the humanities. This serves two main purposes: first, it offers a means for all scientists to homogeneously and efficiently communicate their ideas to the community; second, it allows for an analysis/reading of the paper at many sub-levels that conform to particular restraints, e.g. time. So, a reader can skim titles to quickly get the gist of the paper, or scan the abstracts to get general information, or read the background and abstracts for an introduction to the subject, or read teh results and conclusion for an in-depth analysis.
    -Parts of the format: 1. Abstract: Usually 1 paragraph long. The abstract summarizes all the major components and aspects of the paper. It covers first the purpose of the paper (question investigated), second the experimental design (and methods used), third the major findings and results, and fourth the interpretation of the results (and implications and conclusions drawn).
                  2. Introduction: Establishes the context of the paper and the hypothesis. The type of approach being used and the rationale for using it is discussed. 
                  3. Materials and Methods: The layout of the study is discussed. Describe exactly what was studied and the setup of the field site. Describe the experiment or sampling design. Describe how the data were analyzed. 
                  4. Results: Objectively discuss the key results and data. Use appropriate graphs and other visual aids as well as text. Organize the data in logical sequence. Always begin with text. Summarize the statistical analysis being applied. 
                  5. Discussion: Interpret the results with respect to current models and the information that already exists. Connect back to the introduction's hypothesis and investigative question. Discuss if the results support the hypothesis, if they agree with any other experiments testing the same question, and what new information, if any, has come to light due to the data. 
                  6. Acknowledgments: (Optional) Thank all entities that contributed significant assistance to the study. Acknowledge outside reviewers and sources of funding. Keep it brief.
                  7. Literature Cited: An alphabetical listing of all references cited in the body. Use appropriate in-text citations; these should contain, in parentheses, the last name of the author and year of publication. If the reference has a page number, cite that as well. Put multiple authors' names together if necessary (e.g. A and B...). Place the period after the parentheses.
                    -Example: (Gumwad 1952:209; Bugjuice 1970)
                    -http://abacus.bates.edu/~ganderso/biology/resources/writing/HTWcitations.html
                  8. Appendices: (Optional) Non-essential information that further clarifies information presented in the body. Identify multiple appendices with Roman numerals.
-DCIM: Stands for Digital Camera IMages. It is part of the photo and camera system on portable devices.
-Viscosity: A measure of a fluid's resistance to mechanical stress; more specifically, how well it resists shear forces and tensile stresses. It has its root in frictional interactions that consume energy between the particles of a fluid, especially when neighboring parcels of a fluid travel at varying velocities. A fluid with zero viscosity is an ideal fluid (A.K.A. inviscid fluid); they exist only as theoretical constructs to simplify calculations or at extremely low temperatures in superfluids. Viscosity is measured in Pascal-seconds (Pa * s)
    -Viscosity of water: 1E(-6) Pa*s
    -Viscosity of air: 1E(-5) Pa*s
-de Broglie relations: (Pronounced day-broy) A hypothesis that relations and equations involving the energy of waves could be translated into use for matter, because the two were really indistinguishable. 
    -Derivation: Using Einstein's equation E = mc^2 for matter and Plank's equation E = hf for waves, de Broglie showed that, if particle-wave duality were true, mc^2 = hf. Because matter cannot travel at the speed of light, substitute v = c, and because f = v/(lambda), substitute for f. Thus, mv^2 = h(v/(lambda)). Therefore, lambda = wavelength = h/(mv) = h/p; p = momentum. 
             Moreover, it can be shown that f = E/h.
-Feynman diagrams: A useful tool for visualizing particle scattering. The instate is the set of particles that are going to collide, and the outstate is the set of products; these can be measured. What happens in between is described by Feynman diagrams. Collisions involving matter-antimatter interactions have no outstate and cease at an energy production, which may or may not produce additional matter/antimatter. The diagrams allow one to work out the probability of any given scattering pattern (between the instate and the outstate) occurring. Graphically, a Feynman diagram is a graph of two particles as time moves on, with the particles represented as straight lines
           and the energy released expressed as wavelike lines. 
    -Image of a Feynman diagram: http://upload.wikimedia.org/wikipedia/commons/1/1f/Feynmann_Diagram_Gluon_Radiation.svg
-NASCAR: National Association for Stock Car Auto Racing
-Why fire glows: Pure flames release lots of energy that excite the atoms of the air into higher energy states; when they return to equilibrium, blue light is released. Less pure flames (e.g. wood fires) produce blue light, but it is overpowered by the intensity of red and orange light produced by the particles of soot that are glowing red hot (black body radiation).
-Alcubierre drive: (pronounced AL-KOOB-EE-AIREE) A proposed spacecraft design that allows for superluminous travel. It operates in theory by creating a bubble in spacetime in which the spacecraft stays. The space in between the warp bubble and the destination would be contracted at many times the speed of light (using gravitational forces), while the space between the warp bubble and the starting point would be correspondingly and equally expanded (by using the forces created by exotic matter), thus creating the illusion of faster-than-light speeds, though within the warp bubble itself all laws of physics are upheld. Relativistic effects such as time dilation or length contraction would not apply, as the spacecraft within the warp bubble is 
                         technically stationary. The idea can be derived from the solution to Einstein's field equations. 
    -Energy requirements: Although Alcubierre's model would have required the energy equivalent of the mass of Jupiter, a NASA physicist named Harold White proposed increasing the thickness of the negative energy warp bubble (e.g. analogous from going from a string wound in a circle to a donut) and oscilLating the warp bubble, which took the energy requirement down to 65 exajoules (the amount of energy used by the USA in one year).
    -Einstein's field equations: A set of 10 equations that completely describe gravity, incorporating the force as a property of spacetime in the presence of mass. Given a mass-energy with linear momentum, the corresponding spacetime geometry can be determined (specifically, a permutation of stress-energy in spacetime is mapped onto the appropriate metric tensor of spacetime). In sub-relativistic velocities and weak gravitational fields, the equations approximately reduce to Newton's equations (which are really approximations).
        -The equation: R_((mu)(nu)) - 0.5(g_((mu)(nu)))(R) + (g_((mu)(nu)))(lambda) = (((8pi)(G))/(c^4))(T_((mu)(nu))) where R_((mu)(nu)) = Ricci curvature tensor, R = scalar curvature, g((mu)(nu)) = metric tensor, lambda = cosmological constant, pi = Archimedes' circle constant, G = Newton's gravitational constant, c = speed of light in vacuum, and T_((mu)(nu)) = stress-energy tensor
            -Ricci curvature tensor: A tensor describing the error in approximating the volume of a geodesic ball (a rough sphere made out of n geodesic polygons) with the volume of a perfect sphere in n-dimensional Euclidean space.
            -Metric tensor: A rank-4 tensor that accurately generalizes a gravitational field.
            -Stress-energy tensor: A tensor describing the density and flux of energy and momentum at a point or space in spacetime. 
                -Stress tensor: (i.e. stress) A specific form of the above tensor used in Newtonian physics that expresses the internal forces that the particles of a system exert on each other.
-Time complexity: A property applied to algorithms that measures the amount of time it takes for a given algorithm to solve a problem of a given input size. Because different inputs of the same size may involve different amounts of time to solve, the maximum time for all inputs of a given size is used (worst-case time complexity). 
    -Big O Notation: As the input size approaches infinity, the time complexity is expressed in Big O Notation. It is the largest amount of time taken that makes a significant difference and depends on the input size. A time complexity of a certain function f(x) is written as f(x) = O(g(x)) where g(x) is the function for the time complexity. If g(x) is linear, the time complexity is linear, if it has higher exponents, f(x) has a polynomial time, and if it is exponential, f(x) has exponential time, and so on.
        -For example, the function f(x) = 10x^8 + 3x^3 - 5 is the sum of three factors: 10x^8, 3x^3, and -5. As x (input size) tends to infinity, only the 10x^8 (the largest factor) is significant, and because the coefficient 10 is not dependent on x, the time complexity is g(x) = x^8. Therefore, f(x) is a big-oh of (x^8). Mathematically, f(x) = O(x^8).
        -The notation describes the worst case scenario, in that the growth rate given is the least possible growth rate. 
-P vs. NP Problem: One of Clay Institutes's Seven Millennium problems that is a huge unsolved problem in computer science. It asks if every problem with an easily verifiable solution (by a computer) is also easily solvable (by a computer). A problem is defined as "easily solvable" if an algorithm exists that can solve the problem for any input in polynomial time. 
-Antivenom: Antibodies used to combat venom. They do not reverse the damage of venom, but they do halt further damage. It is created by injecting small amounts of venom from the desired animal whose venom is in question into a horse, sheep, or goat. The horse, sheep, or goat's body creates antibodies that can be harvested and injected if the time comes. It is extremely difficult to make, as a single snake milking provides very little venom, and not many antibodies are produced. One vial can cost up to $1500, and to save someone with an extreme amount of venom injected can take up to 20 or 30 vials.
    -Mithridatism: The process of injecting oneself with the venom so as to naturally make the antibodies oneself. Eventually, total immunity can be achieved, even for multiple bites of extremely venomous animals. It is not effective against all types of poison. It is generally an unpleasant process, as even the trace amounts of venom injected at regular intervals (e.g. monthly) have adverse effects, and so it is mainly used by people that regularly handle venomous animals, such as zoo keepers or researchers. 
-Homemade dry ice: Small amounts of solid carbon dioxide can be produced at home. It is extremely cold at -77.5 degrees Celsius, and causes instant frostbite. It is primarily used for cooling objects when ordinary ice is not sufficient, and it also does not leave residue. It sublimates directly into the gaseous form of carbon dioxide at atmospheric pressure and room temperature. It should not be handled without defensive equipment for more than 3-4 seconds of contact with skin at a time. To make it:
    -Materials: Either a CO_(2) fire extinguisher (it will be marked with a CO_(2) symbol) or a CO_(2) tank, a cloth bag (such as a pillow case), heavy-duty gloves, and optional duct tape.
    -Process: Using the heavy-duty gloves, put the nozzle of the CO_(2) container into the cloth bag. Clamp the bag's mouth around the nozzle, making sure it is as tight as possible; if necessary, use duct tape to tape it shut. Turn on the fire extinguisher or CO_(2) tank; dry ice immediately forms. When the desired amount has been achieved, inactivate the CO_(2) container, making sure to dislodge any loose dry ice still on the nozzle. To extend the shelf life of the dry ice, put it in the freezer. 
-Emergency orienteering techniques: Finding true north without a compass can be useful in survival situations.
    -With an analog timepiece: (Applies to locations in the Northern hemisphere) Hold the watch horizontally and point the hour hand at the sun. Exactly halfway between the hour hand and the 12, going clockwise from the hour hand to the 12, is north. 
    -Shadow-Tip method: Plant a stick in the ground, making sure its shadow is clearly visible and cast on level ground. Mark the tip of the shadow with a landmark. Wait 10-15 minutes for the shadow to move. Mark the new tip of the new shadow with another landmark. The line passing through the two landmarks is the east-west line, with landmark 1 being the western point and landmark 2 being the eastern point. Standing with landmark 1 to the left and landmark 2 to the right, the direction ahead directly perpendicular to the east-west line is true north. 
        -Note that it does not have to be a stick; any fixed object with a visible shadow will work. The taller the object, the more accurate.
    -Using stars: (Applies to locations in the Northern hemisphere) Find the Big Dipper constellation. The last star on the handle is the Northern star, Polaris. An imaginary line from Polaris to the ground in front of someone looking at Polaris is true north. 
-Morse code: A method of communication by transmitting textual information visually or audibly, often in the absence of technology. It has several advantages; although it takes considerably longer than other forms of communication, it can be done without any technology, such as with a light or by tapping. It works by assigning each letter and digit in the Latin-Arabic letter scheme a code, and words are then spelled out by typing the code. Codes are made up of a series of dots and dashes, each defined by its duration. A dot lasts approximately the duration of a single tap, whereas a dash lasts exactly 3 times a dot. Each dot/dash in a letter is followed by a silence equal in duration to that of a dot, and entire letters are separated by a
         a silence equal in duration to that of a dash. Entire words are separated with a silence equal in duration to 7 dots, or 2 dashes and 1 dot. The codes are assigned in such a way as the length of time to type a letter is inversely proportional to the letter's frequency of use in the language, for efficiency. 
    -The English alphabet:
        A: .-             N: -.
        B: -...       O: ---
        C: -.-.       P: .--.
        D: -..        Q: --.- 
        E: .          R: .-.    
        F: ..-.       S: ...
        G: --.        T: -
        H: ....       U: ..- 
        I: ..         V: ...-
        J: .---       W: .--
        K: -.-        X: -..-
        L: .-..       Y: -.--
        M: --         Z: --..
    -Arabic numerals:
        1: .----      6: -....
        2: ..---      7: --...
        3: ...--      8: ---..
        4: ....-      9: ----.
        5: .....      0: -----
    -Punctuation: 
        Period: .-.-.-
        Comma: --..--
        Question mark: ..--..
        Apostrophe: .-----.
        Exclamation mark: -.-.--
    -Demonstration: http://www.youtube.com/watch?v=L6gxfX4GrbI
-Radians to degrees: 1 radian = 57.2957795 degrees, 1 degree = 0.0174532925 radians
-Meters per second to miles per hour: 1 m/s = 2.23694 mph, 1 mph = 0.44704 m/s
-Miles to kilometers: 1 mile = 1.60934 kilometers, 1 kilometer = 0.621371 miles
-Daylight savings time: March 3 to November 10 in the US
-Perturbation theory: Solving a problem, or approximating it, by simplifying the problem and obtaining an approximate answer, then slowly approaching the original problem with slight deviations to approximate an answer. The exact solution to a related and simpler problem is used to perturb the simple problem, by slightly altering its conditions to make it more closely resemble the original problem, and so a newer and better approximation of a solution is obtained. Successive iterations become more and more complex, however.
-Bayesian inference: Using Bayes' Rule to continuously update an event's probability as further information is learned. 
    -Bayes' Rule: If event A1 has a certain probability and event A2 also has a certain probability, the prior probability is the ratio of the two probabilities. The posterior probability is the ratio of the probability of A1 given a new event B to the probability of A2 given the same B. THe rule states that the posterior and prior probabilities are proportional and related by Bayes' factor, denoted lambda.
    -Frequentism: The standard interpretation of probability in which an event's probability is the limit of its relative frequency in a large number of trials; the proportion of times it occurs in a very large number of trials is the probability.
    -Bayesianism: An alternate interpretation of probability that quantifies the degree of belief that something is true as the probability. It therefore represents a state of knowledge and can be altered with new information. 
-Markov chain: A system that constantly transitions from one state to the next, with the process of transition being completely random and therefore predictable in the long run. The next state that will be transitioned to depends only on the current state and not on any prior states; the process is memoryless. 
    -Stochastic matrix: A matrix used in probability theory to concisely formulate the probabilities of moving from one state in a Markov chain to the next. Specifically, the matrix is right stochastic (all rows sum to 1), because the probability of moving from one state to any other must be 1. The probability of moving from state i to state j is given by the probability of j given i, and is set in the i-th row and j-th column of the stochastic matrix. The probability of moving from state i to state j in n steps is given by the entry in the i-th row and j-th column of the n-th power of the stochastic matrix (ie the stochastic matrix multiplied by itself n times). 
-Periodic continued fraction: A fraction that continues forever and is of the form a_(0) + 1/(a_(1) + 1/(a_(2) + 1/(a_(3) + 1/(a_(4) + ...)))). Many irrational numbers can be expanded into a periodic continuous fraction. Any periodic continued fraction must be a quadratic irrational number (an irrational number that is a root of a quadratic equation ax^2 + bx + c = 0 and can therefore be written in the form (P + sqrt(D))/(Q)), and conversely, any quadratic irrational number has a periodic regular continued fraction expansion. 
    -Notation: For the above periodic continued fraction, the notation would be x = [a_(0); a_(1), a_(2), a_(3), ...] 
        -Kettenbruch notation: When the numbers in the continued fraction representation follow a pattern that is described by a sequence, we can use notation similar to sigma notation. Instead of using a sigma, a large K is used. An index is specified, such as i, with upper and lower bounds. The argument of the operating notation is (b_i)/(a_i), where b_i and a_i are both sequences. This represents the continued fraction a_0 + (b_1)/(a_1 + (b_2)/(a_2 + (b_3)/(a_3 + ...))).
    -Square roots: Every square root of a natural number has a periodic (ie repeating) continued fraction representation. 
        -Algorithm: To find the continued fraction representation of a square root of a natural number, use the following algorithm. Assume that we are finding a representation of sqrt(n), for natural number n.
            1. Find the largest perfect square that is smaller than n. This means we want a natural number m such that m^2 < n < (m + 1)^2.     
            2. Then there exists a positive real number x such that sqrt(n) = m + 1/x.
            3. Solving for x, we have: x = 1/(sqrt(n) - m). 
            4. Rationalize the right hand side of the above equation by multiplying it by (sqrt(n) + m)/(sqrt(n) + m). Since the denominator of 1/(sqrt(n) - m) multiplied by the denominator of (sqrt(n) + m) divided by itself is a difference of squares, we have: x = (sqrt(n) + m)/(n - m^2).
            5. Therefore, sqrt(n) = m + 1/((sqrt(n) + m)/(n - m^2)). Repeat steps 1 through 3 by using the expression (sqrt(n) + m)/(n - m^2) as the new "n" value. Continue repeating until the final expression (at step 3 of the algorithm) is of the form sqrt(n) + k, where k is some integer. The continued fraction representation will now repeat through all the current values obtained.
        -Example: To find the continued fraction representation of sqrt(14):
            1. Since 9 < 14 < 16, there exists positive real number x such that sqrt(14) = 3 + 1/x. 
            2. Solving for x and rationalizing: x = 1/(sqrt(14) - 3) = 1/(sqrt(14) - 3) * (sqrt(14) + 3)/(sqrt(14 + 3)) = (sqrt(14) + 3)/5.
            3. Since the above expression is not of the form sqrt(14) + (some integer), we repeat the process using (sqrt(14) + 3)/5. Since sqrt(14) < sqrt(16) = 4, and 4 + 3 = 7, sqrt(14) + 3 is bounded above by 7, and the expression (sqrt(14) + 3)/5 is bounded above by 7/5, which implies that it is bounded above by 2 (since 7/5 < 2). Thus, we have: (sqrt(14) + 3)/5 = 1 + 1/x.
            4. Solving for x and rationalizing: x = (sqrt(14) + 2)/2. This expression does not match our criteria for termination, so we repeat.
            5. Since sqrt(14) is bounded above by 4, the quantity (sqrt(14) + 2)/2 is bounded above by 3. Thus, we have: (sqrt(14) + 2)/2 = 2 + 1/x.
            6. Solving for x and rationalizing: x = (sqrt(14) + 2)/5. We repeat.
            7. (sqrt(14) + 2)/5 = 1 + 1/x.
            8. Solving for x and rationalizing: x = sqrt(14) + 3. We have now reached our termination criterion, so we stop.
            9. Thus, the continued fraction representation of sqrt(14) is [3; 1, 2, 1, ...] = 3 + 1/(1 + 1/(2 + 1/(1 + 1/(1 + 1/(2 + ...))))).
    -Continued fractions representation of pi:
        -pi = 4/(1 + (1^2)/(2 + (3^2)/(2 + (5^2)/(2 + (7^2)/(2 + ...))))) = 3 + (1^2)/(6 + (3^2)/(6 + (5^2)/(6 + (7^2)/(6 + ...)))) = 4/(1 + (1^2)/(3 + (2^2)/(5 + (3^2)/(7 + (4^2)/(9 + ...)))))
-Quadratic formula: The zeros of a quadratic function in general form ax^2 + bx + c = 0 is x = (-b +/- sqrt(b^2 - 4ac))/(2a). It can be derived by completing the square of the general form equation. Note that a cannot be equal to 0.
-Root of unity: Any complex number that yields 1 when raised to some positive integer power.
    -Cube root of unity: w = 1/2 + ((sqrt(3))/2)i where i = sqrt(-1). w^2 = 1/2 - ((sqrt(3))/2)i, or the complex conjugate of w itself, and w^3 = 1.
-Cardano's Formula: Gives the roots of a cubic function.
-Clarke's 3 Laws: The three laws of prediction:
    1. When a distinguished but elderly scientist states that something is possible, he is almost certainly right. When he states that something is impossible, he is very probably wrong.
    2. The only way of discovering the limits of the possible is to venture a little way past them into the impossible.
    3. Any sufficiently advanced technology is indistinguishable from magic.
-Four factors determining the wages of a job:
    1. Supply of labor (how many people are available to do the job or want to do the job)
    2. Specialization (How specialized the job is)
    3. Unpleasantness: (How unpleasant the job is)
    4. Demand (How much demand there is for the products of the job)
-Gamma function: Gamma(n) = integral from 0 to infinity of (e^(-x))(x)^(n-1), where n = positive integer
    -Properties: 1. Gamma(1) = 1
             2. Gamma(n + 1) = (n)(gamma(n))
            -Proof: i. gamma(z + 1) = int(0, inf) of ( (t^z)(e^(-t)) ).
                    ii. By integration by parts with u = t^z and dv = e^(-t), gamma(z + 1) = ( (-t^z)(e^(-t))|(0 to infinity) ) + (z)(int(0, inf) of (t^(z-1))(e^(-t))
                iii. The first term is 0, and the second reduces to (z)(gamma(z))
                iv. Therefore, gamma(z + 1) = (z)(gamma(z)) = (z)(z-1)(gamma(z-1)) = (z)(z-1)(z-2)(gamma(z-2)) = ... = z!
             3. Gamma(n + 1) = n! because gamma(n + 1) = n(gamma(n)) = n(n-1)(gamma(n-2)) = ... = n!(gamma(1)) = n!
             4. (gamma(n))(gamma(1-n)) = pi/sin((n)(pi))
             5. Gamma(1/2) = sqrt(pi)
    -Pi function: pi(z) = integral from 0 to infinity of (e^(-t))(t^z) = gamma(z + 1) = (z)(gamma(z)). Essentially, it is an offset of the gamma function to transform it into the factorial function, since pi(n) = n!. 
-Q.E.D.: Quod erat demonstrandum; means "it has been demonstrated".
-Primality test: A test applied to numbers to check if the number is prime or composite. Some tests output that a number is prime while others output that it is composite. The tests do not produce prime factorizations of numbers, as it becomes exceedingly difficult to factor large numbers (factoring a 129 digit number took a group of 100 people 17 years) and so they merely check if the number is prime or not. They are relatively efficient and run in polynomial time.  
    -Simple methods:
        -Most simple method: For an integer n, check if every integer up to n-1 can be divided by n and leave a remainder of 0.
        -More advanced methods: To narrow down the integers that must be checked as factors, first notice that no factor of n can ever be greater than n/2, so only integers up to n/2 must be checked. Even numbers greater than 2 do not need to be checked, because (if only the primality of a number is in question) if a number is divisible by an even number, it is also divisible by 2, which is being checked. Numbers greater than the square root of n do not need to be checked because this is a "tipping point" in the factors of n; after the factors of (sqrt(n))(sqrt(n)), the factors simply reverse from those below sqrt(n). All integers can be written in the form 6k +/- (-1, 0, 1, 2, 3, 4) where k is an integer. Because 2 and 3 are always factors of 
                    6k + 0, 6k + 2, 6k + 3, and 6k + 4, all prime numbers must be of the form 6k +/- 1. So to check if a number is prime, approximate its square root and check the numbers 2, 3, and integers of the form 6k +/- 1 up to sqrt(n).
-Vinculum: A fraction bar. It is a grouping symbol and extends back to the middle east. It is also used as the bar over repeating digits in a repeating decimal and as part of the square root radical.
-Diuretic: A substance that, when ingested, stimulates the production of urine and increases urination. 
-Senses: Biological mechanisms used by organisms to gather data about the environment. The traditional five senses are sight, hearing, taste, smell, and touch. These are scientifically referred to as, ophthalamoception, audioception, gustaoception, olfacoception, and tactioception, respectively. Other senses:
    -Equilibrioception: A sense of balance and acceleration. These capabilities are stored in fluids in the human ear that move about when the body accelerates, giving rise to a sense of balance.
    -Thermoception: A sense of temperature.
    -Proprioception: A kinesthetic sense of self-awareness; allows one to instinctively know where each part of the body is without having to use any other senses to gather that data.
    -Nocioception: The ability to detect physiological or mental damage to one's body; the ability to feel pain.
    -Chronoception: A biological clock that allows one to have a rough estimate of the flow of time.
-Lagrangian interpolation: Finding a polynomial of the least degree (degree of (n-1)) that passes through any given n ordered pairs.
    -Process: Create an input and output table, and then rewrite it as a sum of roots and the actual values. For example, for the ordered pairs (1,5), (3, 10), and (11, 21):
        x | y   =    x | y   +   x | y   +   x | y      
        1 | 5        1 | 5       1 | 0   1 | 0
        3 | 10       3 | 0       3 | 10  3 | 0
        11| 21       11| 0       11| 0   11| 21
          Then set up a number of equations such that (C1)(x-3)(x-11) = f(x), (C2)(x-1)(x-11), and (C3)(x-1)(x-11). For each of the tables in the summation of the total table, multiply all the zeros by a constant to find the equation for that table. Plug in the ordered pair that isn't zero to find the constant. When each individual table has its own polynomial, add them all up to obtain a polynomial that passes through the original points. 
-Lagrangian point: In a restricted 3-body gravitational problem (in which the only relevant forces are gravitational; called restricted because the mass of the third object is negligible), there are 5 Lagrange points (L-points) where the gravitational forces of the two massive objects exactly balance to keep the third object orbiting in a constant shape at the same speed as the first object's orbit. In the earth-sun problem in which a satellite needs to orbit with the earth at the same speed: 
    -L1: The first L-point is located 1.5 million km between the earth and the sun. The reduced radius of orbit means the sun's gravity is stronger on the satellite than on the earth, causing the satellite to move faster, but the earth's gravitational pull in the opposite direction offsets the sun's extra gravitational pull to exactly compensate such that the satellite travels at the same speed as the earth.
        -Illustration: Sun......................Satellite.....Earth
    -L2: The second L-point is located 1.5 million km away from the earth behind the earth. Because the satellite is further away from the sun, it orbits slower, but the earth's extra gravitational pull compensates to allow it to travel at the same speed.
        -Illustration: Sun.........................Earth.....Satellite
    -L3: The third L-point is located on  the exact other side of the sun as the earth, except slightly closer to the sun than the earth is (because the earth also affects the sun with gravity, so both objects technically orbit around the sun-earth system's center of mass, not the sun's center. This COM is still within the sun's body, however). At this point, the earth and sun's combined gravitational pull allow the object to orbit with the same period as the earth.
        -Illustration: Satellite................Sun...................Earth
    -L4 and L5: A line drawn between the sun's and the earth's center of masses can be used to construct two equilateral triangles with sides of the same length as the distance between the two objects and two vertices at the two body's centers of mass. (in the case of the sun and the earth, it is an equilateral triangle with sides of length 150 million km); the first triangle has its third vertex below the sun-earth line and the other has it below. These are the L4 and L5 points, respectively. The same distance between the two objects balances out. 
-Dirty thunderstorm: A volcanic eruption releases enough charged particles to induce lightning that is mixed in with the volcanic plume. 
-Massive retaliation: The military doctrine of responding to attacks with forces much larger than and greatly disproportionate to the attack, with the aim of deterring initial attacks in the first place. Thus, the strategy must be widely known. It operates on the same basic principles as mutually assured destruction.
-Capital Pi Notation: The capital pi is used to denote a product. Just as a capital sigma is used for summation of terms that are related to their position in a sequence, the pi function is the same thing but the product. Otherwise, sigma and pi notation are identical.
-Graphene manufacture: Rub some pencil graphite on paper. Using tape form a compression onto the graphite and peel the tape off. This divides the graphite into two layers. Use more tape to do the same with the piece of graphite on the tape, dividing it into more layers, each thinner than the last (by a factor of about 1/2). Repeat several times.
-Ransberger Technique: A debate technique used to disagree with someone without bruising their ego. It usually comes in 3 steps:
    1. Ask the person (that you think is wrong) to explain their position. 
    2. Understand why they feel that way (i.e. why they are wrong)
    3. Explain common ground you share with their position, no matter how basic, and use it as a starting point to explain why your solution is best for the agreed problem.
-Fahrenheit-Celsius conversion: F = (9/5)C + 32
-Gallium and mercury destroy aluminum, which absorbs gallium like a sponge absorbs water. The modified aluminum is so weak it can be torn apart like wet paper.
-Body language: When identifying specific body language behaviors and drawing conclusions from them, first establish a baseline for each specific person (i.e. how they normally behave under normal circumstances) then look for deviations.
    -Stress response: Freeze, Flight, Fight.
        -Freeze: Under stress, organisms first freeze to avoid detection (predatory detection systems are principally attracted to motion) and to give the brain a chance to analyze the situation. If a person suddenly ceases all baseline movement, it is a sign of stress. If the legs are locked in one position the entire time, it is a freeze response.
            -Turtle Response: Lowering the head and raising shoulders, with hands in front and walking with a hunch is the turtle behavior and is done to minimize the amount of space taken up and therefore decrease chances of being spotted.
        -Flight: Distancing oneself from something or someone, or putting objects between oneself and something or someone. Turning feet away. Leaning backward. 
            -Blocking behaviors: Blocking the eyes, rubbing them, closing them are all signs of revulsion or anger. Raising the eyebrows is a sign of happiness.
        -Fight: Threats to one's personal space mean the offender is being aggressive. Puffing out the chest is also a sign of aggression. 
    -Pacifying behaviors: Things the brain automatically has the body do when under pressure or stress to soothe and calm itself.
        -Covering the supernasternal notch (neck dimple): A sign of insecurity, emotional discomfort, fear, or concern.
        -Rubbing the forehead: A sign that someone is struggling with something or undergoing slight to severe discomfort.
        -Neck touching: A sign of doubt, insecurity, or emotional discomfort. 
        -Touching the face: A sign of nervousness, irritation, or concern. 
            -Men prefer to touch their face while women tend to play with jewelery, such as a necklace.
            -Men will also try to adjust their ties.
        -Exhaling with puffed out cheeks: A stress reliever that signals pressure. 
        -ManipuLating objects: Playing with pencils or fingers or other objects, or chewing more gum, or smoking, etc. is a sign of nervousness. 
        -VentiLating behavior: People may open the neck hole of their shirt to ventilate themselves. Women may brush their hair back to ventilate their neck. These are signs of stress and unhappiness.
        -Cleansing behavior: Rubbing the hands against the legs (specifically, the top of the thighs) soothes the brain and cleanses sweaty palms. It is a sign of stress.
-Sting Operation: A deceptive operation that pretends to be criminal in nature to lure out and catch criminals. 
-Rollin film: A 30 nanometer thick film of liquid helium. In the film, waves can propagate, but rather than being governed by gravity (as normal waves are) the restoring force on the waves is a variety of Van der Waals forces. This vibration creates a new kind of sound known as third sound.
    -Second sound: An instance of heat transfer between two bodies not in thermodynamic equlibrium in which the heat is transferred in wave-like or periodic motion. Whereas pressure in a medium creates sound waves, in the waves of second sound, heat propagates through the medium and creates the waves. 
-Capillary action: When capillary forces allow fluid to defy gravity and "creep" on the sides of, over, and out of containers, or into narrow spaces. Liquid helium and other superfluids exploit these intermolecular forces between the liquid and the solid of the container to creep out of any container holding them.
    -Capillary forces: Surface tension in liquid helium and attractive forces between the liquid helium and the solid container pull the liquid out of the container. It works best if in narrow spaces, where surface tension is highest. These are seen on lesser scales when paper towels soak up water or when trees absorb liquid.
    -A prerequisite for capillary action is that adhesive forces (between fluid's molecules and other container atoms) must overcome cohesive forces (between a fluid's molecules themselves).
-Rules of engagement: Directives that military forces must follow when in conflict. These rules vary based on circumstance of conflict, and govern the intensity, mechanism, and other specifics of battle. 
-Sodium thiopental: A common rapid-acting general anesthetic. It is a core medicine on WHO's Essential Medicines List, and is used, among other drugs, in lethal injections for anesthesia. It is often used as a truth serum, illegally, to weak the resolve of a subject and make them more complacent to interrogation. They decrease neocortical and higher order brain functioning capabilities, and because lying is more complex than telling the truth, the chemical can be effectively combined with pressure on a subject to increase chances of him or her telling the truth. 
    -Barbiturate: Drugs that depress the central nervous system. They can produce anything from mild sedation to total anesthesia. 
-Isopropyl chloride: (A.K.A. 2-chloropropane) A colorless flammable chemical compound. It is often used in domestic conflict as pseudo-chemical warfare to gas out criminals or fugitives.
    -Preparation: It is prepared by refluxing isopropyl alcohol with concentrated hydrochloric acid and zinc chloride. 
-Flash point: A property of a volatile (tendency to vaporize) substance that encompasses the lowest temperature at which it can vaporize in air to form an ignitable substance. 
    -Fire point: The temperature at which a fuel will continue to burn for at least 5 seconds following ignition by an open flame. 
    -Autoignition temperature: (A.K.A. Kindling point) The lowest temperature at which a substance will spontaneously ignite in normal atmosphere, without any other ignition source. The specific temperature is therefore sufficient to supply the activation energy needed to start the reaction. 
-Five Factor model of personality: Acronym: OCEAN, for Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism
    -Openness: An appreciation for art, emotion, adventure, unusual ideas, etc; the drive to seek out new experiences and push oneself. Correlates highly with intelligence. 
        -Factors: Large vocabulary, vivid and quick imagination, good problem solving skills, creative, quick to understand things, reflective.
    -Conscientiousness: The ability to maintain self control, discipline, and to act dutifully. The ability to regulate impulses and to plan for the future.
        -Factors: Always prepared, detail oriented, prefers order, maintains a schedule.
    -Extraversion: Positive emotions, the tendency to seek stimulation, sociability. 
        -Factors: Conversational, impulsive, very comfortable around people, desiring to be the center of attention.
    -Agreeableness: Tendency to be compassionate, cooperative, and empathetic.
        -Factors: Interested in other people, shows sympathy, easily swayed, takes time out for other people.
    -Neuroticism: Tendency to experience negative emotions, or emotional instability. 
        -Factors: Easily disturbed, moody, easily irritated, stressed out, easily upset, often sad, angry, or anxious.
-Creative visualization: Visualizing oneself doing an action to train muscle memory. 
    -A Russian study in which a group of Olympian athletes was broken down into 3 groups, one that trained 100% of their training time, one that trained 75% of the time and mentally trained the other 25%, and one that physically trained 50% of the time and mentally trained 50% of the time, showed that mentally training oneself can have significant and measurable effects on biological performance. 
-Timeline of life on earth: An outline of the history of life on earth. Abiogenesis, the creation of life in the beginning, is still an area of study and research with no definitive answer.
    -Hadean Era: 4.6 billion - 4 billion years ago
        -4.6 billion years: Clouds of rock and dust gravitationally attract each other to form an accretion disk that grows into the planet earth, orbiting a young sun. Complex organic molecules, necessary for life but not characteristic of living organisms, form in the dust grains in the protoplanetary disk around the sun.
        -4.5 billion years: Giant impact hypothesis: Moon is formed when Earth and the protoplanet Theia collide, releasing millions of tiny chunks of rock that eventually accrete to form an orbiting moon. The moon's gravity stabilities Earth's unstable orbital axis. 
    -Archean Era: 4 billion - 2.5 billion yaers ago
        -4 billion years: The Greenstone belt, zones of metamorphisized mafic (any silicate mineral with high concentrations of magnesium and iron; formed from the words "magnesium" and "ferric") and sedentary rock  in Canada. It is the oldest rock belt.
        -4.1 billion - 3.8 billion years: Period of Late Heavy Bombardment: An era of millions of asteroid and meteorite impacts on the earth and other inner planets. There is also widespread hydrothermal (heat vents) activity. 
        -3.9 billion - 2.5 billion year: The first organisms, resembling prokaryotes, appear as chemoautotrophs, consuming carbon dioxide and oxidizing inorganic materials, two exergonic processes used to release energy. Prokaryotes eventually evolve glycolysis to break down glucose and store the energy in the phosphate bonds of ATP. 
        -3.5 billion years: Bacteria and archea split; the last universal ancestor dies. Bacteria develop primitive photosynthesis (which at this point does not yet produce oxygen as a by-product).
        -3 billion years: Cyanobacteria, with the ability to photosynthesize, appear. They use water to reduce energy rich compounds, releasing oxygen as a by-product. This oxygen oxidizes dissolved iron in the oceans, creating iron ores. The rising amount of oxygen in the ocean is toxic to bacteria and begins killing them. 
            -Climate: The moon is in very close proximity to the earth, leading to 1000 foot tides. The earth is also filled with constant hurricane winds.
    -Proterozic Era: 2.5 billion - 542 million years
        -2.5 billion years: Cyanobacterial photosynthesis leads to the Great Oxygenation Event (Around 2.4 billion years ago; The spike in the amount of free oxygen in the Earth's atmosphere. Normally, by-product oxygen was captured by dissolved iron in the ocean or organic matter, but when these oxygen sinks became saturated, free oxygen accumulated in the atmosphere, wiping out most of the Earth's anaerobic life forms.) The excess oxygen also reacted with the methane in the air and reduced its concentration. This triggered Huronian glaciation (The reduced methane concentration made the atmosphere less adept at trapping heat), leading to a long Snowball Earth Period (any period in which the entirety of the Earth's surface, including oceans and landmasses, is completely frozen).
        -2 billion years: Acritarchs (small, non-acid soluble, organic life forms) expand and diversify. 
        -1.85 billion years: Eukaryotes appear. 
        -1.4 billion years: Stromatolites (laminated (two or more materials fused together), organic, mineral-rich microorganisms) expand.
        -1.2 billion years: Sexual reproduction appears.
        -1.1 billion years: First dinoflagellates (single-celled organisms with two flagella. They produce toxins as a defense mechanism and usually live in saltwater, though some have been found in freshwater.) appear.
        -1 billion years: First vaucherian algae (a genus of yellow-green algae.) appear.
        -750 million years: First protozoa appear.
        -850 million - 630 million years: Global glaciation occurs.
        -600 million years: Ozone layer forms from accumulated oxygen. 
        -580 million - 542 million years: The first large, complex, multicellular organisms appear as Ediacarian biota.
        -580 million - 500 million years: Modern phyla of animals begin to appear.
        -560 million years: First fungi appear.
    -Phanerozoic Eon: A long period of abundant, shell-forming organisms. The period is divided into 3 eras that are further subdivided by major mass extinctions.
        -Paleozoic Era: 542 million years - 251 million years ago.
            -535 million years: Ocean life forms diversify into chordates, arthropods, echinoderms, mollusks, foraminifers, radiolarians, etc.
            -530 million years: First known footprints on land. 
            -485 million years: First boned vertebrates appear. 
            -434 million years: First primitive plants move onto land. They evolved from algae. Fungi also move onto land.
            -420 million years: First ray-finned fishes, trigonotarbid arachnids, and land scorpions appear.
            -410 million years: First fish with teeth appear.
            -395 million years: First lichen and stonewarts appear. 
            -363 million years: Insects begin to populate the planet and soon evolve wings as well. Sharks are the apex predators of the oceans.
            -360 million years: First crabs and ferns appear.
            -340 million years: Amphibians diversify for the first time.
            -251.4 million years: Permian-Triassic extinction event (See above). Land life takes 30 million years to recover.
        -Mesozoic Era: 252 million years - 66 million years ago.
            -250 million years: Mesozoic Marine Revolution: predators begin to dominate the oceans and the balance of power shifts to them. Marine predators become increasingly well adapted.
            -225 million years: Earliest dinosaurs, first teleost fishes, and first mammals appear.
            -200 million years: First viruses appear. 
            -170 million years: First salamanders, newts, cryptoclidid and elasmosaurid plesiosaurs appear. 
            -165 million years: First rays appear. 
            -155 million years: First blood sucking insects appear. 
            -130 million years: Angiosperms (flowering plants) begin to rise and dominate. 
            -100 million years: Earliest bees appear.
            -90 million years: Ichthyosaurs go extinct, first snakes and ticks appear. Angiosperms diversify.
            -80 million years: First ants appear.
        -Cenozoic Era: 66 million years ago - present.
            -66 million years: Cretaceous-Paleogene extinction event (See above). About half of animal species die and all dinosaurs (except the early forms of birds). Conifers and ginkgos dominate quickly. Mammals become the dominant species. Ants diversify.
            -63 million years: Creodonts (a group of carniverous mammals and the dominant predatory group in Africa during their time period. They are the ancestors of modern carnivores and had stocky, dog-like bodies with long, low skulls.) evolve.
            -60 million years: Flightless birds evolve, earliest primates appear.
            -52 million years: First bats appear.
            -40 million years: First modern butterflies and moths appear. Giant whales go extinct.
            -35 million years: Angiosperms diverge into the evolving grasses. Mammals diversify, leading to the appearance of the first dogs, hawks, eagles, ground sloths, peccaries, and glyptodonts.
            -30 million years: Earliest pigs and cats appear. 
            -25 million years: Earliest deer.
            -20 million years: First giraffes, bears, and giant anteaters. Birds diversify.
            -6.5 million years: First hominin appear.
                -Hominin: A group consisting of modern humans as well as all extinct human species and immediate ancestors.
                -Hominid: A group consisting of all modern and extinct Great Apes, including modern humans.
            -5 million years: First tree sloths and hippopotamuses. Grazing herbivores diversify, leading to zebras and elephants. Large carnivores such as lions and dogs appear. 
            -4 million years: First modern elephants, giraffes, zebras, lions, rhinos, and gazelles.
            -3 million years: Great American Interchange: Many land and freshwater fauna migrate from North America to South America and vice versa, via Central America.
            -2.7 million years: Evolution of paranthropus (early bipedal hominins with strong jaw muscles, grinding herbivore-like teeth, and broad and thick skulls.
            -2 million years: Earliest species of the genus Homo evolve.
            -1.7 million years: Australopithecines go extinct.
            -1.2 million years: Paranthropus goes extinct, Homo antecessor (A group of extinct early human ancestors, about 5.5-6 feet tall and 200 pounds. Brain size of 1000 cc, compared to modern size of 1350 cc.) evolves.
            -600 thousand years: Homo heidelbergensis (An extinct species of the genus Homo. They had large brains (1100 - 1400 cc) and had advanced tools and behavior.) evolves.
            -350 thousand years: Neanderthals appear.
            -200 thousand years: Anatomically modern humans (humans with the same phenotypes as those today) appear in Africa.
            -30 thousand years: Neanderthals go extinct, dogs are first domesticated.
            -15 thousand years: Wooly rhinos go extinct.
            -10 thousand years: Holocene Epoch (a warm period after the ice age) begins and still continues into the present.
                -Last Glacial Maximum: (26.5 thousand years - 19 thousand years ago) Ice sheets reach maximum extension.  
                -Late Glacial Maximum: (13 thousand years - 10 thousand years ago) Warming of climates
-Laplace Transform: L{f(t}) -> F(s) where L{f(t)} = integral from 0 to infinity of ( (e^(-st))(t) dt) 
    -Intuition: Some prefer to view the Laplace transform as a continuous analog to the power series representation of a function. A power series representation is f(x) = sum from n = 1 to infinity of a(n)*x^n. Notice that the coefficients a(n) are discrete, since n can only by a natural number. The key difference in this analogy is that typically we have a function f(x) and we WANT to find a function a(n) to produce a power series representation. In analogy, the Laplace Transform BEGINS with a function a(n) and searches for a function f(x). Instead of using the discrete variable n, we use the continuous variable t, which is allowed to take upon every positive real number, and instead of summing, we integrate. Instead of a(n), we use f(t), and so we have integral from 0 to infinity of f(t)*x^t dx = integral from 0 to infinity of f(t) * e^(t * ln(x)) dx = integral from 0 to infinity of f(t) * e^(-st) dx, where we define -s = 
    ln(x) (the negative sign is necessary to ensure that s is a positive number, since ln(x) is always negative, since x is always between 0 and 1 because otherwise the integral diverges). This is the Laplace transform.
    -Inverse Laplace transform: The inverse Laplace transform is known as Mellin's inverse formula (after Hjalmar Mellin). It is: f(t) = 1/(2*pi*i) * (limit as T approaches infinity of (integral from (gamma - i*t) to (gamma + i*t) of (e^(s*t) * F(s) ds)))
    -Common Laplace transforms: L(Dirac-Delta function) = 1, L(1) = 1/s, L(t) = 1/s^2, ..., L(t^n) = s^(-n -  1) * Gamma(n + 1), L(e^(-a*t)) = 1/(s + a), 
    L(t*e^(-at)) = 1/(s + a)^2, L(sin(omega*t)) = omega/(s^2 + (omega)^2), L(cos(omega*t)) = s/(s^2 + (omega)^2), 
    L(e^(-a*t)*sin(omega*t)) = omega/((s + a)^2 + (omega)^2), L(e^(-a*t)*cos(omega*t)) = (s + a)/((s + a)^2 + (omega)^2)
-Fourier Series: If a function is periodic, meaning f(t) = f(t + P) for some period P, then it can be expressed using Fourier series as f(t) = ( (a_0)/2 )( sum from (n = 1) to infinity of ( (a_n)(cos( (n*pi*t)/L ) ) ) ) + ( sum from (n = 1) to infinity of ( (b_n)(sin( (n*pi*t)/L ) ) ) )
                                                                         = (a_0)/2 + (a_1)cos( (pi*t)/L ) + (a_2)cos( (2*pi*t)/L ) + (a_3)cos( (3*pi*t)/L ) + ... + (b_1)sin( (pi*t)/L ) + (b_2)sin( (2*pi*t)/L ) + ...
    -Fourier coefficients: For f(t) with period P = 2L, on the interval [-L, L]: a_0 = (1/L)(integral from -L to L of f(t) ), a_n = (1/L)(integral from -L to L of ( f(t) * (cos( (n*pi*t)/L ) ) ), b_n = (1/L)(integral from -L to L of ( f(t) * sin( (n*pi*t)/L ) )
    -Dirichlet conditions: A periodic function f(t) with period P = 2L can be expressed as a Fourier series if:
        1. The number of discontinuities is finite.
        2. The average value over the period is finite.
        3. The number of maxima and minima is finite.                                                                           
    -Explanation: Technically, any function can be represented using a Fourier series, the same way any function can be represented with a Taylor series (though of course, problems of convergence arise). Fourier series are akin to Taylor series; they simply use trigonometric or sinusoidal functions as their basis functions, instead of polynomials. Consider the Taylor series have certain coefficients appended to infinite polynomials, ie an infinite summation in which the base function (polynomials) have increasing exponents, with each term multiplied by some coefficient that depends on the function. In a Fourier series, we instead seek an infinite series of with sine and cosine functions as our base functions, and instead of increasing exponents, we use increasing frequencies (ie sin(t) + sin(2t) + sin(3t) + ... + cos(t) + cos(2t) + cos(3t) + ...), with each term multiplied by some coefficient. Thus, we define the Fourier series of a function f(x) as:
        f(x) = c_0 + sum from n = 1 to infinity of ( (a_n)(cos( (omega)_n * x )) + (b_n)(sin( (omega)_n * x )) ), where (omega)_n = frequency of n-th sinusoid = n*(omega)_0, and (omega)_0 = initial frequency = (2*pi)/T, T = period. Thus, we have now created the formulation for what our function's approximation should look like. However, we still need to find the coefficients a_n and b_n that actually make the sequence converge to the function f. 
        -Deriving coefficients: Now that we have a framework for what we want the series representation to look like, we only need to derive the exact forms of what the coefficients should look like. However, before we do this, we must discuss the orthogonality.
            -Orthogonality: Two functions f and g are orthogonal over an interval [a, b] if: integral from a to b of f(x)*g(x) dx = 0. A set of functions is orthogonal if every function in the set is orthogonal to every other function in the set (though not necessarily to itself, of course). We will need to discuss the orthogonality of the set of sinusoidal functions with increasing frequencies over a symmetric interval [-L, L]. 
                -Conventions: For brevity's sake, we will follow the convention that "integral from a to b of f(x) dx" is written "int[a, b] f(x)".
                -Orthogonality of the set {cos((omega)_n * x)} from n = 0 to n = infinity, where (omega)_n = frequency = (pi*x)/L. To show that this set is orthogonal, we must show that int[-L, L] cos((n*pi*x)/L)*cos((m*pi*x)/L) = 0. First, notice that cosine is an even function, and the product of an even function is also even, so the integral reduces to: 2 * int[0, L] cos((n*pi*x)/L)*cos((m*pi*x)/L). To continue, we now examine two cases: (1) n = m, and (2) n != m. We will show that cosine is not orthogonal to itself, but the set of cosine functions with increasing frequencies is orthogonal.
                    -n = m case: Again, we examine two cases: (1) n = m = 0, and (2) n = m != 0.
                        -n = m = 0 case: In this case, the integral simplifies to: 2 * int[0, L] (1 * 1) = 2L.
                        -n = m != 0 case: Substituting m = n, the integral becomes: 2 * int[0, L] cos^2 ((n*pi*x)/L) = int[0, L] (1 + cos(2*(n*pi*x)/L)) = (x + L/(2*n*pi) * sin(2*(n*pi*x)/L)) for (x = 0 to x = L) = L + L/(2*n*pi) * sin(2*n*pi) = L since the sine function is zero for integer multiplies of pi. 
                    -n != m case: By product-sum identities, the integral becomes: int[0, L] ( cos(((n - m)*pi*x)/L) + cos(((n + m)*pi*x)/L) ) = L/((n - m)*pi) * sin(((n - m)*pi*x)/L) + L/((n + m)*pi) * sin(((n + m)*pi*x)/L) for (x = 0 to x = L) = 0 since the sine function is 0 for integer multiples of pi.
                -Orthogonality of the set {sin((omega)_n * x)} from n = 0 to n = infinity, where (omega)_n = frequency = (pi*x)/L. Again, consider the cases (1) n = m, and (2) n != m.
                    -n = m case: The integral reduces to: 2 * int[0, L] sin^2 ((n*pi*x)/L) = int[0, L] (1 - cos(2*(n*pi*x)/L) = L - L/(2*n*pi) * sin(2*n*pi) = L.
                    -n != m case: Again using product-sum identity, the integral reduces to: int[0, L]  ( cos(((n - m)*pi*x)/L) - cos(((n + m)*pi*x)/L) ) = (L/((n - m)*pi) * sin((n - m)*pi) - L/((n + m)*pi) * sin((n + m)*pi) = 0.
            -Deriving c_0: To derive c_0, integrate both sides of the definition over one period, ie over [x_0, x_0 + T]: int[x_0, x_0 + T] f(x)  = int[x_0, x_0 + T] c_0 + int[x_0, x_0 + T] (a_1)(cos((omega)_1 * x) + int[x_0, x_0 + T] (a_2)(cos((omega)_2 * x) + ... + int[x_0, x_0 + T] (a_n)(cos((omega)_n * x) + ... + int[x_0, x_0 + T] (b_1)(sin((omega)_1 * x) + int[x_0, x_0 + T] (b_2)(sin((omega)_2 * x) + ... + int[x_0, x_0 + T] (b_n)(sin((omega)_n * x) + ... = (c_0)(int[x_0, x_0 + T]) + 0 + 0 + ... = (c_0)(int[x_0, x_0 + T]) (since cosine and sine are zero over one period) = (c_0)(T).
                       Therefore, c_0 = (1/T)(int[x_0, x_0 + T] f(x) dx) 
            -Deriving a_n: Multiply both sides of the definition by cos((omega)_n * x) and integrate over one period: int[x_0, x_0 + T] f(x)*cos((omega)_n * x) = int[x_0, x_0 + T] (c_0)*cos((omega)_n * x) + int[x_0, x_0 + T] (a_1)*cos((omega)_1 * x)*cos((omega)_n * x) + int[x_0, x_0 + T] (a_2)*cos((omega)_2 * x)*cos((omega)_n * x) + ... + int[x_0, x_0 + T] (a_n)*cos^2 ((omega)_n * x) + ... + int[x_0, x_0 + T] (b_1)*sin((omega)_1 * x)*cos((omega)_n * x) + int[x_0, x_0 + T] (b_2)*sin((omega)_2 * x)*cos((omega)_n * x) + ... + int[x_0, x_0 + T] (b_n)*sin((omega)_n * x)*cos((omega)_n * x) + ... = (b_n)(T/2) (by orthogonality relationships).
                       Therefore, a_n = (2/T)*(int[x_0, x_0 + T] f(x) * cos((omega)_n * x).
            -Deriving b_n: Multiply both sides of the definition by sin((omega)_n * x) and integrate over one period: int[x_0, x_0 + T] f(x) * sin((omega)_n * x) = int[x_0, x_0 + T] f(x) * sin((omega)_n * x) + int[x_0, x_0 + T] (a_1)*(cos((omega)_1 * x) * sin((omega)_n * x) + int[x_0, x_0 + T] (a_2)*(cos((omega)_2 * x) * sin((omega)_n * x) + ... + int[x_0, x_0 + T] (a_n)*(cos((omega)_n * x) * sin((omega)_n * x) + ... + int[x_0, x_0 + T] (b_1)*(sin((omega)_1 * x) * sin((omega)_n * x) + int[x_0, x_0 + T] (b_2)*(sin((omega)_2 * x) * sin((omega)_n * x) + ... + int[x_0, x_0 + T] (b_n)*sin^2 ((omega)_n * x + ... = (b_n)*(T/2).
                       Therefore, b_n = (2/T)*int[x_0, x_0 + T] f(x) * sin((omega)_n * x).
-Elephant's Foot: A large mass of corium (melted down-lava like mixture of nuclear fuel, fission products, and control rods formed in the aftermath of a severe nuclear meltdown) were formed in the Chernobyl accident. Temperatures reached 2600 degrees Celsius. It was so hot that if you looked at it directly, you would be killed instantly; even a camera burned up when trying to get a picture. Existing photographs were taken using complex systems of mirrors.
-Angel Problem: A problem created by Joseph Conway. It involves an angel and a devil. The game has variations depending on the angel's power, which is a natural number k (k > 1 and is an integer). The game is played on an infinite chess board, and an angel with power k can move k squared in each direction (such as a queen in chess). The devil can block squared the angel is not on (one per turn), but the angel can jump over them, though not onto them. The devil wins if it can trap the angel such that it can never escape, and the angel wins if it can survive independently.
    -Bowditch proved that a 4-angel can win, and Mathe and Kloster proved that a 2-angel can win.
-Benford's Law: In real life data of all sorts (e.g. taxes, Gallup polls, street addresses, electricity bills, etc.), the digits 1-9 do not appear uniformly, but rather follow a logarithmic distribution:
    -Distribution: 1 - 30.1%, 2 - 17.6%, 3 - 12.5%, 4 - 9.7%, 5 - 7.9%, 6 - 6.7%, 7 - 5.8%, 8 - 5.1%, 9 - 4.6%
    -It is also observed in the set of Fibonacci numbers, factorials, and powers of 2.
-Birthday paradox: With only 23 people in a room, it is statistically probable (50.7%) that at least 2 people have the same birthday. At 57 people, the probability is 99%, and at 366, it is 100%.
    -Proof: Let's generalize the problem above. Let {r_1, ..., r_n} be independently distributed integers that are all members of the set [1, B]. We assume that {r_i} is uniformly distributed; for the purposes of this proof, this will not matter, as a uniform distribution is the worst case scenario anyways and so proving the paradox for this case proves it for all cases. We wish to find n such that the probability of a collision {r_i = r_j but i != j for i, j as integers in [1, n]} is greater than 0.5. For the specific case of birthdays, B = 365 and we will show that n = 23.
        1. To find the probability of a collision, we look at the complement: P(collision) = 1 - P(no collision at all). 
        2. P(no collision at all) = (1)*((B - 1)/B)*((B - 2)/B)*((B - 3)/B)*...*((B - n + 1)/B) = product for i in [1, n - 1] of (1 - i/B).
            -Mathematical details: Each element in {r_i} is independently selected, so the probability of no collision at all is equal to the product of every probability of each r_i not colliding with every other r_j. To find this product we use the following reasoning. First, consider r_1. This is the first element we have so it obviously cannot collide with anything. Thus P(no collision) = 1 up until this point. Now consider r_2. This element can occupy any possible value in the output space (which is, as stated above, [1, B] in integers) EXCEPT the value occupied by r_1; thus, r_2 can occupy any of (B - 1) states, and the probability of it being in any of these states is (B - 1)/B. Consider r_3: this element can also occupy any value in [1, B] except those occupied by r_1 and r_2; thus it has (B - 3) options, and the probability of it being in one of those options is (B - 3)/B. In general, for r_i, the possible values for r_i to have without colliding with any of the previous elements {r_1, ..., r_(i - 1)} 
                           leave r_i (B - (i - 1)) = (B - i + 1) possible choices with a probability of occupying any of those acceptable positions of (B - i + 1)/B.
        3. Therefore, P(collision) = 1 - P(no collision) = 1 - (product for i in [1, n - 1] of (1 - i/b)). Now consider the inequality (1 - x) <= e^(-x).  Plugging in the product we obtained for P(no collision), we find that (1 - (product for i in [1, n - 1] of (1 - i/b))) <= (1 - (product for i in [1, n - 1] of e^(-i/B))) = (1 - e^((-1/B)(sum for i in [1, n - 1] of i))) = (1 - e^((-1/B)((n - 1)(n - 2)/2))) (from Gauss' formula that sum for i in [1, m] of i = ((m)(m - 1))/2). Since (n - 1)(n - 2) is bounded above by n^2 and the exponential has a negative exponent (and therefore bigger exponents yield smaller values), our final inequality is: P(collision) = (1 - (product for i in [1, n - 1] of (1 - i/B))) >= (1 - e^(-n^2/(2B))).
            -Proof of inequality: It is clear that (1 - x) <= e^(-x) because of the Taylor expansion of e^(-x) about x = 0: e^(-x) = 1 - x + x^2/2 - x^3/6 + ... = (1 - x) + (some positive error bound). Therefore, (1 - x) <= e^(-x).
        4. Since P(collision) is bounded by (1 - e^(-n^2/(2B))), if we want the probability of collision to be at least 0.5, we merely set (1 - e^(-n^2/(2B))) = 0.5 and solve for n, which produces n = (sqrt(ln(4))) * B^0.5 which is approximately 1.177*sqrt(B).
            -Notice that for B = 365, n = (sqrt(ln(4)))*sqrt(365) = 22.49438... = 23 when rounded up.
    -Birthday attack:  
        -Algorithm: Suppose H is a hash function that converts some message m to a series of bits, ie H: M -> {0, 1}^n (notice that M is the message space and that {0, 1}^n is a sequence of binary digits n units long). The nature of a hash function is to compress messages to smaller numeric addresses, so we assume that |M| = size of M >> 2^n (which is the size of the base-2 number to which M is converted). The following algorithm will find a collision in the Hash function, and it will have time complexity of O(2^(n/2)). The algorithm:
            1. Choose 2^(n/2) random messages in M, the message space: m_1, m_2, ..., m_(2^(n/2)). These messages are not guaranteed to be distinct (ie two or more might be the exact same), but it it extremely probable.
            2. For each of the above messages, compute t = H(m) to obtain t1, ..., t_(2^(n/2)).
            3. Look in the sequence {t_1, ..., t_(2^(n/2))} for a collision, ie for i != j, t_i = t_j, where both i and j are integers in the set [1, 2^(n/2)].
            4. If no collision is found, go back to step 1.
        -Explanation: Although the algorithm may seem rather brute force inelegant, the mathematics behind the birthday paradox (upon which the attack is based) actually guarantee a pretty high probability of success and make this method fairly efficient. This is because we are not searching for a collision between a given message and any other message (this has very low probability); rather we are searching for a collision between any message at all and any other message at all (this has considerably higher probability). This is analogous to the birthday paradox: we are not searching for someone with the same birthday as some specific person but rather any two people at all with the same birthday, and this distinction drastically improves the probability
        -Hash function: A hash function is a mapping from any kind of digital file to a specific number that is used as an address for the file. Hash functions are commonly used to map various strings to small, condensed numeric values. This is to increase efficiency in searching through and otherwise manipuLating a database of information (in this case, strings), since it is easier to search for a small number, especially amidst a database of numbers, than it is to search for a string among a database of strings.
            -Structure: The hash function will manipulate the file's information content in some way to derive a number from it.
                -Examples: Some trivial and simple (and therefore weak and not recommended) methods are (1) Division-remainder method (the number of items in the table is divided by the original value to obtain a remainder, which is the hashed value), (2) folding method (The original value, if it is a number, is divided into partitions, and each partition is added up; the last several digits are then used as the hash value (the rest are truncated)), (3) Radix transformation method (simply converting the inputted number to a different number base), and (4) Digit rearrangement method (taking some subsequence in the original and reversing it; this is the hash value).
            -Collisions: The actual rule that defines the correspondence between the file and numbers can be anything, but generally, mapping large files containing a lot of information to small numeric values will not be a one-to-one mapping, since the rules must be general enough to allow for such information compression, but this level of generality sacrifices the guarantee that each numeric value is matched to only one file. When two different file inputs are mapped by a hash function to the same numeric address, we say a collision has occurred.
-Estimating pi: Inscribe a circle with radius 1 in a square with side length 2. Randomly select points in the square, and divide the number of points in the circle and square by the total points chosen. Then multiply by 4 to approximate pi.
    -Area of circle = pi(1)^2 = 1, area of square = 2^2 = 4. Probability of selecting a point in circle and square = area_(circle)/total area = area_(circle)/area_(square) = pi/4. pi/4(4) = pi.
-Neutron star collisions form black holes and release enormous amounts of energy in the process; these energies are responsible for all gold and heavier elements in the universe. Supernovae can only fuse elements up to iron.
-Astrophysics procedure for estimating age of star clusters:
    1. Use a telescope to obtain images, exposed for a certain amount of time, in at least two filters (usually visual (another name for green) and blue).
    2. Obtain dark images (images with the lens cap on; these will be subtracted from the real images to compensate for instrumental noise) of the same exposure time as the image.
    3. Obtain flat images (images with light shone on them) at any exposure level. Obtain another set of dark images whose exposure time matches that of the flats.
    4. Combine all the darks into one master dark. Combine all the flats into one master flat. Combine all the darks in the second set of darks into one master dark as well.
    5. Subtract the master dark that has the same exposure time as the flats from the master flat to create an updated master flat.
    6. Use CCDSoft to subtract a master dark from each of the raw data file (raw data and darks should have same exposure time) and divide the result by the master flat.
    7. Split up the raw data into folders, by filter (in this case a Blue folder and a Visual filter).
    8. Align the images in each folder.
    9. Combine the images in each folder. 
    10. Open the master images for each filter in Astrometrica. Make sure that in the settings the appropriate focal length is specified and that the appropriate filter that was used is specified.
    11. Data reduce each image. Open up the Log File by going to file -> view file log.
    12. Take the table of values detailing the declination, right ascension, instrumental magnitude, and instrumental flux for each image and copy-paste onto a text file.
    13. Write a program (usually using Python) to sort the data by coordinate. Not all the data in the log file applies to the cluster, so only data from all filter's master images that share coordinates will be used, as those images are in the star cluster itself. The program should sort that data.
    14. In Python, import the text file, define a variable for the difference in the filters (in this case B-V) and graph it on a scatterplot against the lower intensity filter (in this case V). This is the Hertzsprung-Russell diagram, with temperature on the horizontal axis and luminosity on the vertical axis.
    15. Identify the main sequence turnoff point in the H-R diagram by looking for the sharp turn in temperature but no change or an increase in luminosity (this marks stars going ending the main sequence and going into giant mode, where the temperature decreases but the luminosity does not).
    16. Use the main sequence turnoff point's stars' temperatures and luminosities to identify what kind of star it is, and the time at which that star ends its main sequence is the age of the star cluster. 
-Curvature: The amount by which a geometric object deviates from being flat or straight. Curvature of a circle = 1/r, and the curvature of a curve at a point is the curvature of the oscuLating circle (a circle tangent to the tangent line at the point whose center passes through the line normal to the tangent line at the point).
-Cauchy sequence: A sequence whose elements become arbitrarily close to each other as the sequence progresses; e.g. {1,2,3,3.5,3.7,3.9,3.99,...} which converges to 4.
-Imagining the 10th Dimension: The fourth dimension of time can be represented as a long unduLating string in 4 dimensions, with all version of the past, present, and future on it (e.g. at the beginning is your birth, and at the end is your death), but the reason that time is perceived as moving continuously is that the string passes through the 3 spatial dimensions, and so only the 3-dimensional cross section of the 4-dimensional worldline (i.e. the present) is seen, and is constantly changing as the worldline (i.e. the string) moves through the dimensions.
                   The worldline of spacetime that exists in the 4th dimension is really curved in the 5th dimension, but time is perceived as moving linearly (i.e. from time t = a  to t = b smoothly) because in the 5th dimension, it is curved in such a way that the surface is non-orientable, analogous to how a Mobius strip is a representation of a 2-dimensional object (since a line drawn will always fill both sides and end up where it started, the strip only has 1 side and is therefore 2-dimensional) in 3 dimensions
-How batteries work: Place a material whose atoms have electrons in high energy levels parallel and nearby to a material whose atoms have electrons in a low energy level. The electrons from the high energy level material tend to flow across their gradient into the low energy level material simply through space, but if an electron barrier is placed between the materials and a circuit connects the materials, the electrons have no choice but to travel through the circuit, where the electricity can be harnessed. All the atoms in the high energy level material are now ionized, and flow through space (the electron barrier only blocks electrons) into the low energy level material, but they cannot rejoin their old electrons because these electrons are
             in a low energy environment and can't go to a higher energy environment spontaneously. When the low energy material is saturated with positive ions from the high energy level material and with electrons, the battery is discharged and the circuit has no current; to recharge the battery, energy must be put in to push the electrons and positive ions back to the high energy material.
-Turing test: A test designed to gauge if a machine possesses artificial intelligence, i.e. it has intelligence equal to or superior to that of a human. The test does not measure correct answers, but rather measures how much the answers suggest true intelligence or resemble a human's answers. 
-Pressure points: Pressure points are areas where nerves end, criss cross with another nerve, or branch off to form a "Y" shape. They hare depressions, holes, cavities, or joints on the body where even relatively weak amounts of pressure can incapacitate the individual, allowing smaller fighters to take down opponents much larger, faster, or stronger than themselves. Pressure points are all linked, and so applying pressure to one pressure point automatically makes the next one weaker. In order to effectively weaken a pressure point, pressure must be applied on 1 of 3 ways, unique to the point in question: striking, rubbing, or bending. When activating pressure points, visualize your motion as going THROUGH the arm, not at it, as this is more effective. So, for example, 
          if you are pressing a point, visualize yourself pressing THROUGH the arm; this will cause you to press harder and more precisely. When rubbing a point, rub THROUGH the point in a downward fashion.
    -Arm pressure points: These are typically used if your opponent grabs you somewhere. Full video demonstration: http://www.youtube.com/watch?v=o3TMTmVFMUw
        -Below the elbow (forearm, wrist, hand, etc.)
            -Lung 8: A small depression about an inch above the wrist where one's pulse might be taken. Rub the depression with the hard part of your thumb, going down your opponent's arm (towards your own body); this works best if your opponent has grabbed your wrist. Keep pushing downwards, aiming (mentally) to bring their arm several inches lower than it currently is, causing their entire body to arch. The harder they grab your arm, the tighter the radial nerve gets, and the more effective this technique.
            -Heart 6: A point opposite Lung 8; it is about a half inch above the wrist joint on the side of the pinky (the bottom of the wrist). When your opponent has made a tight fist, pressing into this point (usually upward if your opponent is in punching position, since then the point will be on the bottom of the wrist) causes the wrist to sharply bend, setting your opponent up for a wrist lock.
                -Eagle claw grip: Grip an opponent's wrist with all the fingers bent at the first joint (the one closest to the knuckles); this aligns the thumb knuckle with lung 6 and the index, middle, and ring fingers with heart 6; squeezing and pulling the wrist (specifically, pull your opponent's pinky finger side of the hand closest to you first) towards you, gives you control.
            -Lung 6: Directly in the middle of the wrist and elbow where the upper forearm muscle drops down, towards the inside of the bone gap between the radius and the ulna. This point is meant to be struck rapidly and forcefully, usually with the knuckle of a fist (though the forearm also works, to a lesser degree), and will cause the arm to fly away. 
            -Anti grappling points
                -Lung 5: A depression that lies on the inside of the radial bone on the crease of the elbow, towards the outward side of the arm and a bit below where the elbow bends. This point can be pressed or struck to bring your opponent's arms, and subsequently, his body, down. Generally thumb pressure is used. Always strike towards yourself, rather than towards your opponent; this is more effective and brings your opponent closer to you in his vulnerable position.
                -Large Intestine 10, 11: Two points located slightly on the back side of the elbow, towards the top, right above the bony joint. Strike this point in, towards the bone, to turn your opponent away from you. 
            -Pericardium 6: A depression between the two tendons in the wrist (palmaris longus tendon and flexor carpi radialis tendon), about 1.5 inches above the wrist bone. Sharply pressing this point causes the body to fold. The technique is most effective when combined with the pressing of the Large Intestine 10 and 11 points.  
        -Above the elbow (bicep, tricep, etc.) These are typically used when your opponent grasps you on the upper side of your torso, or even in a choke.
            -Heart 2: A slight depression located right in between the bicep and tricep muscles, though closer to where the muscles meet than to the exact middle of the two, on the inner side of the arm (i.e. the side that faces the body in resting position). Press this point inwards and downwards simultaneously. The point, if struck, numbs the arm, and if pressed, brings your opponent down and causes his knees to buckle.
            -Large Intestine 12: A hollow point that lies between the bicep and the forearm, on the outer side of the body (i.e. the side that points away from the body in resting position). It is located a few inches above the elbow crease. This point can be pressed with the thumb, but can also be grabbed with the fingers, from the inside, and pulled, to bend the elbow and give you control. To grab, plant the palm of your hand slightly above his elbow joint, and allow your fingers to loop over; then pull, ensuring that your fingertips dig into the pressure point. This point, along with Heart 6, can be used even when your opponent is grabbing you with both arms.
            -On the back of the arm, there are 2 additional points that can be used if you have your opponent in a wrist grab and are facing the back of his arm. 
                -First point: The first point is located right in the tiny crease in the bony elbow joint, and is rubbed to bring your opponent down. The rubbing can be done with the thumb, but it is more effective to rub by pressing the hard part of the ulna bone into the point and rotating your forearm, rubbing the hard bone into the point. This tricks the body into thinking that the elbow is broken, causing all the muscles in that part of the hand to relax (to prevent further dislocation or damage).
                -Second point: The second point is located right in the middle of the tricep, and is struck to bring your opponent down. Because this point is fairly large and can be struck with brute force, it is preferred over the second one when there is not enough time to locate the first one and rub appropriately, such as when countering a punch.
        -Lower forearm disarm point: If your opponent is holding a weapon, such a knife, stick, or gun, use this point to cause a release of the weapon. This point is most effective against an armed opponent because other techniques can cause the weapon to fall towards you, come at you, or go off (if the weapon is a gun), but this technique always forces a release towards the opponent or the ground.
            -Pressure point: The point is on the forearm, directly between the elbow and the wrist. It is roughly in the middle of the forearm width-wise, though slightly leaning towards the inside of the forearm (the line that connects with the little finger when the palm is up). Grab your opponent's wrist and strike this point forcefully with the hard ulna bone of your forearm or your fist. When you have grabbed the wrist, turn the forearm 90 degrees from its palms-up position so that the inside is not facing you; strike the point by coming up from underneath your opponent's arm and hitting the point. Use your striking arm as a lever the instant it makes contact and pull the wrist (which you had grabbed) downwards, causing the elbow joint to fly upwards and the hand to point downwards, until the forearm is vertical and you are behind your opponent. The weapon will fall to the ground.
                -Note that striking this point automatically bends and weakens the wrist. When the wrist bends, the hand opens, and so even if your opponent has a gun, he cannot pull the trigger (regardless of where the gun is pointed).
    -Leg pressure points: 
        -Spleen 11: Halfway between the hip and knee, and a half rotation from the top of this part of the left clockwise (so you are on the inside of the leg, i.e. the part facing the other leg), is a small depression. This can be struck with the knee, the ball of your foot, your ankle, or even squeezed with fingers if possible. This point is typically struck when you are lined up with your opponent face to face, especially if one foot is in front of another (i.e. a fighting stance).
        -Gallbladder 31: Halfway between the hip and knee, and a half rotation from the top of this part of the left counterclockwise (so you are on the outside of the leg, i.e. the part pointing away from your body), is a small depression; if you stand up straight with your hands at your sides, your middle finger will approximately be on top of this point. These muscles on the outside of the leg are much denser than those on the inside, so most kicks will not have much effect. More direct hits such as knees or side kicks will work much better. This point is typically struck when you are on the outside of his body or facing your opponent at an angle, such as a follow up after an arm pressure point that brings your opponent in front of you. For the next 10 minutes, this will numb and impair the injured leg.
        -Bladder 56, 57, 58: These 3 points are lined up on the back of the calf muscle. The first is an inch underneath the knee crease, the second is a few inches down directly centered between the knee and the end of the calf muscle (where the top of the ankle begins), and the third is half an inch above the end of the calf muscle. Thumb pressure on the top point (by the knee crease) can bring a man down. If you manage to get behind your opponent, pushing sharply down on this point with the heel of your foot will buckle the knee of that leg and bring your opponent down.
        -Another point is located right between the beginning of the little toe and the fourth toe down (big toe first); this slight depression, when stomped on, brings a man down. If an opponent has you from behind, a good way to begin your escape is by stomping on this point. Additionally, when stepping in after blocking a punch, stepping on this point hard will make your counter much more effective. 
        -Another point is located on the inside of the leg (facing the other leg) halfway between the ankle bone and the lower end of the calf muscle. Use the foot diagonal, not lined up with, the your opponent's target leg, and make a scooping motion upwards as you strike this point with your heel. 
-Cryogenic chamber therapy: A patient is put in a cryogenic chamber for no more than 3 minutes; the chamber is cooled to -120 degrees C, though -140 or even -160 have been used. The cooling is typically done using liquid nitrogen, which cools the tank but does not touch the patient at all. The patient wears a bathing suit, as well as socks, gloves, and earmuffs to protect against acute frostbite. Core body temperature does not drop. The process releases endorphins, causing pain relief, and when the blood starts flowing again after therapy, a feeling of rejuvenation has been experienced. 
-Recovery position: A position in which an unconscious but breathing subject should be placed to avoid suffocation or hypoxia from a closing or obstruction of the airway. Forms of obstruction are mechanical obstruction (physical/solid objects block the airway; in most cases it is the subject's own tongue, which falls backwards towards the throat as the muscles relax in the unconscious state) and fluid obstruction (Fluids block the airway; it is usually vomit, drowning the person).
    -The position: 1. Extend the subject's arm closest to you away from the body at a right angle. Bend the other arm at the elbow and place the back of their hand against their cheek.
               2. Bend the subject's far knee upwards, bending the leg.
               3. Roll the person towards you by pulling the far knee (which is bent and pointing upwards) towards you and towards the ground, over the near leg.
               4. Tilt the head up slightly, making sure that the airway is open. Unless the person has a fever or heat-related illness, put a blanket or coat over them.
-i.e.: An abbreviation for the Latin "id est", which means "that is" or "in other words".
 e.g.: An abbreviation for the Latin "exempli gratia", which means "for example".
-Tracing a phone call: Traces can be completed in seconds, and the FBI can remotely turn cell phones on and even use them as microphones to spy on people.
-Miranda rights: A warning given to suspects in custody of the police before the suspect is interrogated; even if someone is arrested, they need only be read their Miranda rights if they will be interrogated. 
    -The warning: "You have the right to remain silent when questioned. Anything you say can and will be used against you in a court of law. You have the right to consult an attorney before speaking to the police and to have an attorney present during questioning now or in the future. If you cannot afford an attorney, one will be appointed to you before any questioning, if you wish. If you decide to answer any questions now, without an attorney present, you will still have the right to stop answering questions at any time until you talk to an attorney. Knowing and understanding these rights as I have explained them to you, are you willing to answer my questions without an attorney present?"
-Tolman-Oppenheimer-Volkoff Limit: An upper bound to the mass of neutron stars, past which the neutron star collapses into a black hole. The limit is 1.5 - 3 solar masses, which corresponds to an initial mass of 15 to 20 solar masses.
-Betz' Law: The maximum amount of power that can be extracted from the wind, irrespective of the design of the wind turbine. The law is based on the conservation of mass and momentum of the air flow. It states that no wind turbine can extract more than 16/27 (about 59.3%) of the kinetic energy of airflow.
-Flight by helium: Larry Walters, in 1982, tied 45 weather balloons, filled with helium, to a lawn chair and took flight, attaining a height of 16,000 feet.
-Position space: The set of all position vectors of an object.
    -Phase space: A space in which all possible states of a system are represented, and are represented by one point. 
-Plausible deniability: Senior figures in the chain of command assign responsibility for an action to lower ranks and render all records and instructions inaccessible to themselves, keeping themselves in the dark about the project. This is because if the project involves illegal or unpopular activities, knowledge of the project and therefore involvement in it can be denied if the project gets out.
    -Term was first used by senior CIA officials under the Kennedy administration. 
-Major system: A basic system to remember numbers visually, assuming users are familiar with the memory palace technique. Each number is associated with a phonetic sound, i.e. a letter or pair of letters, and numbers are broken down into their digits, which are converted into letters, effectively turning a long number into a string of letters. Spaces and vowels can be added at will to turn the string of letters into a meaningful phrase or sentence, which is then encoded into the memory using the memory palace technique.
    -Number-letter associations: 0 - s, 1 - T/D, 2 - N, 3 - M, 4 - R, 5 - L, 6 - Sc/Ch, 7 - K/G, 8 - F/V, 9 - P/B
-Word Memory Championship disciplines: 1. One Hour Numbers, 2. 5 Minute Numbers, 3. Spoken Numbers (read at a rate of 1 number/second), 4. 30 Minutes Binary Digits, 5. One Hour Playing Cards, 6. Random Lists of Words, 7. Names and Faces (15 minutes), 8. 5 Minute Historic Dates, 9. Abstract Images (randomly generated black and white spots), 10. Speeds Cards (memorize the order of a deck of cards as fast as possible).
-NATO phonetic alphabet: A - Alpha, B - Bravo, C - Charlie, D - Delta, E - Echo, F - Foxtrot, G - Golf, H - Hotel, I - India, J - Juliett, K - Kilo, L - Lima, M - Mike, N - November, O - Oscar, P - Papa, Q - Quebec, R - Romeo, S - Sierra, T - Tango, U - Uniform, V - Victor, W - Whiskey, X - Xray, Y - Yankee, Z - Zulu
-Prison security classifications: Prison security varies in terms of security measures, protocol, amount of guards, type of housing, and weapons/tactics used by staff to control inmates. More dangerous inmates go to higher security prisons; the scale runs from a minimum of 1 to a maximum of 6.
    -Supermax: Supermax facilities house the most dangerous inmates, such as those convicted of assault, murder, and prison gang members. Many lower security prison have supermax sections. 
        -ADX Florence: The most secure prison in the country, the entire prison is a supermax, with an ultramax component. In the prison, prisoners are under 23 hour confinement with no amenities, but the ultramax section has 24 hour confinement with no human contact or opportunity to earn better conditions through good behavior; the ultramax houses international and domestic spies, traitors, terrorists, and other top criminals that the US government cannot execute.
    -Maximum security: All prisoners have individual cells with sliding doors controlled by a remote station. Prisoners are allowed out of their cells one hour out of the 24, and are accompanied by restraints and correctional officers.
    -Close security: Prisoners reside in 1- or 2-person cells; each cell has its own toilet and sink. Prisoners can leave their cells for work assignments or correctional programs, and are allowed into a common area or yard. Fences are usually double fences with guard towers interspersed in between. The guards are always armed, and the fence may have electric current passing through it.
    -Medium security: Medium security prisoners sleep in dormitories and in bunk beds, and have lockers for personal belongings. There are communal showers and sinks; dorms are locked only at night, and movement is less restricted. There is usually a double fence with guards on patrol.
    -Minimum security: Prisoners in minimum security are generally non-violent white collar criminals, and they live in less secure dorms. There are communal showers and sinks. There is a single fence that is watched but not patrolled. Prisoners often work on correctional projects, such as community service. Many states allow prisoners Internet access.
-Miracle fruit: Formally known as Synsepalum dulcificum is a berry that, following consumptions, alters the tastes of many foods, such as causing sour foods to taste sweet. This effect has a duration of around 60 minutes, which is as long as until the protein miraculin washes out of the saliva. 
    -The berry grows on a shrub that is 6-15 feet tall, with dense foliage. Leaves are 4-7 inches long, and 1-2 inches wide, and are clustered at the ends of branchlets. The flowers are brown.
    -Miraculin: The main glycoprotein in the miracle fruit. The protein is not sweet itself, but binds to sweet receptors in the tongue and causes even sour foods to taste sweet. 
-Latent inhibition: The observation that a familiar stimulus takes longer to acquire meaning than a new one. 
    -Low latent inhibition: (You were diagnosed with LLI by Donna) People are constantly bombarded with an intense stream of incoming stimuli, but most can block it out; those with low latent inhibition cannot, and are perceptive to all of it. Low latent inhibition thus correlates with hyperactive behavior and distracted conduct. This distractedness can manifest as a form of ADHD, a tendency to switch conversational topics quickly and abruptly, and absentmindedness. 
-Plexiglass: Formally, poly(methyl methacrylate), or PMMA. It is a transparent thermoplastic (pliable above a certain temperature, solid below; see above) that is lightweight and shatter-resistant. It is cheap to make and easy to handle, and so is an economical alternative to polycarbonate, though it lacks the extreme strength found in polycarbonate. 
-Polycarbonate: A.K.A. PC. A subgroup of thermoplastic polymers that can be easily molded and thermoformed (the process of heating a thermoplastic sufficiently to induce pliability, and then taking advantage of the pliability by molding it in any desired shape; the thermoplastic is then cooled to re-solidify). Polycarbonates are long chains of carbonate groups (a carbonyl group joined by 2 alkoxy groups; structure: -O-(C=O)-O- in PCs). PC is very durable, and has high impact resistance. However, it has low scratch resistance, so usually a hard-coating is applied in manufacturing. PCs are also usable over a large range of temperatures.  
    -PCs are used in many applications, from lenses to bulletproof glass.
    -PCs have over 90% light transmission, and protect from UV rays.
-Dunning-Kruger effect: Stupid people tend to think of themselves as much smarter than they actually are, probably because their low learning curves prevent them from identifying or recognizing their own mistakes and making their own abilities appear more flawless than they are. Smart people may think of themselves as stupider than they are, because they attribute their intelligence as being possessed by all of their peers and as average. 
    -Stupid people will overestimate their own intelligence, underestimate the intelligence of others, miss their own mistakes, and fail to look back on themselves and observe a pattern of growth (as this would suggest that they were stupid in the past).
-Hotwiring a car: Works best on cars manufactured before the mid 1990s. New cars are mostly run by computers and have their own CPUs, and so are extremely difficult to hotwire and require advanced equipment and lots of time.
    1. Using a screwdriver, remove the steering wheel's casing; the screws should be located on the top and bottom of the steering panels. 
    2. Pry off the steering panel. A screwdriver might be necessary to jam in between the panel to get it off. This will expose the ignition cylinder (where the key is normally inserted) and all the wires running to it.
    3. Figure out which wires are for the battery, and which wires are for the starter; there should be 2 battery wires and either 1 or 2 starter wires. Generally, the 2 red wires are for power to the battery and the brown wire(s) are for the starter, though color varies per car and a manual specific to the car model might be needed. The other wires control power to accessories such as AC, radio, etc.
    4. Recommended that gloves are worn for this step. Cut the power (batter) wires from the ignition cylinder.  
    5. Strip the ends of the now cut battery/power wires to expose the copper underneath. Twist the copper ends of the 2 wires together to provide power to the car's electrical systems.
    6. Cut the starter wire(s) from the ignition cylinder, and strip the ends to expose the copper wiring underneath. 
        -These wires carry a live current, and will electrocute organic flesh in contact with it, so it is safe not to touch the copper wiring.
    7. If the car has 1 starter wire:   
        Touch the stripped end of the wire, with the copper wiring exposed, to the twisted together copper endings of the battery/power wires.
        If the car has 2 starter wires:
        Touch the stripped ends of the 2 copper wires to each other.
        This step momentarily provides power to the starter chip in the car to get the engine running.
    8. Deal with additional security, such as steering locks; some steering locks can be bypassed by wedging a screwdriver in between the steering wheel's joint to free it.
-Legal emancipation: An emancipated minor is someone under the age of 18 who is allowed to conduct his own occupation, for their own account, outside the influence of a parent or guardian. Emancipation is a legal process that frees a child from the control of his parents, and frees the parents of any responsibility of the child. It is usually granted by a court. Minors must file a petition with a family court for emancipation, and has to list reasons why it's in his "best interest" to be emancipated. They must also prove financial self-sufficiency. 
-Brouwer's Fixed Point Theorem: A theorem in mathematics (specifically, topology) that states that, in Euclidean space, every continuous function that maps a subset of n-dimensional space (this subset is called a "closed ball" in Euclidean space) to itself has at least one fixed point. This means that if some closed ball of Euclidean space is rearranged (i.e. mapped to itself), in any configuration following a continuous function, then there exists at least one point in its intial position (i.e. for a function f mapping a closed ball R to itself, f: R -> R, there exists at least one point, x_0, such that f(x_0) = x_0).
    -A function maps a closed ball onto itself if every point in that closed ball has an image (i.e. an output value from the function) also in the closed ball. For example, the function (x + 1)/2 maps onto itself over the interval (-1, 1).
    -Applications: 1. Coordinate transformations, that follow a continuous function, have at least one point in the exact same location. To visualize this, take 2 identical sheets of paper, with identical coordinate systems on them. If one of the papers is crumpled up in any way (analogous to a coordinate transformation), and is placed on top of the first paper in any location as long as the paper doesn't land outside the first, then there is a point on the crumpled paper that is directly above the corresponding point in the first, flat paper. This is because at least one point remains fixed despite the transformation. 
                2. If saltwater (or any liquid with particles in it) is stirred any amount, then when the stirring is finished, one salt particle will be in the exact same place it started. This is because salt particles in water can be viewed as a domain of a function, in which the glass is a subset of 3D space, and the stirring maps each point in the glass, which is represented by a salt particle, to some other point also in the glass (meaning the stirring is really a function that maps the closed ball that is the glass onto itself). 
-Approximating roots of a polynomial: All equations in 1 variable can be put into the form f(x) = 0; when f(x) is a polynomial, i.e. f(x) = (a_n)x^n + (a_(n-1))x^(n-1) + . . . + (a_1)x + (a_0), when the solution x = a is a root if f(a)  = 0.
    -Bisection method (A.K.A. binary search method, dichotomy method). This method takes an interval in which a root must be within and shrinks it successively. It is robust and reliable, but converges very slowly, gaining a tiny bit of accuracy per iteration. It is often used to generate a rough approximation of the root that is then used as a starting point in more risky, but faster converging, methods.
        -Method: 1. Find an interval [a, b] on which f(x) is defined, such that f(a) and f(b) have opposite signs. If this is true, then f(a) and f(b) have opposite signs, and so by the Intermediate Value Theorem, the function (which is continuous) must cross the x-axis at some point on [a, b]; this point is the root.
                          2. Take the midpoint of [a, b], given by midpoint = c = (0.5)(a + b). Then , either f(c) is a root, or f(a) and f(c) have opposite signs, or f(b) and f(c) have opposite signs.
                - In case 1, f(c) = 0, and the method concludes.
                - In case 2, f(a) and f(c) have opposite signs, so this becomes the new interval.   
                - In case 3, f(b) and f(c) have opposite signs, so this becomes the new interval.
                          3. In case 1, we are finished. In cases 2 and 3, reapply the method iteratively until a desired level of accuracy is attained. The intervals halve with each interval, so the accuracy doubles per iteration. Applying infinitely, the test converges to the real root.
        -Successively plotting iterations, the value of c for which f(c) is closest to 0 is the desired approximation for the root.
    -Newton's method: (A.K.A. Newton-Raphson method) This method builds off the idea that at, beginning with a guess f(x_0) on the function, the root of the tangent line at f(x_0) is closer to the root of f(x) than f(x_0) is. The tangent line's root, x_1, is used as the next point starting point, and another tangent line to f(x_1) is used and its root is found, and that point, x_2, is again used iteratively. This method converges relatively quickly, in quadratic time. However, if the initial guess is extremely far from the actual root, the method may or may not converge at all in some cases. Additionally, if an iteration point is a critical point, i.e. f '(x_n) = 0, then a divide-by-zero halts the method.
        -Method: Make an initial guess x_0, and use the formula x_(n + 1) = x_n - ( f (x_n) )  / ( f ' (x_n) )
        -Derivation: The slope of the tangent line is, by definition of derivative and of slope, m = f '(x_n) = (delta-y) / (delta-x); taking the slope from (x_n, f(x_n) ) to (x_(n + 1), f(x_(n + 1)) ) produces m = f '(x_n) = ( f (x_n) - f(x_(n + 1)) ) / ( x_n - x_(n + 1) ). Since x_(n + 1) is supposed to be a zero of f(x), as suggested by the method, we now have m = f '(x_n) = ( f (x_n) - 0 ) / ( x_n - x_(n + 1) ). Solving for x_(n + 1) provides the above formula.
-Hydraulic brake systems: A braking system in automobiles that uses braking fluid to initiate the brakes. When the brake is pushed, an attached pushrod is also pushed. This pushrod, horizontal, is attached to an also horizontal piston, both of which are contained in the master cylinder. Thus, when the brake is applied,  the pushrod pushes the piston forward, decreasing the volume between the end of the master cylinder and the piston. Brake fluid, contained in a fluid reservoir above the master cylinder, flows into this volume between the piston and the end of the master cylinder through a fluid inlet port. The reduction in volume increases the pressure of the fluid. However, the end of the master cylinder contains a valve adjoining a pipe that runs to brake caliber pistons. 
                 The increase in pressure forces fluid through the pipe and into the volumes between the brake pistons, symmetrically positioned so as to sandwich the wheel's rotator disk between them. When the fluid occupies the volume between the brake pistons and the chamber, it forces the brakes to close onto the rotator disk, where friction brings it to a stop (for this reason, the more incompressible the braking fluid, the better).
                  Removing the brake pulls the piston in the master cylinder back, allowing fluid to flow out of the brake pistons' chambers and bake into the reservoir, removing the friction and allowing the rotator disk to spin freely.
-Firing pin: In firearms, bullets contain primers within them, i.e. small metal containers of the primary explosive. The firing pin crushes the primer, increasing pressure on the explosive inside; the resulting hot gases ignite the propellant inside the bullet, firing the gun. Without the firing pin, a gun cannot fire. 
    -Construction: Firing pins are short rods of steel, small in size and rounded on one end so as to ensure that the primer is crushed and not pierced on contact. 
-Radiotrophic fungus: Fungi that use the energy of gamma radiation to drive their biochemical processes that allow them to survive. The exact pathway used to extract the energy is unknown, but it has been speculated that it would be similar to the photosynthetic pathway, in which energy of a photon is used for energy. 
    -This type of fungus was discovered in Chernobyl. 
-Cloud seeding: An attempted method of altering the amount and nature of precipitation by purposely injecting and dispersing materials into clouds to provoke rain. Natural rainfall occurs when the air is so saturated with moisture that it begins to condense into liquid water, and the air can hold no more moisture, so it falls. Cloud seeding uses particles that attract water to act as nuclei for gaseous water molecules to cluster around, aiding the condensation process. Common materials that are used are silver iodide, dry ice, and salts. Three main methods:
    -Static cloud seeding: Moisture is already present in the clouds, so a material (e.g. silver iodide) provides a crystal around which moisture can condense.
    -Dynamic cloud seeding: Injects materials into the air to produce vertical air currents that encourage more water to pass through the clouds, meaning more rain. 100 times more crystals are used than are used in static cloud seeding, and the entire process is very complex with many steps. 
    -Hygroscopic cloud seeding: Spreads salts throughout clouds using explosives; the salts attract water and grow in size as more and more water joins. This is the most promising area of cloud seeding research.
-Highway numbering system:
    -For Interstates: Regular interstate highways are 1 to 2 digits, running from 1 to 99, with an I in the front for interstate (e.g. I-5 or I-80). They are marked with blue shields with a red overhead banner at the top with the words "INTERSTATE".
        -Even numbered routes: Run from west to east, with the lowest numbered even routes running in the most southern part of the country and the highest numbered even routes running in the northern part of the country.
        -Odd numbered routes: Run from south to north, with the lowest numbered odd routes running in the western part of the country and the highest numbered odd routes running in the eastern part of the country.
        -Auxiliary interstates: Highways that are extensions of regular interstates, which are their "parent" highways. Auxiliary highways are designated by 3 digits, with a single number added to the front of the parent highways (e.g. I-680 is an auxiliary highway of the parent interstate I-80). These highways are extensions in that they can be loops (the highway diverges from the parent highway but loops back to it eventually), spurs (a branch from an interstate that diverges and doesn't reconnect), or other alternate routes. 
            -Even first digit: An even first digit typically (there are exceptions) designates a loop route from the parent highway.
            -Odd first digit: An odd first digit typically designates a spur route from the parent highway.
    -For US highways: US highway routes existed before the interstates were created in the 1950s (under Eisenhower). They are marked with a white shield against a black background.
        -2 digit routes: The even/odd and high/low numbering conventions for interstates are the same for US highways. 
-Finding square roots using Newton's method: The function f(x) = x^2 - A has a zero at sqrt(A). So, to approximate the square root at A, approximate the zero of f(x) = x^2 - A. Using Newton's method, x_(n + 1) = x_n - ( (x_n)^2 - A ) / ( (2(x_n) ) = 0.5x_n + 0.5(A/x_n) = average of x_n and A/x_n, where x_n is your guess and A is the number being divided by. Use a perfect square that's as close to A as possible but still underneath A. The square root of this number is our guess x_n. 
    -Example: Approximating sqrt(5000). Since 4900 < 5000 and is a perfect square, A = sqrt(4900) = 70. So, sqrt(5000) is approximately the average of 70 and 5000/70. 5000/70 = (10)(50/7) = avg(70, 71.42) which is approximately 70.7. The actual answer is 70.7106....
    -Dividing method: To divide a number A by a number x. The quotient can be viewed as the initial number, i.e. the divisor, plus some number D; D can be positive, negative, or 0. That is, A = (x)(x+D). So, D = (A - x^2)/x. Add this back to the divisor to get the quotient.
    -General algorithm for integer roots: To find the nth root of a number A, recursively apply the formula x_(k + 1) = (1/n)( (n-1)(x_k) + A/( (x_k)^(n-1) ) )
        -Derivation: The square root method above is a special case of the general algorithm, which is also derived from Newton's method. 
            1. Make an initial guess, x_0.
            2. Applying Newton's method to the equation x^n - A = 0, we have x_(k + 1) = x_k - f(x_k)/f '(x_k) = x_k - (x^n - A) / (n*(x_k)^(n-1)) = x_k - x^n/n  + A/(n*(x_k)^(n-1)) = (1/n)( (n-1)(x_k) + A/( (x_k)^(n-1) ).
-Map projections: A transformation from the points on the surface of a sphere or ellipsoid onto a plane or curved plane, i.e. a cylinder. There are infinite projections for any given ellipsoid, and all map projections have some distortion (proved by Gauss' Theorema Egregium).
    -Properties of distortion: The properties that concern a 3-D surface are area, shape, direction, bearing, distance, and scale. Some of these properties can be preserved by projections at the expense of others; all cannot be preserved at once. 
    -Creation process: A very common method is to first project the sphere or ellipsoid, non-developable surfaces, onto some developable surface (A surface that can be unfolded into a plane without stretching, tearing, shrinking, or diLating; cones, cylinders, etc. are developable, but the sphere, ellipsoid, etc. are not) and then unfurling it. 
    -Metric preservation:
        -Conformal: Conformal projections locally preserve angles. So, infinitesimal circles on the surface map to circles (of varying size) on the projection; non-conformal projections map circles to ellipses. 
        -Equal Area: Preserve's area ratios.
        -Equidistant: Preserve's distance from a standard point or line.
        -Gnomonic: A projection that maps great circles (A great circle of a sphere is the intersection of that sphere with a plane when the plane contains the sphere's center in it) to lines.    
        -Retroazimuthal:  Preserve's direction. The bearing to point B from point A on the surface is preserved in direction on the mapping from point A to B.
    -Mercator projection: The most commonly used projection; very good at preserving shape, very bad at preserving area.
-Ulam spiral: If the natural numbers are written in a spiral, going counter-clockwise and increasing in step, the prime numbers tend to be found on diagonals; with very large amounts of numbers, it can be shown that the lines are non-random.
    -Example: The primes are on diagonals.
                        69 68 67 66 65 64 63 62 61 
                        40 39 38 37 36 35 34 33 60
                        41 18 17 16 15 14 13 32 59
                        42 19  5   4   3  12 31 58  
                        43 20  6   1   2  11 30 57
                        44 21  7   8   9  10 29 56  
                        45 22 23 24 25 26 27 28 55
                        46 47 48 49 50 51 52 53 54
-Gravity assist: A technique for altering a spacecraft's trajectory and velocity by using a celestial body's relative motion and gravitational field. The extra energy comes from the planet's speed itself; it loses an equal amount of momentum to that gained by the spacecraft, though the planet's mass is so much bigger than the spacecraft's that the loss in momentum does little to affect the planet's speed. 
    -Principle: Simplified, when a spacecraft approaches a planet at speed V, if it is moving at a sufficient speed for orbit (so it doesn't crash into the planet) it will loop around the planet and move in the opposite direction at velocity - V. However, the velocity V is relative to the planet; the planet is also moving relative to the sun. Relative to the sun, the planet has velocity -U, and so the spacecraft, when approaching the planet, has a velocity, in the reference frame of the sun, of U + V (since the planet and spacecraft move towards each other). When the spacecraft loops around, the velocity is the velocity of the spacecraft relative to the planet (U + V) plus the velocity of the planet relative to the sun (V), so the new speed is U + 2V; the spacecraft is moving faster. 
    -Analogy: If a tennis ball is thrown at a 50 mph train at 30 mph, the train sees the ball approaching at 80 mph. So, it must also leave, relative to the train, at 80 mph. However, from the perspective of the ground, the 80 mph speed of the tennis ball is relative to the train itself, and the train's velocity has not been factored; the speed relative to the ground is 80 + 50 = 130 mph.
-Psychological priming: An effect on the subject's implicit memory in which exposure to one stimulus affects the response of the subject to a new stimulus. For example, if someone's brain has been recently exposed to the word "table" and then they are asked to complete the word "tab-" the probability of that person choosing the word "table" is greater than that of an unprimed person. The priming works best when stimuli are of the same modality (the particular way in which information is encoded into the brain, e.g. visually, musically, etc.).
-Estimating time with stars: Locate Polaris - the North star - and the big dipper; the the last 2 stars on the end of the big dipper's pot are the pointer stars; they point at Polaris. Polaris is the center of the clock, and the line through Polaris and the pointer stars is is hour hand. 
    1. Estimate what hour value, e.g. 1 o'clock, 2 o'clock, etc. the line through Polaris and the pointer stars is at.
    2. Count the number of months the date is after January 1, to an accuracy of a quarter month. 
        - For example, if the date were March 16th, which is about mid-March, the value would be 2 months + 2 quarter months = 2.5 
    3. Add the hour value from step 1 and the month value from step 2, then double the sum.
    4. Subtract the number obtained at the end of step 3 from 16.25; if the number from step 3 is greater than 16.25, subtract from 40.25 instead. 
    5. If the final value if less than 12, then that's the time in PM. If the value is greater than 12, subtract 12 to get the time in AM.
-Performing CPR: 
    1. Assess the victim's consciousness by tapping him on the shoulder and talking to him. If he responds, CPR is not required, though preventing or treating shock might be.
    2. If the victim does not respond, send for help. Do NOT check for a pulse, as it can be difficult to locate and therefore checking for one can waste time. Assume the worst-case scenario and proceed.
    3. Check for breathing by putting your ear right above the victim's nose and mouth, listening for breathing and try to feel breath on your ear. Watching the chest rise and fall is also a sign of breathing. If the victim is breathing do NOT perform CPR, as it could cause the heart to stop beating.
    4. Place the victim on his back, as flat as possible. 
    5. Put the heel of one hand on the victim's breastbone, right between the nipples. Put the other hand on top of the first one, palm down. Lean over on your knees so that your arms are rigid and your shoulders and head are directly above your hands.
    6. Perform 30 chest compressions; press down about 2 inches each compression and move at a relatively quick frequency, roughly 103 beats per minute. A useful technique is to perform compressions at the beat rate of the song "Stayin' Alive".
    7. Periodically make sure that the airway is open by putting your hand on the victim's forehead and 2 fingers under the chin, tilting the head back to open the airway.
    8. Give 2 rescue breaths every 30 compressions, making sure that breathing in causes the chest to rise. 
        -Make sure you can feel the breaths go in; if, after repeated attempts, they do not go in, the victim may be choking; perform the Heimlich maneuver.
    9. Repeat the cycle of 30 compressions and 2 breaths until help arrives.
-Cartesian product: The product of 2 sets is every possible combination of each element in each set. So for A = {1, 2, 3} and B = {a, b, c}, A x B = {(1, a), (1, b), (1, c), (2, a), (2, b), (2, c), (3, a), (3, b), (3, c) }
-Prime number theorem: A formalization of the idea that the distribution of primes becomes less common as numbers get bigger. Informally, the probability that a randomly selected integer between 0 and N is a prime is about 1/ln(N); this means that the probability of finding a prime is about linearly inversely proportional to the number of digits that number has. Formally, if pi(x) is the prime counting function, outputting the number of primes less than or equal to x, then the function x/ln(x) is a good approximation of pi(x), in the sense that in the limit as x approaches infinity, pi(x) / (x/ln(x)) ) = 1. 
    -Alternatively, pi(x) = integral from 2 to x of dt / ln(t)
    -Riemann hypothesis: Conjectures that the error between pi(x) and x/ln(x), i.e. the difference between the number of primes and approximating it using the prime number theorem, is exactly (sqrt(x))(ln(x)), where the above integral is used. Phrased in other terms, the real part of all non-trivial zeros of the Riemann zeta function is 1/2. 
        -Riemann zeta function: f(s) = sum from (n = 1) to infinity of 1/n^s, where s is a complex number; the series converges when Re(s) > 1. 
-Domain coloring: A way to graph complex function, which is a function from R^2 to R^2. As this would require 4 orthogonal axes to graph in the usual way, in 4-D space, a better way to do so in our 3-D limitations is by using color. Essentially, on a 2-D plane with Cartesian style axes, a real and an imaginary axis are placed. A point's position on the graph represents the input of the function; for a point (2, 3), that represents a real part of 2 and an imaginary part of 3 for the point 2 + 3i, which is the input of the function, f(2 + 3i). The image of that point, i.e. the function's value for it, is also a complex number, but rather than showing the real and imaginary parts of it, we will show the phase (the angle, theta) and modulus (the absolute value), and we do this using color. Note that in polar form, z = r*exp(i*theta), r = mod(z), theta = phase(z).
    -The phase, theta, of the function's input's image is represented using hue. Since hue follows a cyclical/circular format, and the angle theta moves about the unit circle, this correspondence works well. The color wheel runs from red to yellow to green to cyan to blue to magneta to red again.
        -Legend of hue to phase: red = 0 mod (2pi), yellow = (pi/3) mod (2pi), green = (2pi/3) mod (2pi), cyan = pi mod (2pi), blue = (4pi/3) mod (2pi), magneta = (5pi/3) mod (2pi).
    -The modulus also follows an intuitive correspondence. The intensity, or lightness, of the color represents the absolute value of the number.
-HEAT round: High Explosive Anti-Tank (H.E.A.T.) warheads are a type of missile designed to create a stream of metal traveling at hypersonic velocities under great pressure; the force originates from a shaped explosive charge. The metal, when traveling at those speeds, is superplastic and punches through most other barriers. It does not depend on thermal energy, despite the name.
    -In a solid material, the metal can move up to 25 times the speed of sound. The warhead must be detonated at the correct distance from the target; too close and there isn't time for the particle stream to fully develop. For this reason, HEAT warheads have the payload in the middle and a long nose at the end to provide some distance between the payload metal and the target. After about 2 meters, the particle stream disintegrates. 
    -The warhead punches right through tank metal and obliterates whatever is inside. 
    -Spinning the warhead (typically a good way to maintain accuracy for any rifle-based projectile) decreases the power of the stream; the centrifugal force de-concentrates the particle stream. Fins are often added instead for accuracy. The file describes a triangulated (A surface broken down into simplifies; a simplex is a triangle generalized to higher dimensions. In 2-D, a shape is triangulated into many triangles of varying size and form; in 3-D objects are broken down in various pyramids) and unstructured figure using unit normal vectors and the vertices of the pyramids involved in the triangulation, given by the Cartesian coordinate system.
    -Munroe effect: Shaping a charge can direct the entire force of the explosion in one direction rather than in all directions (i.e. a sphere) and thus increase the impact on a desired target. For example, detonating C-4 on its own will cause it to explode outwards in all directions, so a lot of explosive force is lost to the part of the C-4 not touching the target. Putting it in a cylinder with an inverted cone (vertex pointing into the cylinder) at one end makes detonation far more efficient; when the C-4 explodes, the cone inverts outwards out of the cylinder and the blast shockwave and heat follow, directing the entirety of the explosion in one direction. 
-Cluster munition: (A.K.A. cluster bomb) A missile that carries a warhead composed of many, up to thousands, of tiny bombs; the warhead breaks apart while the missile is still in free fall (cluster bombing is a form of dumb bomb) and sends thousands of tiny bombs falling down. Sometimes, many of these bombs are intentionally (and often unintentionally) left undetonated to act as land mines when further enemy troops enter the area.
-Fused deposition modeling: (FDM) An additive manufacturing (manufacturing by adding substances, rather than taking away) technique. The process works by laying down the materials in successive horizontal cross sections on top of each other.
    -STL: (STereoLithography) A file format used for FDM manufacturing. The format is popular in CAD (computer aided design). The format describes the surface geometry of a 3-D object without describing color, texture, or other properties.
-Pyramid scheme: A type of scam, illegal in many countries, that promises participants a high return on their investment with the condition that they recruit more investors; the geometric progression of the model makes it unsustainable. Typically, successful schemes use a fake, sophisticated sounding, easy to understand money-making idea under the guise of a credible yet fake business. The founder of the scheme makes the initial investment, but receives a cut of every subsequent investment. Once that founder finds new recruits, those recruits invest and the founder gets some money. Those recruiters recruit a second set of people (since each recruit recruits more than one person, for every one person that joins, one can expect more than one to join subsequently; this is what gives rise to the exponential growth) who then make their investment; the founder receives the same cut and the 
                initial recruiters get a smaller cut. This goes on until the founder and some at a very high level in the pyramid make a very significant amount of money, the bulk of the pyramid makes very little money, and the bottom loses money. Because the growth is exponential, a single group of people at the bottom can sustain all those above it (for example, if each person must recruit two more, the founder gets 2, they then get 4 total, who get 8 total; the bottom 8 can sustain the 7 above them).
-Partially ordered set: (A.K.A. poset) A formalization of the concept of arrangements and order. A poset contains a set of elements and a binary (inputs two elements) relationship that outputs which is greater, or if both are equal. The poset is partially ordered in the sense that some elements do directly precede other elements, but other elements can be put into pairs for which neither precede the other (e.g. the set {2, 3, 4, 4, 5}). 
    -Definition: A binary relation <= (greater than or equal to) over a set S which is reflexive, antisymmetric, and transitive.
        -Reflexivity: a <= a
        -Antisymmetry: If a <= b And b<= a, Then a = b.
        -Transitivity: If a <= b And b <= c, Then a <= c.
    -Hasse diagram: A diagram used to represent finite posets. Each element of a poset S is represented as a vertex; lines are drawn between the elements, with the condition that a line drawn upward (in the 2-D plane) from x to y implies that y covers x (i.e. that x < y And there is no z in S such that x < z < y; y is "directly" above x, such as the numbers 5 and 6 under a set of integers). Equivalent elements are aligned with each other horizontally. There exist many Hasse diagrams for a given finite poset, with some emphasizing some characteristics but ignoring others; there usually isn't a perfect Hasse diagram. 
-Determinants (of a matrix): The intuition behind the determinant is that it's a function that inputs N vectors (the columns of the matrix) and outputs a number. Geometrically, for the unit hypercube (sides of length 1) in N-dimensional space, the hypercube is spanned by N unit vectors; the determinant of the matrix of those spanning unit vectors is the signed hypervolume of the unit hypercube. All matrices can be thought of as a linear transformation applied to the identity matrix, which is the set of N column unit vectors (i.e. the vectors have all 0s in components and exactly one 1) in N-dimensional space (so the matrix is N x N), and the determinant of that linear transformation matrix is the hypervolume of the transformed hypercube.
    -Properties: The properties of the determinant follow intuitively from this interpretation.
        -The determinant of the linear transformation T applied to the N x N identity matrix such that for each element, multiply by 1 and add 0, returns the identity matrix's determinant, which is its volume, which is 1.
        -Multiplying the a row by a constant multiplies the entire determinant by that constant. This is because multiplying a row of the matrix scales up one entire side of the hypercube by that constant, and since the hypervolume of the resulting hyperparallelopiped is the product of its sides, the volume is scaled by that constant, and so the determinant is as well.
        -Exchanging two rows changes the sign. This is because exchanging two rows inverts the hypercube by reflecting it about the origin, which makes the signed hypervolume negative.
    -Jacobian: A matrix meant to define a generalized and complete form of a derivative for vector-valued multivalued function, i.e. a function, say f, such that f: R^m -> R^n for natural numbers m, n. Such a function can be viewed as a transformation from one vector space, R^m, to another vector space, R^n; from this interpretation, f doesn't take m inputs and produce n outputs, but instead takes one input, an m-dimensional vector, and produces one output, an n-dimensional vector. Such a function can always be written as n functions, each from R^m -> R, i.e. for f: R^m -> R^n, we can write f = [f_1(x_1, ..., x_m, ..., f_n(x_1, ..., x_m)]. The Jacobian matrix, usually denoted J, is given by the n x m matrix whose rows are the partial derivatives of each sub-function F_i with respect to each of the n variables.
        -Structure: Explicitly, the Jacobian matrix is structured as J = [ d(f_1)/d(x_1)     ...     d(f_1)/d(x_m) ]
                                      [         ...            ...             ...        ]
                                      [ d(f_n)/d(x_1)     ...    d(f_n)/(d(x_m) ]
        -Intuition: First, consider the one-dimensional derivative. The definition of the derivative is, for a function f: R -> R | for all x in R, x maps to f(x), df/dx = limit as t -> 0 of (f(x + t) - f(x))/t. This implies that f(x + t) = f(x) + t*(df/dx). This last equation shows that when we perturb x slightly by an amount t, the function is perturbed slightly by an amount df/dx*t, i.e. a tiny change in the independent variable maps by an amount t maps to a tiny change in the dependent variable that is scaled by the rate of change of the dependent variable. This also goes to show how derivatives are both linear transformations and also a kind of linear approximation to the function. The same logic applies with higher vector-valued and multivalued functions. Since it's no longer one-dimensional space being transformed by the function, the rate of change can no longer be specified by a single number; rather, it's given by a matrix, which is essentially a linear mapping anyways. Thus, by analogy to the one-dimensional case, for a function f: R^m -> R^n, for a tiny perturbation by a vector T, for input vector X, f(X + T) = f(X) + J*T, where "*" denotes matrix multiplication and J is the Jacobian matrix.
-Thermoelectric effect: The direct conversion of temperature gradients to electric potential gradients (i.e. voltage), and vice versa. It is caused by delocalized electrons in the metal, which behave as a gas, moving down the temperature gradient from hot to cold, just as a gas expands when heated; the moving charge carriers create a current.
-Proof that 1 + 1 = 2: 
    1. Define a set of numbers N known as the natural numbers. N is a subset of the complex numbers C, and every element in N obeys the following postulates (known as the Peano postulates):
        P1. 1 is in N
        P2. N is a totally ordered set. If an element x is in N, then there exists a direct successor element x' also in N
        P3. There is no x such that x' = 1
        P4. For all x not equal to 1, there exists a y in N such that y' = x
    2. Now we can define the binary operator of addition using the idea of a successor and recursion. 
        -Definition: Let a and b be in N. IF: b = 1, THEN: We define "+" as a + b = a'
                         Otherwise, IF: b is not equal to 1, THEN: Let there be a c in N such that c' = b (from P4). We now define "+" as a + b = (a + c)' 
                                if c is not equal to 1, consider the element d in N such that d' = c and apply the same logic. Recursively move forward thus until you arrive at an element of N that is equal to 1.
    3. We can now prove the theorem 1 + 1 = 2. 
        a. Define the number 2: 2 = 1'
        b. 2 is in N (from P1, P2, and the definition of 2)
        c. We above defined "+" as a + b = a' IF b = 1.
            Let a = b = 1. Then a + b = 1 + 1 = 1' = 2.
            Q.E.D.
    -Peano axioms: First, define the set of natural numbers N as the intersection of all inductive sets (set with the property that if x is in the set, so is the successor of x) that contain 0
        1. 0 is in N
        2. If a is in N, then a' is in N, where a' is the successor of a
        3. There does not exist an element a in N such that a' = 0
        4. If two numbers have equal successors, then the numbers themselves are also equal.
-Hausdorff space: A topological space in which any two points can be separated by disjoint open neighborhoods. 
    -Ex: The real numbers are a Hausdorff space, because any two elements are separated by an interval r, meaning that two possible neighborhoods exist around those two points with radius r/2 such that each neighborhood around a point does not contain the other point.
-Convolutions: The essence of a convolution is a generalization of a moving weighted average. You take the unit (meaning simplest point representing the scaling identity 1) point located at the origin (which is the most simple position; the additive identity, 0) and find the value of that point in the function you're concerned with. To find the value at any other point, you can use the value of the unit point located at the origin, but scaled up to the point in question and positionally shifted to the position of the point in question. If the function is h(x,y) for the simplest point, then to find the value of the i-th point, multiply by the weight m_i of the point in question and scale by x_i and y_i for the position: (m_i)(h(x - x_i, y - y_i). Do this for every point, then add it all up to get the total impulse; if the points are continuous, this becomes an integral. Often times the weight of a point also depends on its 
            position, in which case m = m(x, y)
    -Formula: For functions f and g, both functions of t, the convolution is (represented by the asterisk): f * g = integral from (- infinity) to infinity of f(T)g(T-t) dt, where T is just a dummy variable to represent that the functions are over all space in the 2-D Cartesian plane
-Partition numbers: A partition of a number is the set of ways to write that number as a sum of positive integers, independent of order. The partition numbers are the number of ways the natural numbers can be partitioned. Since 1 is partitioned into 0 ways, 2 into 1 way, 3 into 2 ways (1+2 and 1+1+1), and so on, the partition numbers begin 0, 1, 2, ...
    -Partition function: The function for the number of ways a given positive integer could be partitioned took years to find. It is: 
        p(n) = (1/(pi*sqrt(2)))(sum from k=1 to infinity of ( sqrt(k)*(A_k (n))*(d/dn((1/(sqrt(n-1/(24))))(sinh(pi/k * sqrt(2/3(n-1/(24))))))) where A_k (n) = sum over all positive integers less than k that are relatively prime to k of (exp(pi*i(s(m, k) - 1/k(2nm)))) 
-Cauterization: The process of burning a body part to close off that part of it, such as to prevent exsanguination (bleeding out).
-Fiberglass: A plastic polymer reinforced with fibers of glass. It is lightweight and very strong and robust, and although it cannot rival carbon nanotubes it is less brittle and expensive. The fibers of glass must be completely free of defects in order to attain extremely high tensile strengths (Ultimate Tensile Strength (UTS): The maximum deformation force (pulling/stretching force) that a material can withstand, measured in units of maximum force per cross-sectional area, or Pascals). 
    -Manufacturing: 1. Large amounts of heat are applied in order to melt silica sand, limestone, kaolin clay (a.k.a. kaolinite, a clay mineral layered with silicate), fluorspar (a.k.a. fluorite, a material composed of calcium fluoride, CaF_2), colemanite, dolomite, etc.
                  2. The mixed liquid solution is extruded through bushings (many tiny openings or orifices, around 5 to 25 micrometers in diameter).
                  3. After being coated with a chemical solution, the fibers are bundled together into a roving (a long and narrow bundle of fiber).
    -Use in bulletproof material: To create a bullet-resistant material using household materials, two things are required: woven roving of fiberglass and polyester resin. 
        -Polyester resin: Unsaturated resins (very viscous liquids that harden with treatment; they are hydrocarbon secretions of many plants composed of 
        -Process: 1. Buy several sheets of woven roving fiberglass; use a box cutter or something equally sharp to cut into desired shape. Use scotch or duct tape to secure the edges to make sure the roving doesn't fall apart. 
             2. Saturate the sheets of roving with polyester resin (e.g. a catalyst hardener) by soaking and painting it. Stack the layers on each other; 20-28 layers should be enough. Let it dry for 25-60 minutes. This also has the effect of curing (toughening a polymer material by cross-linking its polymer chains together) the material.
             3. Apply pressure to flatten the stack and squeeze out excess resin and providing consistency to the finished product. This compression process can go on for days. It may be a good idea to cover the stacks with saran wrap to prevent the compression materials (e.g. boards of wood) from sticking to the roving.
            -The finished product will be able to stop most handgun rounds and shotgun rounds, though it will not stop center-fire rifle rounds (Centerfire ammunition: A cartridge with the primer located in the center of the case head. The primer is separate from the bullet's mechanism. It has the advantages of being safer to use (can withstand rougher handling), and the higher pressures result in greater velocity).
-180-degree Rule: A general principle in filmography that states that whenever a conversation between two characters is being filmed, it is best to keep a camera's arc of motion restricted to 180 degrees, or in other words, on one side of the line of action (the imaginary line connecting the two characters or along which one character moves). In this way one character is always on the left and one on the right; when the camera cuts to the other side of the line of action, the characters switch places.
    -The camera can shift its angle to the line of action. Decreasing the angle to close to 0 degrees (i.e. closer to the left character) allows the camera to focus on the right character, whereas increasing the angle to near 180 degrees (i.e. closer to the right character) allows the camera to focus on the left character. Typically, part of or all of the back of the head of the character to which the camera is closest in one of these angular extremes is seen on one side of the screen, with the emphasized subject on the other side.
    -Advantages: Prevents disorientation in the audience. It also makes the filmography more fluid. 
    -Breaking the rule: Can help with dramatic purposes, or to emphasize a character's disorientation, confusion, or loss of direction. 
-Divergent thinking test: A test designed to measure creativity; specifically, divergent thinking. It consists of giving the participant an object (usually ordinary and mundane, such as a paperclip or brick) and having them brainstorm as many possible uses for that object in day-to-day life as they can in a set time period. Three categories are measured to be compounded into a creativity test: fluency (the raw number of responses), originality (how unique, unorthodox, and unusual the responses are), and flexibility (how many different categories of use are spanned by the responses).
-Brisance: The effectiveness with which a high explosive can shatter matter. It depends most primarily on the detonation pressure; additionally, the higher the pressure, the finer the fragmentation of the explosive case. Greater pressure correlates to greater detonation velocity of the shock front, but does not correlate to greater total energy of an explosive, so a more brisant explosive will have smaller fragments but will not necessarily project those fragments at greater velocity. 
    -High explosive: An explosive which detonates (produces an exothermic shock wave that propagates at supersonic speeds).
    -Low explosive: An explosive which deflagrates (the subsonic thermal propagation of combustion due to heat transfer; burning).
-Prandtl-Meyer Expansion Fan: When a supersonic shock wave turns around a convex corner, it decomposes into an infinite number of diverging Mach waves, expanding the shock wave and gradually turning it about the corner.
-Cartridge: (A.K.A. round or shell) A type of ammunition for firearms. It consists of the bullet, a propellant, and a primer, all contained within a casing. The primer reacts first, which leads to the propellant igniting; the jet of hot gases produced create enormous pressures within the casing that push the bullet out of the bore (gun barrel). Typically the casing itself will expand a bit in response to the propellant's ignition, and re-contract after the bullet leaves the casing and pressure returns to atmospheric pressure. The casing will also have a rim on the back, where the gun can grip the casing to prevent it from flying off with the bullet.
    -Primer: The primer is in the back of the cartridge, with the propellant in the middle and the bullet in the front; when the trigger is pulled, the firing pin strikes the back of the cartridge, thereby striking the primer, which causes the primer to deflagrate (rapidly burn but not detonate). The jet of burning gas from the primer ignites the propellant. 
    -Propellant: (usually gunpowder/black powder) A mixture of sulfur, charcoal (The greyish residue of trace carbon and ash left over from combusting (i.e. burning) materials in low-oxygen environments; the low oxygen content leaves a lot of the carbon behind from the fuel as charcoal), and saltpeter (potassium nitrate). Gunpowder explodes, producing the hot pressurized gases needed to propel a projectile, but is a low explosive, due to a slow rate of decomposition and low brisance; therefore, gunpowder deflagrates at a subsonic speed. 
        -Composition: Typical firearm rounds contain, by weight of proportion, 75% saltpeter, 15% softwood charcoal (charcoal produced by the pyrolysis (slow burning in the absence of oxygen) of softwood), and 10% sulfur. For mixtures (unsuitable for firearms) designed for more shattering power (e.g. to blast rock), a better composition would be 70% saltpeter, 14% charcoal, and 16% sulfur. 
            -Nitrates: The nitrate (i.e. saltpeter, KNO_3) provides the oxygen for the reaction. 
            -Charcoal: Supplies the fuel.
            -Sulfur: Doubles as a fuel and a catalyst; although it participates in the combustion, it also lowers the activation temperature of the reaction, which increases the rate of combustion. 
    -Bullet: The main purpose of the bullet is to leave the bore at high velocity. To do this, the bullet must be designed to form a seal with the gun's bore, so the propellant's pressurized gases do not escape the cartridge and reduce both efficiency and accuracy. It must also exit the bore cleanly, without distorting either the barrel or itself. Typically, the bore is ridged in spirals to spin the bullet as it leaves the bore, increasing accuracy. 
        -Caliber: The caliber of a gun refers to its internal barrel diameter, or equivalently, the diameter of the projectile it fires. By convention, when caliber is in inches, the word "inches" is replaced with "cal", or "caliber", and the decimal point is omitted (e.g. a bullet with diameter 0.22 inches would be pronounced "twenty two caliber").
        -Grain: The grain of a bullet refers to the amount of gunpowder in each cartridge by weight; thus, the grain is a (very very small) unit of weight. 1 grain = 64.69891 milligrams = 0.06469891 grams
        -Materials used: Most commonly, bullets are made out of lead (for low velocity rounds) or a lead-tin alloy (for higher velocity rounds). This is due to lead's high density (more kinetic energy on impact), low melting point (easy to mold into a bullet shape), and low cost. 
            -Jacketed lead: For even higher-velocity rounds, the bullet may consist of a lead core surrounded by harder metals, such as copper alloys, steel, or curponickel. 
                -Soft point bullets: Some bullet jackets don't extend up to the front of the bullet, where it will impact its target. This is to aid the expansion of the bullet inside its target, which increases lethality against biological tissue.
                -Armor-piercing rounds: A jacketed design with an extremely hard and dense metal at the end, such as tungsten, tungsten carbide (chemical formula of WC: equal parts tungsten and carbon atoms), depleted uranium (uranium metal with a very low composition of U-235, the fissile isotope), or steel. 
            -Incendiary: The tip of the bullet is coated with explosive or flammable mixture at the tip, designed to ignite on contact with the target. 
                -Exploding rounds: Other bullets have a small cavity in the tip that contains a low explosive, designed to detonate on impact with a hard surface.
            -Frangible: Bullets designed to break into tiny particles upon impact. This minimizes the danger of shooting straight through a target and hitting other unwanted targets.    
-Deep web: The part of the World Wide Web that standard search engines can't access and index. It contains the overwhelming majority of the information on the Internet (estimates put it at 96%, a total of about 7.5 petabytes = 1000 terabytes = 10^15 bytes. It's content ranges from serious underground criminal activity (e.g. child and snuff pornography, hired mercenaries, drug trading, etc.) to academic studies and papers, scientific research, government publications, e-books, articles, mailing lists, bulletin boards, etc. 
    -Tor browser: The most popular browser designed for the deep web. Its most primary use is anonymity and security. To route your Internet traffic, Tor repeatedly encrypts the message several times, and then sends it through several different network nodes around the world. Each network node (referred to as an Onion router, due to how decrypting the layers of encryption resembles peeling an onion) decrypts one additional layer of the encryption and then sends the message, which now has one fewer encryption layer, to the next onion router. Thus, each server through which the message passes has no knowledge of the message's origin, destination, or content. 
    -Most popular uses:
        -Ordering cannabis online: Many people use the deep web to order drugs, mainly marijauna, online. It can be delivered either through standard DHL shipping (DHL Express, is a division in the German logistics company Deutsche Post DHL, and is the world's largest logistics company; it leads the world in international express mail services), in which it is vacuum sealed four times (all the air is removed), or through drop shipping, where the product is stashed somewhere in nature, and GPS coordinates and a description of the drop site are sent to you.
        -Silk Road: An online black market in the deep web. It's similar to Amazon in the sense that sellers have feedback ratings, and the website administrators work very hard to prevent scammers from getting into the website. The site has a 97% success rate overall. It mainly (70% of goods) sells drugs, from cannabis to LSD to opiates, etc. but also sells legal items, such as art, apparel, clothing, etc. Other black market items also appear; the only exception is that the site does not allow any products whose purpose is to harm, such as child porn, stolen identities or credit cards, murder-for-hire, weapons of mass destruction, etc. The website had a sister site, called The Armory, whose purpose to sell weaponry (mainly guns), but which was shut down due to lack of demant. Silk Road was seized by the FBI in October of 2013, but was up and running again in November of 2013.
        -Murder-for-Hire: Many hitmen and mercenaries market their services on the deep web, accepting payment through BitCoin in exchange for killing the person of the buyer's choice. The most popular assassination websites are White Wolves and C'thuthlu. Prices start at 20,000 EUR for an ordinary citizen and can reach 1.5 million EUR for high-ranking politicians. Otherwise, many people sell things like a syringe of HIV-positive blood for around $6.
        -Rob-to-Order: People will steal items for you, from armored vehicles to stolen organs. 
        -Buying weaponry: The largest weapons market on the deep web is controlled by a site called Euroarms. They will ship most weapons that you'd want right to your doorstep, although they don't sell ammunition, which one would have to buy at another site. The service is only available in Europe, as very loose gun control laws in America ensure little demand. 
        -Buying credit cards
    -Bitcoin: (More comprehensive overview below) Bitcoins are a peer-to-peer digital cryptocurrency, used most often by people making transactions through the deep web. The currency uses cryptography to ensure that all transactions are untraceable. The bitcoin system relies on blockchains, or a public ledger that confirms every bitcoin transaction. Recording these transactions protects the neutrality of the system, and brings order to the currency. People who manually generate hashes to verify and confirm these transactions are known as miners, and they receive bitcoins for transactions they record through a transaction fee; mining is the primary way of creating and obtaining bitcoins (though they can obviously be obtained through transactions as well). Typically, it takes around ten minutes for a transaction to be confirmed, after which it is irreversible. 
        -Details: Every participant in the bitcoin economy has a bitcoin wallet to hold their bitcoins. To exchange bitcoins, one sends them to the bitcoin address of the person receiving the currency; a bitcoin address is never used more than once. Each person has a private key, which proves one's ownership of his bitcoins; bitcoin addresses are mathematically derived from a user's private key, but it is impossible to derive one's private key from any address. Anyone with one's private key can spend one's bitcoins, so it is important to keep it hidden. 
        -Cap: The number of bitcoins in existence will never exceed 21 million.
        -Exchange rate: The exchange rate is constantly changing, but fluctuates in the range of 1 BTC = ~ $570
        -Traceability: Although the transactions themselves are recorded, the transactants cannot be traced. This is because bitcoins are sent to a bitcoin address which automatically deposits the bitcoins into one's bitcoin wallet (the bridge between the address and wallet is the private key, which mathematically generates bitcoin addresses for its owner), but no information about the person who owns the address can be gleaned. In addition, bitcoins move around very fast, even if someone could follow the money, it would probably be in a totally different person's hands very quickly. 
    -Darknet: An overlay network (a network that's built on top of another network, so that logical connections between nodes in the overlay network are implemented using paths between nodes in the underlying network) which isn't open to the public, ie it can only be accessed using pre-determined specialized tools or authorizations. The most common example of a darknet is the Tor network, which is an example of a privacy network (a computer network designed to give nodes anonymity and provide secure communication between nodes), the other type of darknet being a friend-to-friend network (a computer network in which nodes only make connections to other nodes they already know).
-Types of knife blades:
    -Normal blade: A curving edge with a flat, dull back; the dull back allows the wielder to press his fingers into the back of the knife to apply and concentrate force. The knife can chop, pick, and slice. 
    -Trailing-point curved blade: Similar to a normal knife, with a flat back and curved edge, but both the back and the edge curve upwards at the end (around the tip) of the knife. This knife is optimal for slicing and slashing. These knives provide a larger belly (cutting area) and is frequently used as a skinning knife.
    -Clip-point blade: Like a normal blade, but the tip is "clipped", i.e. concavely curved inwards (towards the edge), forming a catenary-like shape. The sharp edge is good as a pick or for cutting in small places. The back of the knife has a false edge which can be sharpened to increase effectiveness at piercing.
    -Drop-point blade: The knife's back has a convex curve towards the tip. It is similar to the clip-point, but a tip that curves convexly rather than concavely makes the tip stronger but less suitable for piercing. 
    -Spear-point blade: A blade with a symmetric back and edge, and a spine that runs along the middle of the blade. The spine converges to the tip of the blade at the end. Both the back and the edge convexly curve towards the spine (the center of the blade) to form the tip; the knife can be single-edged or double-edged. 
    -Needle-point blade: Similar to the spear point blade, but rather than the back and edge convexly curving down to form a tip, both the back and the edge linearly converge to the point; the difference in tips between the spear-point and needle-point is akin to the difference between a rounded triangle and a regular one. The blade is highly tapered and a common fighting tool. The long, narrow blade is excellent at piercing but weak and susceptible to breaking. It's main use is stabbing; the knife can also be referred to as a "stiletto" or a "dagger".
    -Spey-blade: A knife with a single edge that is completely straight (i.e. horizontal) until very close to the tip, where it curves sharply upwards. The back is dull and also meets the tip by horizontally running up to the tip and then linearly and sharply turning down to the tip. This blade is best for skinning but terrible when it comes to penetration. It was once used to neuter animals.
    -Kamasu Kissaki blade: A design with a straight or very gently curved edge and back with a chisel-like tip that gets thicker towards the point. This thickening makes the point strong. The edge is more likely to be completely straight until it ilnearly turns sharply upward to meet the point, with the back being straight up until the tip and then gently angles downwards to form a tip. Both the back and edge are sharpened. 
    -Sheepsfoot blade: A knife with a straight edge and a straight, dull back. The back curves towards the edge at the end; the point is formed where the curved tip of the back meets to horizontal edge. The shape of the blade is meant to prevent accidental injury, and was used by sailors where the tosses and turns of a ship made handling knives dangerous.
    -Wharncliffe blade: Similar to the sheepsfoot, but the back's curving begins much earlier (closer to the handle) and is much more gradual. The blade is also abnormally thick.
-Fermentation: A metabolic (breaking compounds down to produce energy) process which inputs sugars and outputs, sometimes as by-products, acids or alcohols. It is mainly used by some bacteria and yeast, as well as in eukaryotic cells in a low-oxygen environment, as an alternative to glycolysis. However, in most biological systems, rather than replacing glycolysis, fermentation is used to replenish NAD+ from spent NADH so the NAD+ can be used in glycolysis as an electron-acceptor in the electron-transport chain. 
    -Types of fermentation:
        1. Ethanol fermentation: (A.K.A. alcoholic fermentation) Sugars, mainly glucose, fructose, and sucrose, are broken down for energy, and release carbon dioxide and alcohols in the process as by-products. For example, the alcoholic fermentation of glucose follows the chemical equation C(6)H(12)O(6) -> 2 C(2)H(5)OH + 2 CO(2) + energy; C(2)H(5)OH is ethanol.
        2. Lactic acid fermentation: This form of fermentation is used by biological organisms to replace oxidative phosphorylation, which replenishes NAD+ from NADH by using oxygen as an oxidizing agent in the electron-transport chain, when oxygen is unavailable. Instead of oxygen, lactic acid fermentation uses pyruvate molecules to oxidize NADH. The reduction of pyruvate produces lactic acid. 
-Centroid: (of an n-dimensional shape) The geometric center of a shape; the "average position" of all the points composing the figure. 
    -Centroid of triangle: The intersection of the three medians of the triangle (the line segment from the triangle's vertex to the other side, such that the line segment is perpendicular to the other side).
    -Methods for finding the centroid:
        -Plumb line method: The centroid of a lamina may be found experimentally by inserting a pin into the figure on or near its perimeter in such a way that the figure can freely rotate about the pin. Drop a plumb line (a string or rope going straight down under the influence of gravity) from the pin. Repeat the process with the pin in a different position of the object; the intersection of any two plumb lines is the centroid. 
            -Plumb bob: A weight suspended from a string to be used as a reference for the vertical. It exploits the very intuitive property that gravity will pull the weight (and therefore the string) straight down perpendicular to its surface, making the string a vertical reference.
        -For a finite set of points {x_1, x_2, ..., x_n} where each x_i is a vector pointing to the respective point, the centroid is given by: C (vectorially) = (x_1 + x_2 + ... + x_n)/n
        -If a figure can by broken down into several smaller figures, each of which has a known centroid, the centroid of the composite figure can be found. For C = (C_x, C_y), C_x = (sum of (each centroid's x-coordinate * sub-figure's area) )/(total area), C_y = (sum of (each centroid's y-coordinate * sub-figure's area) )/(total area)
            -Polygonal triangulation: Any non-self-intersecting polygon can be split up into triangles. Since the centroid of each triangle can be found by intersecting two of its medians, the centroid of the polygon can be found using the above formula.
                -Methods for polygonal triangulation: A polygon with n sides can always be broken down into (n-2) triangles. The simplest algorithm is the ear-clipping algorithm, which finds "ears" of polygons and creates triangles by using two edges of the polygon and one side lying entirely inside the polygon from the ends of the two edges. The ears are removed and the process repeated. This algorithm has a time complexity of O(n^2).
-Superposition principle: For all linear systems, the net response of the sum of two stimuli is the same as the sum of the responses of each stimulus individually. Or, for linear function F(x), F(x_1 + x_2 + ... + x_n) = F(x_1) + F(x_2) + ... + F(x_n) and F(ax) = a F(x) for constant a.
-Nitroglycerin: (Formally glyceryl trinitrate or 1,2,3-trinitroxypropane) A dense and odorless explosive, liquid at room temperature. Nitroglycerin has medical uses in vasodilation (opening up blood vessels to increase oxygen flow), used to treat heart conditions, as well as occasionally in prostate cancer treatment. It is widely used in many explosives and some gunpowders. Nitroglycerin has the potential to deflagrate (burn with shockwaves at subsonic speeds), but will detonate (produce shockwaves at supersonic speeds) if the initial decomposition produces a pressure wave with enough energy to induce detonation. The shock wave will propagate through the nitroglycerin at Mach 30. 
-Eurozone crisis: (***INCOMPLETE***)
-Mathematical universe hypothesis: First put forward by theoretical physicist Max Tegmark, the MUH proposes that all external physical reality is merely a complex mathematical structure. Living organisms are merely mathematical substructures that perceive the surrounding mathematical structure as physical, due to the mathematical relationship between the universe's structure and an organism's substructure. 
-Hiding files inside pictures: Put all the files to be hidden into a folder with the picture you want to store them in. As it will later be necessary to navigate to this folder, it is optimal to create the folder in a location easy to access, such as directly on the C:/ drive. 
    1. Compress all the files to be hidden into a compressed file, such as a .rar file. 
    2. In command prompt, first navigate to the folder. If the folder is named "NAME" and is located directly on the C:/ drive, type "cd\" and hit enter, and then type "cd NAME" and hit enter; this navigates to the folder. 
    3. Type "COPY /B picture.jpg + hiddenfiles.rar newpicture.jpg", where picture.jpg represents the image in which the files will be stored, hiddenfiles.rar represents the compressed folder containing the hidden files, and newpicture.jpg represents the new picture that will be created and which will contain the hidden files.
    -Retrieving the files: One way to access the files is to change the image's extension to a .rar extension, and then open the image with WinRAR. Another way is to right click on the image and select Open with WinRAR. In both ways, the hidden files appear in WinRAR.
-Brain waves: The billions of neurons that make up the brain communicate using action potentials that generate electrical activity; the macroscopic pattern of the total electrical activity follows an oscillatory pattern, hence brain waves.
    -Frequencies: Different frequencies of brain waves correspond to different brain functions and thoughts; the brain's electrical activity changes with the environment and the person's actions.
        -Delta: Delta brain waves range in frequency from 0.2 hz to 3 hz, and represent stage 4 (deep, dreamless) sleep. When delta is the dominant brain wave, the body is healing itself and resetting internal clocks. This state is one of full unconsciousness.
        -Theta: Theta brain waves range in frequency from 3 hz to 8 hz, and is the most receptive mental state. It represents light sleep or extreme relaxation. This brain wave pattern has been linked to mechanisms of learning and memory, or spatial reasoning and keeping track of one's environment.
        -Alpha: Alpha brain waves range in frequency from 8 hz to 12 hz, and represent an awake but relaxed state in which the subject is not processing large stimuli or amounts of information. This brain wave is dominant in the mornings, after awakening, and at night, before sleeping. Alpha activity has been linked to pain suppression, memory recall, and stress reduction.
        -Beta: Beta brain waves range in frequency from 12 hz to 27 hz, and represent a state of alertness; wide awake. This is the most common dominant brain wave pattern of the day, and most people are in beta wave states throughout the day. Low beta activity can lead to disorders such as depression and ADHD, and high levels have been linked to concentration, attentiveness, and high energy levels.
        -Gamma: Gamma brain waves are frequencies above 27 hz, and are the most energetic brain wave pattern. They are associated with problem solving, creativity, forming new ideas, language and memory, and learning. 
-80-20 rule: A common rule of thumb that, for many events, about 80% of the effect is caused by only 20% of the input. For example, in a pea garden, 20% of the pea plants contain 80% of the peas, and in many countries, the top 20% of the economy holds 80% of the wealth. In business, 80% of revenue tends to come from around 20% of clients.
-Pareto optimality: A specific allocation of resources and actions such that no actor can change his position (to increase his own utility) without decreasing another actor's utility. A pareto improvement is an action that benefits one player without reducing the well-being of any other players, so an alternate definition of pareto optimality is a state in which no pareto improvements exist. 
    -Intuition: As stated above, the formal definition of a Pareto optimal or Pareto efficient state is an allocation of resources among players such that no player can improve his position without making at least one other player worst off. Intuitively, a Pareto efficient state is the social optimum, because everyone is doing as well as they can do given that everyone else is also doing as well as they can do, ie the only way for a player to do better is by making someone else do worse, which from a societal standpoint, is not a net positive at all.
-Effects of cocaine: (formally known as benzoylmehtylecgonine, a derivative of the ecgonine family, which is a tropane alkaloid (alkaloid: naturally occurring compounds mostly composed of basic nitrogen atoms; tropane alkaloid: an alkaloid with a tropane ring. Most tropane alkaloids are stimulants or anticholinergic agents (substances that inhibit the receptors that reabsorb acetylcholine)) Cocaine is obtained from the leaves of the coca plant, hence coca + ine (suffix for alkaloid) = cocaine. Cocaine is a TRI, or triple reuptake inhibitor (prevents the reuptake (the departure of neurotransmitters after they are release) of serotonin, norepinephrine, and dopamine) and amplifies the mesolimbic reward pathway, making it addictive. High doses of cocaine, due to their blocking of sodium channels, lead to sudden cardiac death (instant death due to cardiac arrest).
          Cocaine is both hydrophilic as well as lipophilic (an uncommon combination due to the high concentration of hydrocarbons in lipids), which allows it to cross the blood-brain barrer very easily. 
    -General effects: Cocaine stimulates the central nervous system to a very high degree, with effects lasting from 15-30 minutes and up to an hour. During the high, it induces feelings of alertness, euphoria, well-being, and energy accompanied by high motor activity. During the comedown, users experience paranoia, anxiety ,and restlessness. Occasional use of cocaine () doesn't cause even minor physical or social problems.
        -Acute effects: Itching, tachycardia (higher than normal heart rate for extended periods of time), hallucinations, and paranoia. Overdoses cause hyperthermia and sharp increases in blood pressure, as well as arrhythmias (irregular electrical activity in the heart) and death. 
        -Chronic effects: Some side effects include insatiable hunger, body aches, sleeping disorders, and constant fatigue and sometimes depression with suicidal thoughts for heavy users. 
    -Quantities: Typically, a gram will be enough for 6 hours at a rate of one line per 30 minutes for a light to moderate user. For an entire night, some use a sixteenth (of an ounce) for themselves or an eighth or more for multiple people. In general, 2 grams for a night will be plenty. Prices range from $55-$65 per gram for mid-level quality; many dealers reduce prices for bulk buyers, such as $150-$200 for an 8 ball (an eighth of an ounce, or 3.5 grams) which comes out to $42-$57 per gram. 
        -Price discrepancies: When buying in large bulk, prices vary extensively depending on geographic location; cities used by cartels as distribution hubs have the cheapest product, with distance from these cities raising the price. In most places in the US, a kilogram of cocaine will cost around $16000 - $24000 (about $454/ounce - $680/ounce). In Puerto Rico, a kilogram may cost around $8000 - $12000 (about $267/ounce - $340/ounce). In cities such as New York and Atlanta, a kilogram will cost from $17000 to $20000 (about $482/ounce - $567/ounce).
-Chain of custody: In the capture and subsequent processing of suspects by the justice system, the chain of custody is the chronological trail of the evidence as it passes through seizure, custody, control, transfer, analysis, and finally disposition. The purpose of such a rigid and scrupulous system is to avoid future allegations of evidence tampering during the trial or sentencing of the suspect. It also ensures that evidence is not planted. 
-Effects of heroin: (formally known as either diacetylmorphine or diamorphine) is an opioid analgesic (pain reliever); note that "opioid" and "opiate" are not the same term; the latter, "opiate" refers to the alkaloid found in the resin (plant excretion) of the opium poppy plant. Roughly 87% of the world's supply of illicit raw opium comes from Afghanistan, although Mexico is another big producer whose production has increased 6-fold since 2007 (to 2011). Heroin is a depressant.
    -Opioid: Substances that bind to opioid receptors in the central and peripheral nervous systems, as well as in the gastrointestinal tract. These opioid receptors are G protein-coupled receptors (molecules inside a cell that activate a signal transduction pathway when they sense other molcules that are outside the cell; GPCRs specifically activate cAMP (cyclic AMP (adenosine monophosphate)) or phosphatidylinositol pathways)). Opioids act as ligands (A molecule whose binding to another molecule sets off a signal transduction pathway) to GPCRs.   
    -General effects: During the high, the main effect of heroin is the inducing of intense euphoria and relaxation. This occurs when the diacetylmorphine molecules reach the brain and are catabolized into morphine and 6-monoacetylmorphine (A.K.A. 6-MAM). The presence of 6-MAM causes heroin to induce more euphoria than other opioids, though this is only true if the drug is injected. 
        -Adverse side effects: Unaltered heroin doesn't cause any long-term effects other than physical dependence, constipation, and detrimental social effects. Average purity on the street varies from 30% to 50%; heroin on the border of Mexico and the US has purity ranging from 40% to 60$. Side effects of heroin include abscesses (collections of pus that accumulates under tissue due to the inflammatory response and turns black) and decreased kidney function. The act of heroin usage brings with it adverse effects as well, such as STDs or bacterial infections from sharing needles and poisoning from substances added to heroin to lower purity. The risk of overdose with heroin is extremely high, with ODs resulting in death. 
                     Some people mix cocaine and heroin; the substances is referred to as "speedball" if injected and "moonrocks" if smoked. This combination produces a high of very intense euphoria without the negative comedown effects of either drug, such as anxiety and depression, since heroin is a depressant while cocaine a stimulant. Cocaine effects wear off much quicker than do heroin effects, so if too much of either drug is used, the resulting overdose ends in fatal respiratory depression. 
    -Price: Heroin price has a lot of variation and depends on the purity of the product and distance from a major distribution hub. The most compressed (i.e. in rock form) and the browner, the more potent; "white china" heroin, or heroin with white/off-white/brown powder, is more pure than "black tar" heroin. Prices can vary with lower bounds of around $50/gram to $400/gram, with an average price of $200/gram. Heroin is often sold in "bags" of 100 mg, or 0.1 g; these "bags" are often severely cut down in purity, and therefore it can several for a heavy user to get high. Prices of bags run around $10. In major distribution hubs such as New York City, bags can cost as little as $6, or even $4 if purchased in bulk. 
    -Quantities: There is no general consensus on the dosage required to attain a high, as it varies widely from person to person depending on metabolism, size, brain function, etc., and also depends on the purity of the product as well. However, first-time users have a far smaller chance of overdosing if they first smoke the heroin through aluminum foil or other filters, rather than injecting it.
        -Recommended (approximate) doses for opioids: The below list is in the format "Formal name, street name, oral dose, insufflated (snorted) dose, smoked dose, rectal dose, injected dose"
            (Dextro)propoxyphene    Darvon  120 - 160 mg    60 - 100 mg 30 - 40 mg  120 - 160 mg    20 - 25 mg
            A-methylfentanyl    Alphamethylfentanyl 15 - 30 microg  7.5 - 15 microg 3.75 - 7.5 microg   15 - 30 microg  5 - 10 microg
            Alfentanil  Alfenta 2 - 4 mg    0.5 - 2 mg  500 - 1000 microg   2 - 4 mg    300 - 700 microg
            Allylprodine    Allylprodine    1 - 3 mg    0.5 - 1 mg  250 - 750 microg    1 - 3 mg    150 - 500 microg
            Bezitramide Burgodin    5 - 10 mg   2 - 5 mg    1 - 2.5 mg  5 - 10 mg   1 - 2 mg
            Buprenorphine   Suboxone    2 - 8 mg    0.5 - 4 mg  0.5 - 2 mg  2 - 8 mg    350 - 1350 microg
            Butorphanol Stadol  5 - 20 mg   2 - 10 mg   1 - 5 mg    5 - 20 mg   1 - 4 mg
            Carfentanyl Wildnil 1 - 3 microg    0.5 - 1.5 microg    0.25 - 0.75 microg  1 - 3 microg    0.17 - 0.5 microg
            Desmethylprodine    MPPP    10 - 25 mg  5 - 10 mg   3 - 6 mg    10 - 25 mg  1.5 - 4 mg
            Dextromoramide      Dimorlin    45 - 90 mg  15 - 45 mg  10 - 25 mg  45 - 90 mg  7.5 - 15 mg
            Dezocine    Dalgan  90 - 180 mg 30 - 60 mg  20 - 50 mg  90 - 180 mg 15 - 30 mg
            DIACETYLMORPHINE   HEROIN 50 - 70 mg    5 - 20 mg   5 - 20 mg   50 - 70 mg  5 - 10 mg
            Dihydrocodeinone    Vicodin 20 - 60 mg  10 - 30 mg  5 - 15 mg   20 - 60 mg  3 - 10 mg
            Dihydroetorphine    Dihydroetorphine    1 - 3 microg    0.5 - 1.5 microg    0.25 - 0.75 microg  1 - 3 microg    0.17 - 0.5 microg
            Dimorphone  Hydromorphone   5 - 8 mg    1 - 2 mg    1 - 2 mg    5 - 8 mg    800 - 1300 microg
            Diphenoxylate   Lomotil 5 - 10 mg   2 - 5 mg    1 - 2.5 mg  5 - 10 mg   1 - 2 mg
            Dipipanone  Diconal 30 - 60 mg  10 - 20 mg  7.5 - 15 mg 30 - 60 mg  5 - 10 mg
            Etorphine   Immobilon   5 - 10 microg   2.5 - 5 microg  1.25 - 2.5 microg   5 - 10 microg   0.8 - 1.6 microg
            Fentanyl    Actiq   150 - 300 microg    75 - 150 microg 40 - 75 microg  150 - 300 microg    25 - 50 microg
            Ketobemidone    Ketorax 5 - 15 mg   2 - 5 mg    1 - 4 mg    5 - 15 mg   1 - 2.5 mg
            Lefetamine  Santenol    3 - 6 mg    1 - 2 mg    750 - 1500 mg   3 - 6 mg    500 - 1000 microg
            Levacetylmethadol   Orlaam  30 - 50 mg  10 - 20 mg  7.5 - 12.5 mg   30 - 50 mg  5 - 8 mg
            Levomethorphan  Levomethorphan  30 - 60 mg  10 - 20 mg  7.5 - 12.5 mg   30 - 60 mg  5 - 10 mg
            Levorphanol Levo-Dromoran   2 - 4 mg    0.5 - 2 mg  500 - 1000 microg   2 - 4 mg    300 - 700 microg
            Loperamide  Imodium 2 - 4 mg    0.5 - 2 mg  500 - 1000 microg   2 - 4 mg    300 - 700 microg
            Meperidine  Demerol 180 - 300 mg    60 - 200 mg 45 - 75 mg  180 - 300 mg    30 - 50 mg
            Meptazinol  Meptid  150 - 300 mg    50 - 150 mg 40 - 75 mg  150 - 300 mg    25 - 50 mg
            Methadone   Methadone   5 - 10 mg   2 - 5 mg    1 - 3 mg    5 - 10 mg   800 - 1600 microg
            Methylmorphine  Codeine 75 - 125 mg 25 - 50 mg  20 - 35 mg  75 - 125 mg 12.5 - 20 mg
            Morphine    Morphine    15 - 30 mg  5 - 10 mg   4 - 8 mg    5 - 10 mg   2.5 - 5 mg
            Nalbuphine  Nubain  5 - 10 mg   2 - 5 mg    1 - 2.5 mg  5 - 10 mg   1 - 2 mg
            Naltrexone  Vivitrol    50 - 100 mg 25 - 50 mg  10 - 25 mg  50 - 100 mg 7.5 - 15 mg
            Nicomorphine    Vilan   5 - 10 mg   2 - 3.5 mg  1.5 - 3 mg  5 - 10 mg   1 - 1.5 mg
            Ohmefentanyl    Ohmefentanyl    0.5 - 1.5 microg    0.25 - 0.75 microg  0.125 - 0.375 microg    0.5 - 1.5 microg    0.08 - 0.25 microg
            Oripavine   Oripavine   15 - 30 mg  5 - 10 mg   4 - 8 mg    15 - 30 mg  2.5 - 5 mg
            Oxycodone   OxyContin   10 - 20 mg  5 - 10 mg   2.5 - 5 mg  10 - 20 mg  2 - 4 mg
            Oxymorphone Opana   6 - 10 mg   2 - 5 mg    1.5 - 2.5 mg    6 - 10 mg   1 - 1.5 mg
            PEPAP   PEPAP   4 - 8 mg    2 - 4 mg    1 - 2 mg    4 - 8 mg    600 - 1400 microg
            Papaver-somniferum  Opium   300 - 500 mg    150 - 250 mg    100 - 125 mg    300 - 500 mg    
            Paramorphine    Thebaine    100 - 300 mg    50 - 150 mg 25 - 75 mg  100 - 300 mg    15 - 30 mg
            Pentazocine Talwin  50 - 100 mg 20 - 60 mg  10 - 25 mg  50 - 100 mg 8 - 16 mg
            Phenazocine Narphen 4 - 8 mg    2 - 4 mg    1 - 2 mg    4 - 8 mg    600 - 1400 microg
            Piritramide Dipidolor   10 - 25 mg  5 - 10 mg   3 - 6 mg    10 - 25 mg  1.5 - 4 mg
            Prodine Nisentil    50 - 100 mg 25 - 75 mg  10 - 25 mg  50 - 100 mg 8 - 16 mg
            Remifentanil    Ultiva  150 - 300 microg    75 - 150 microg 40 - 75 microg  150 - 300 microg    25 - 50 microg
            Sufentanil  Sufenta 20 - 45 microg  10 - 25 microg  5 - 10 microg   20 - 45 microg  3 - 7.5 microg
            Tapentadol  Nucynta 100 - 200 mg    50 - 100 mg 25 - 50 mg  100 - 200 mg    15 - 30 mg
            Tilidine    Valoron 75 - 150 mg 25 - 50 mg  20 - 40 mg  75 - 150 mg 12.5 - 25 mg
            Tramadol    Ultram  100 - 150 mg    50 - 75 mg  25 - 40 mg  100 - 150 mg    15 - 25 mg
-Colored fire: When the fuel of combustion reactants is modified, the flames can be engineered to burn in different colors. 
    -Blue: Heating chemicals such as copper chloride produces blue flames. Most types of alcohol, especially ethanol (found in rum and vodka) and isopropyl alcohol, burn blue. 
        -Copper chloride should first be dissolved in water, with the flammable materials then being soaked in that water and then allowed to dry. To create copper (I) chloride at home, dissolve a piece of copper in a 3% solution of hydrogen peroxide and then add enough muriatic (hydrochloric) acid to create 5% HCl solution. 
    -Green: Boric acid can be used to turn fire green; it can be found at most pharmacies as a disinfectant. Some household insecticides, such as Enoz Roach Away (99% boric acid), contain boric acid. Lastly, another source of boric acid is Heet Gas Line Antifreeze, which is sold among most automotive chemicals. 
        -Dissolve boric acid into the liquid Heet and then ignite the container. 
    -Red: Red fire can be induced using strontium nitrate, potassium nitrate, potassium perchlorate, charcoal, and other ingredients; most strontium salts burn red. Strontium nitrate works best, and can be harvested from road flares. 
        -To obtain strontium nitrate from road flares: Cut open the cardboard tube inside the flare and pour the powder into a bowl; this powder is strontium nitrate. Sprinkle this powder (sparingly) over a campfire to make it turn red.
-Smelling salts: (A.K.A. spirit of hartshorn or sal volatile) Chemical compounds used to induce consciousness. Most smelling salts contain ammonium carbonate, a colorless (sometimes white) crystalline solid ( (NH_4)_2 CO_3 ). The salts release ammonia gas, which when smelled, induces an inhalation reflex (causes the muscles that control breathing, such as the diaphragm, to work faster) by irritating the mucous membranes inside the nose and lungs. This elevates heart rate, blood pressure, and brain activity by activating the sympathetic nervous system. 
    -Risk: In large amounts and at exposure for long periods of time, ammonia gas is toxic. 
-Jury nullification: In criminal cases, juries can choose to set guilty defendants free or condemn innocent defendants. This is the consequence of two provisions in the Constitution: firstly, that juries can't be punished for a "wrong" decision, and secondly, that once a defendant is proven not-guilty, he can't be tried again for the same crime (no double jeopardy). Most juries don't know about jury nullification because discussing it in the wrong contexts can lead to imprisonment. 
    -Fugitive slave law: Most pro-jury nullification groups use this law as an example for the merits of jury nullification. During the mid 1800s, northern juries would refuse to convict escaped slaves and set them free. Anti-jury nullification groups bring up the example of Southern juries refusing to convict lynch mobs, in the same time period. 
    -Although juries can set free guilty defendants in part due to the no-double-jeopardy provision of the Constitution, no analogous provision exists for the guilty, and so if a jury convicts an innocent man, the judge can overrule the decision. Even if this isn't the case, the guilty defendant can continue to appeal to other courts. 
    -95% of criminal cases end in a plea bargain (a deal between the defense counsel and prosecution counsel) rather going to trial. 
-Extradition: The process by which one country transfers a fugitive of another country back to that country. The default position in this situation is that a foreign country harboring a fugitive has no obligation to turn him over, since such an obligation would be a violation of the fundamental principle of sovereignty that a state controls the land and people within its borders. Therefore, extradition in the world is carried out through previously set up extradition treaties, though no country in the world has extradition treaties with every other country in the world. 
    -A country may refuse extradition in any of the following cases, as well as other specific situations:
        -No dual criminality: Dual criminality is the condition that the fugitive in question committed a crime that is illegal in both the requesting state and the requested state; if the fugitive's crime is not considered a crime in the requested state, that state will probably refuse extradition. 
        -Political prisoners: Most countries refuse to extradite political fugitives. 
        -Disagreement on punishment: If the extradition of a fugitive would result in a form of punishment for that fugitive that the requested state considers objectionable, that state will probably refuse extradition. Common examples of such forms of punishment include the death penalty, torture, inhuman treatment, etc.
        -Jurisdiction: If the requested state doesn't think that the requesting state has the jurisdiction to convict the fugitive of the crime to begin with, extradition will most likely be denied. 
    -Extradition in the US: The US has extradition treaties with over 100 countries. Most of these treaties are dual criminality treaties. 
        -List of countries with whom the US does not have an extradition treaty: Afghanistan, Algeria, Andorra, Angola, Armenia, Bahrain, Bangladesh, Belarus, Botswana, Brunei, Burkina Faso, Burma, Burundi, Cambodia, Cameroon, Cape Verde, the Central African Republic, Chad, China (except Hong Kong), Comoros, Congo (Kinshasa), Congo (Brazzaville), Djibouti, Equatorial Guinea, Eritrea, Ethiopia, Gabon, Guinea, Guinea-Bissau, Indonesia, Ivory Coast, Kazakhstan, Kuwait, Laos, Lebanon, Libya, Madagascar, Maldives, Mali, Marshall Islands, Mauritania, Micronesia, Moldova, 
         Mongolia, Morocco, Mozambique, Namibia, Nepal, Niger, Oman, Qatar, Russia, Rwanda, Samoa, So Tom & Prncipe, Saudi Arabia, Senegal, Somalia, Sudan, South Sudan, Syria, Togo, Tunisia, Uganda, Ukraine, the United Arab Emirates, Uzbekistan, Vanuatu, Vatican City, Vietnam, Yemen, and the countries formerly part of Yugoslavia: Bosnia and Herzegovina, Croatia, Kosovo, Macedonia, Montenegro, and Serbia.
            -The major countries with whom the US doesn't have an extradition treaty are Russia, China, Saudi Arabia, and the UAE.
-Contraction mapping: A function that, informally, shrinks the distances between input and output. That is, for a function f with two inputs in its domain, x and y, f can be classified as a contraction mapping if |x-y| < L * |f(x) - f(y)|, for a constant L such that 0 < L < 1.
-Survivorship bias: The error in logical analysis made when one concentrates on the "surviving objects" of the process in question and thus does not take into account the "deceased". This can lead to unfounded and baseless optimism or simply wrong conclusions. 
    -In finance, most statistics and analyses are computed with firms that currently exist; since firms that drive themselves bankrupt or are subject to takeover, i.e. unsuccessful firms, are not included in the assessment, the conclusion will be overly optimistic since only the successful firms that survived were taken into account. 
    -In WW II, the British air force noticed that the planes that returned from battle suffered damage in the wings and tails, so they increased their fortification in those areas. However, this did not reduce the number of planes that were being lost. The correct solution was to fortify the other places that weren't hit; since the planes that came back were survivors, they took damage in areas where they could afford to take damage, i.e. the wings and tails, so all the other, weaker areas of the plane need to be fortified.
-Micromort: A unit of risk equivalent to a 1 in a million chance of death. Activities that gain you 1 micromort:
    -Drinking 0.5 liter of wine (cirrhosis of the liver)
    -Smoking 1.4 cigarettes (cancer, heart disease)
    -Spending 1 hour in a coal mine (black lung disease)
    -Spending 3 hours in a coal mine (accident)
    -Living 2 days in New York or Boston in 1979 (air pollution)
    -Living 2 months with a smoker (cancer, heart disease)
    -Drinking Miami water for 1 year (cancer from chloroform)
    -Eating 100 charcoal-broiled steaks (cancer from benzopyrene)
    -Eating 40 tablespoons of peanut butter (liver cancer from aflatoxin B)
    -Eating 1000 bananas (cancer from radioactive 1 kBED of Potassium-40)
    -Traveling 6 minutes by canoe (accident)
    -Traveling 6 miles by motorbike (accident)
    -Traveling 17 miles by walking (accident)
    -Traveling 10 miles (or 20 miles) by bicycle (accident)
    -Traveling 230 miles (370 km) by car (accident) (or 250 miles)
    -Traveling 6000 miles (9656 km) by train (accident)
    -Traveling 1000 miles (1600 km) by jet (accident)
    -Traveling 6000 miles (10,000 km) by jet (cancer due to increased background radiation)
    -Traveling 12,000 miles (19,000 km) by jet in the United States (terrorism)
-Lucid dreaming: 
    -Process for initiation of lucid dreaming:
        1. Keep a dream journal next to your bed; as soon as you awaken, write out as much as you can remember about your dream. The writing process needs to happen immediately after consciousness is re-aroused; even one or two minutes of delay can erase a substantial amount of the dream from memory. It's also best to minimize movement after waking, since the activation of motor neurons clears the memory of a dream. 
            -The first few days, one will typically remember enough of the dream to write out a few lines, but given a couple weeks, one will achieve multiple pages of description.
        2. Perform many (10+) personalized reality checks every day while awake. 
            -Common reality checks: (1) Push your finger into the palm of the other hand; if the finger passes through, you are dreaming. (2) Squeeze your nose shut with your fingers; if you are still able to breathe, you are dreaming. (3) Watch your hands intently; if they have more than 5 fingers each, or if the fingers randomly and unnaturally bend, you are dreaming. (4) Look in the mirror; if you see a distorted, changing, or non-existent reflection, you are dreaming. (5) 
        3. Techniques for inducing LD: 
            -MILD technique: (Mnemonic-Induction of Lucid Dreaming) A technique that forces one to awaken in the middle of the night and then return to sleep; the blurring of the distinction between consciousness and sleep helps induce lucid dreaming. 
                -Procedure: Set an alarm for 5 hours after going to sleep. Upon awakening, remain awake for 10-15 minutes (or longer, from 30 minutes - 1 hour), the whole time ingraining the intention of lucid dreaming into your brain by constantly thinking about it, repeating the phrase "I will lucid dream when I go to sleep", and other ways of convincing yourself to lucid dream. After 15 or so minutes, go back to sleep. 
    -Stabilization of the dream: Many people, upon attaining lucidity, either awaken from the shock of finding out they are dreaming or attempt to manipulate the dream to an extent to which they are not yet adapted (e.g. trying to fly on one's second or third lucid dream) which jolts them awake. To maintain a lucid dream once achieved, stabilizers can be employed, such as (1) Rubbing hands together; (2) Spinning around in a circle (after you've stopped spinning in the dream, the environment will probably have changed); (3) Loud yelling
    -Some research has gone into supplements to aid the onset of lucidity. The drug Calea zacatechichi, often shortened to "Calea" and now formally and taxonomically synonymous with Calea ternifolia, is said to induce incredibly vivid, intense, and dramatic dreams. They work best when taken directly before REM sleep, so the optimal time to ingest them (common dose is 1 capsule, some increase the dose to 2 for a greater effect) is 4-5 hours after you've already been asleep, and then going back to sleep after consumption. So far, there are no reported side effects other than an association with slight drowsiness upon wakening. Taking the pills in conjunction with other sleeping medication renders the Calea impotent.
        -Methods of ingestion: The plant can be ground into tea, smoked, or ingested a variety of other ways. To ground into tea, crush 5 grams of Calea into fine powder and pour 100 - 150 mL of boiling water over it. Let it soak for 30 minutes and then filter, then drink.
    -Sleep paralysis: Sleep paralysis is a phenomenon that occurs when, the brain has awakened from sleep, the body has not yet disabled its paralyzing function. During normal REM sleep, the brain paralyzes the muscular system to prevent subjects from acting out their dreams (and potentially causing themselves harm); when this natural paralytic does not rescind even after the brain awakens, subjects are awake but completely unable to move or even talk for the next one to three minutes. This can often lead to fear; the process itself also brings with it induced waves of dread as well as hallucinations, usually of other malevolent presences, such as demons, sometimes sitting on one's chest. Breathlessness is another common effect. 
-Disaster preparedness: Always have the following items in case of disaster:
    -Food and water:
        -1 gallon of water per person per day
        -Water filtration device or tablets
        -Nonperishable food
            -Peanut butter
            -Whole-wheat crackers   
            -Trail mix
            -Cereal
            -Power/granola bars
            -Dried fruits (e.g. apricots, raisins, etc.)
            -Canned meat (chicken, tuna, turkey)
            -Canned vegetables (carrots, green beans, peas)
            -Canned soup and chili
            -Multivitamins
        -Can opener
    -Shelter: 
        -Camping stove and propane
        -Sleeping bags
        -Sturdy hiking boots
        -Hats
        -Boy Scout Handbook
    -Tools: 
        -Flashlight
        -Portable radio
        -Batteries
        -Multipurpose pocket knife
        -Crowbar
        -Handsaw
        -Sledgehammer
        -Sturdy gloves and transparent goggles
        -Rope
        -Duct tape
        -Fire extinguisher
        -Matches
    -First aid: Store in a sturdy, waterproof, nylon bag of a bright color. Store things in sealed ziplock bags.    
        -20-30 pairs of Latex gloves
        -2-3 rolls of cloth adhesive tapes (1" or 2")
        -2-3 rolls of plastic adhesive tapes (1" or 2")
        -Sterile gauze pads 
        -Band-aids
        -Roller bandages
        -Cotton swabs
        -Mild disinfectant
        -Hydrogen peroxide
        -Antibiotic ointment
        -Sterile water
        -Tylenol and aspirin
        -Anti-diarrhea medicine     
-Cognitive dissonance: Discomfort stemming from the holding of contradictory beliefs, ideas, or perceptions simultaneously. 
-Purpose of sleep: Some researchers believe that sleeping induces the process of cerebrospinal fluid coming up into the brain and cleansing the axons and dendrites of the neurotoxic by-product built up by the cells through daily use. 
    -By-products: The neurotoxic by-products include beta-amyloid protein (the main component of amyloid plaques (insoluble fibrous protein aggregates with similar structures) found in the brains of Alzheimer's sufferers).
    -Glymphatic system: A term for the process of picking up and expelling the above mentioned neurotoxic by-products. The cerebrospinal fluid circulates throughout the brain tissue, flushing waste into the bloodstream. The waste then makes it to the liver for detoxification.
-Currency manipulation: The altering of a currency's true value (relative to other currencies), away from the true value, in the foreign exchange market in order to benefit the host country. A country can artificially inflate its currency, making it weaker and therefore making its products cheaper, or artificially deflate it. 
    -China: China buys and hoards American dollars cutting supply and therefore increasing demand for the dollar. As the dollar appreciates in value, the Chinese currency, yuan, becomes relatively cheaper and therefore Chinese products are relatively cheaper than their American counterparts, boosting exports and benefiting China's export-driven economy.
-Sputtering: The phenomenon in which an object is bombarded with highly energetic particles, causing the object's atoms to be ejected from the object.
-Pythagorean means: A group of three algorithms that serve as a measure of average for different sets of data, each outputting the "typical" value of the set of data inputted by taking into account the characteristics of that data, such as range and mode. 
    -Arithmetic mean: Given a data set {x1, ..., xn}: arithmetic mean = (x1 + x2 + ... + xn)/n
        -This type of mean is used most often to express central tendencies, but is not a robust statistic (it is highly affected by outliers).
    -Geometric mean: Given a data set {x1, ..., xn}: geometric mean = (x1 * x2 * ... * xn)^(1/n)
        -This type of mean is used most often when trying to find the central tendency of multiple processes, each with a different range (e.g. the average of scores on two scales: a scale from 1 to 5 and one from 1 to 100).
    -Harmonic mean: Given a data set {x1, ..., xn}: harmonic mean = n / (1/x1 + 1/x2 + ... + 1/xn)
        -This type of mean is used to mitigate the effect of large outliers or pronounce the effect of small ones. It's very commonly used to average speeds or rates of something.
-Godel's Incompleteness Theorem: No axiomatic system (A set of assumed axioms and all the statements that logically arise from those axioms) capable of arithmetic exists that is both consistent and complete. The second incompleteness theorem asserts that, in layman's terms, in any axiomatic system capable or arithmetic, it is impossible to prove that a statement cannot be proven.
    -Terminology:
        -Consistence: An axiomatic system is consistent if no permutation of the axioms and all logically deduced statements from those axioms leads to a contradiction, i.e. there does not exist a statement in that axiomatic system that is both true and false.
        -Completeness: All statements within an axiomatic system (which are necessarily either true or false) can be proven as either true or false. 
    -Proof: The proof is essentially a glorified Liar's Paradox. 
        1. Given a set of computable axioms, define the set P of all sentences can be proved using those axioms (i.e. are provable, and therefore true). 
            -Since the set of axioms can be computed, so can the set of proofs that use those axioms, and therefore so can the set of provable theorems (i.e. P). 
        2. Let s be the sentence "This sentence is unprovable". 
            -If s is false, then s must be provable. However, is any sentence is provable, it, by definition, must be able to be proven true, and therefore is true. This contradicts the assumption that s is false.
            -Therefore, s must be true, which implies that there now exists a true statement within the definable set of provable theorems resulting from a computable set of axioms that cannot be proven to be true. Thus, the system is not consistent.
-Upper atmospheric lightning: (More accurate term: transient luminous event (TLE)) Electrical breakdown events that occur at altitude much higher than the altitude level of storm clouds (cumulonimbus). TLEs are closer to electrically induced plasma eruptions than lightning and are very brief, lasting from a few milliseconds to 2 seconds. They appear as bright flashes.
    -Types of TLEs:
        -Sprites: Large-scale electrical disturbances that can take a multitude of shapes. They are caused by positive lightning discharging between a storm cloud and the ground. They appear as reddish orange flashes of light. Types of sprites:
            -Jellyfish sprite: 30 miles in diameter
            -Carrot sprite
            -C sprite: (Column sprite) Sprites that are completely vertical.
        -Blue jets: Blue jets originate at the top of a thundercloud and project up to the beginning of the ionosphere (25-30 miles in altitude). They are blue and NOT triggered by lightning. The color comes from the emission spectra of both neutral and ionized nitrogen.
        -ELVES: (Emissions of Light and Very low frequency perturbations due to Electromagnetic pulse Sources) A dim and flat glow that expands to 250 miles in diameter, occurring in the middle of the ionosphere (62 miles in altitude). They are caused by the excitation of nitrogen molecules due to electron collisions. 
-Painkiller mechanism: Drugs that prevent the sensation of pain (nocioception). Two types:
    -Opioids: Drugs used to treat severe pain. These drugs first interfere with the transmission of pain signals to the brain. They then work within the brain to alter the perception of the little pain it does feel. 
    -Non-steroidal anti-inflammatory drugs: (A.K.A. NSAIDs) Over-the-counter drugs for mild pain relief. The drugs bind to cyclooxygenase-2 and prevent them from releasing prostaglandins.
        -Cyclooxygenase-2: When cells are damaged, they produce cyclooxygenase-2, which in turn releases prostaglandins. Prostaglandins are signal transduction molecules that do two things: (1) They send pain signals to the brain, and (2) They cause the blood flowing in the area of the damaged wounds to release plasmic fluid, which engorges the area (inflammation) so the cells are protected from further damage. Most NSAIDs have a base drug: the naturally-occurring substance salicin (the body metabolizes salicin to salicyclic acid). Although an effective pain reliever, it causes severe digestive problems.  Aspirin is a derivative substance, acetylsalicylic acid (ASA), with less severe side effects.                      
        -COX-2 Inhibitors: Prostaglandins actually form a class of enzymes with two signal transduction pathways: COX1 and COX2. Traditional NSAIDs inhibit the entire class from being produced. However, only COX2 is responsible for pain and inflammation; the inhibition of COX1 only leads to adverse side effects. COX2 drugs only stop production of COX2, thereby producing the same effect as NSAIDs but without as many side effects.
-Rubens' Tube: A tool used in physics in the early 1900s to demonstrate standing waves. The apparatus uses the relationship between sound waves and air pressure to graphically display a standing wave from flames. 
    -Apparatus: A pipe is perforated along the top with evenly spaced holes, with one end of the pipe attached to a speaker (airtightly) and the other end attached to a supply of flammable gas (e.g. propane; also attached airtightly). When a continuous supply of flammable gas fills the tube, with gas escaping through the perforated holes at the top, the escaping gas can be ignited. When the speaker is turned on, the standing wave creates alternating points of low air pressure (rarefactions) and points of high air pressure (compressions). More gas escapes out of holes near compressions, and so the flame is taller there than it is at holes near rarefactions. 
-Libet experiment: An experiment conducted in the field of neuroscience to test the notion of free will. In the 1980s, Benjamin Libet conducted an experiment in which subjects' brain activity were monitored after they were asked to randomly flick their wrist. Specifically, the readiness potential (the build-up of electrical signal in an axon in the brain's neurons that occurs before the action potential) was investigated. Subjects also recorded the exact time when they said they felt the actual conscious will to act, i.e. to flick their wrist. The results were that the unconscious brain activity (i.e. readiness potential) associated with conscious decision to commit an action came about a half second before the conscious decision. This implies that, on some level, all decisions are first made subconsciously and then transferred into the conscious mind to give the illusion that 
        the conscious mind is in control. 
-Post-traumatic growth: Positive psychological changes that arise from being put in a situation that requires a struggle for survival in extremely difficult circumstances. One cause is the forced development of effective psychological coping mechanisms that help subjects deal with stress in civilization, long after they served their use in the survival situation. 
    -Characteristics: People who experience this tend to have a better outlook on life, a more rigid and changed set of priorities, more intimate relationships, more personal strength, and emotional resilience.
-Isomer: Molecules with the same atoms and amounts of each atoms (i.e. the same chemical formula) but with different three-dimensional structures. 
    -Structural isomers: These isomers have atoms in different places, with different bond structures.
    -Stereoisomers: These isomers have the same bond structures, but different geometric positions. 
        -Enantiomers: Molecules that are exact mirror images of each other (the same way human hands are mirror images of each other).
-Kelly Criterion: A methodology for determining the optimal amount to wager in a betting situation in which the odds are in one's favor.
    -Simplified (and more commonly known) version: Assuming a probability p for winning a profit b times your wager, the optimal amount to bet is ( pb - (1 - p) )/b
        -Numerator: This is known as the gambler's edge. The top of the fraction is simply the expected value, which is equal to (probability of winning)(profit if you win) - (probability of losing)(loss if you lose) = (p)(b) - (q)(1), where q = probability of losing = 1 - p and the amount lost is 1 because b is in terms of the amount wagered.
        -Denominator: This is known as the gambler's odds, the ratio of the amount won to the amount lost. 
        -Derivation: Assuming that the average rate of return is given by exp( E(ln(X)) ), as noted in the "Average rate of return..." section below, where X is a random variable describing how much you win or lose and E(ln(X)) is the expected value of the natural logarithm of X. 
            -Let the bet be such that a win, with probability p, multiplies one's wager by b, and a loss simply causes one to lose one's wager. Let x = fraction of wealth to bet. Then, to maximize the average rate of return, we need to maximize exp(E(ln(X))), which is the same as maximizing E(ln(X)) = p*ln(1 + bx) + (1 - p)*ln(1 - x)
             To maximize, set the derivative equal to 0: d/dx( E(ln(X)) ) = 0. This implies that (pb)/(1 + bx) - (1 - p)/(1 - x) = 0. Therefore, x = (pb - (1 - p))/b
    -Little-Oh notation: A function is little-oh of another function is the first function is much smaller than the second, in the long run. For example, 4x = o( x^2 ), x + 5 = o( e^x), 1/x = o(x), etc.
        -f(x) = o( g(x) ) if and only if the limit as x approaches infinity of |f(x) / g(x) | = 0
    -Average rate of return for betting a fixed fraction of one's net worth: If one consistently bets a predetermined fixed fraction of one's current amount of wealth, and wins with probability p, the average rate of return is derived as follows:
        1. Starting with initial wealth w0, the wealth after n bets, w_n, is given by w_n = (w0)(X_1)(X_2)(...)(X_n), where X_i (for 1 <= i <= n) is the binomial random variable determing the win or loss.
        2. w_n = (w_0)( exp( ln(X_1) + ln(X_2) + ... + ln(X_n) ) )
        3. By the Law of Large numbers, the sum of n random variables is approximately equal to the random variable's expected value, denoted by E(X). The error in this approximation is o(n), so we can now write that X_1 + ... + X_n = n*E(X) + o(n)
        4. Using ln(X) as our random variable, we apply the Law of Large numbers to obtain w_n = (w_0)(exp( E( ln(X) ) + o(n) ) = (w_0)(exp( n*E(ln(X)) + n*o(1) ) = (w_0)(exp( n*(E( ln(X) ) + o(1) )), because o(n) = n*o(1).
        5. In the long run, o(1) approaches 0, so we can approximate w_n = (w_0)(exp( E(ln(X)) )^n), with a rate of return of w_n - 1. The wealth after n bets is equal to the initial wealth times the nth power of the rate of return.
            -Note that E(ln(x)) = (p)(ln(1 + fraction of wealth bet)) + (1 - p)(ln(1 - fraction of wealth bet)), because if one wins, one's new wealth equal to the old wealth multiplied by 1 + fraction of wealth bet, i.e. 1 + winnings, and if one loses, one's new wealth is equal to one's initial wealth multiplied by how much wealth is remaining, given by 1 - fraction of wealth bet, i.e. 1 - losses.
    -Variance: In many situations, the rate of return can be positive and even high, but the situation has high variance. This means that although the wins will overcome the losses over an extremely large amount of trials, for trials within a few orders of magnitude, large variation in betting can mean large losses. For example, is one has a 0.6 probability of winning a bet (which doubles the money bet) and decides to bet 1/3 of his current wealth each round, the average rate of return is exp( (0.6)(ln(4/3) - (0.4)(ln(2/3)) ) - 1 = 1.047771%. This means every 50 bets, the return on investment is approximately 70%. However, on the off chance that one of the bets (out of the 50) expected to win happened to lose instead, one has now halved the amount of money one had. There's a 49.3% chance of this happening, and so one now loses money in this short-term scenario. 
          So, in the short-term, variation matters.
    -In general, compute and then maximize E(ln(X)).
        -Example: Let there exist a wager in which there is a 5% chance of winning 20 times your bet, and a 20% chance of receiving a 10% return on your wager; if you lose, you lose your wager. Then, E(ln(X)) = (1 - (0.2 + 0.05))*ln(1 - x) + 0.2*ln(1 + 0.1x) + 0.05*ln(1 + 19x). d/dx(E(ln(X))) = -0.75/(1 - x) + 0.02/(1 + 0.1x) + 0.95/(1 + 19x), which simplifies to the quadratic 0.22 - 14.82x -1.9x^2 = 0, which has positive solution at x = 0.0148166..., meaning the optimal amount to bet is 1.481% of your wealth.
        -With 2 outcomes, we solved a linear equations, and with 3 (above) we solved a quadratic. As bets get more complex, higher-order polynomials are used, and so approximation methods for finding the zeros may be necessary. 
        -Some suggest betting less than the Kelly optimal to account for human error, which tends to overestimate one's edge, and to account for the tendency of Kelly optimals to be theoretically optimal in the long-term but to also have very high variances.
-War of the currents: A scientific and business rivalry in the 1880s between George Westinghouse and Thomas Edison over the promotion of alternating current (AC) and direct current (DC), respectively. 
    -Direct current: Current that flows steadily in one direction, typically with high (and constant) magnitude of current, and low voltage. 
        -DC is used by home appliances, batteries, and electronics, since these require low voltages.
    -Alternating current: A rotating magnet causes the current (and its magnitude) to oscillate with time, typically (in the US) at a frequency of 60 Hz. This allows for very high voltage transmission at low current (amplitudes), which means that the wires used to transport AC can be very thin, minimizing energy loss in transmission. The energy loss in transporting the electricity over large distances is huge for DC.
        -AC is used to transport energy over large distances, and goes directly to wall sockets.
-Bremermann's limit: A theoretical maximum for a given mass for the processing speed of computation for any system, mechanical, organic, or otherwise, capable of computation. The limit is 1.356 E(50) bits/s/kg
    -Theory: Sending a bit of information (defined in computer science as a unit of information with two logical values and in information theory as the uncertainty in a binomial random variable) requires some energy; it is impossible to transmit information at no energy. Because energy is quantized, according to Heisenberg's Uncertainty Principle, according to the equation E = hf, where h = Planck's constant and f = frequency in bits/s, and the energy in a system of mass m is given by E = mc^2, where c = speed of light, we derive f = c^2/h * m. Therefore, the frequency is proportional to the mass by a factor of c^2/h; this factor is Bremermann's limit.
-Koch snowflake: A fractal with infinite perimeter by finite area. It is created by iterating over a triangle: first, an equilateral triangle is formed. Then, divide each of the (three) sides of the triangle into three sections, and construct (three) new smaller equilateral triangles on top of each side using the middle section as a base. The second iteration looks like David's star. Repeat the process for each of the (now six) equilateral triangles indefinitely.
    -Perimeter: Each iteration, the number of sides on the entire figure increases by a net factor of 4 (because for each of the original sides, the creation of a new smaller triangle adds two more sides from the triangle and two more from the first and last sections of the original side; since the middle section, i.e. the base of the smaller triangle, doesn't count as perimeter, we have a net increase of 4 sides per original side). However, each of the new sides has a side length that's 1/3 of the original side, since the side was split up into three sections. Therefore, the perimeter increases by a factor of (4/3) each iteration. Iterating indefinitely, perimeter = limit as n approaches infinity of (P_0)(4/3)^n = infinity.
    -Area: The area must be bounded by the circle that circumscribes the first-iteration triangle, since none of the further iterations exceed this circle. This is because of how much smaller each new iterate is from the old one. Therefore, the area is bounded.
-Pollard's rho algorithm: An algorithm for factoring composite numbers; works best when the factors are relatively small. 
    -Algorithm: 1. Create a function of one variable, f(x), and then define the recursive sequence x_(k + 1) = f(x_k). Choose a starting value, x_0. Each subsequent value is iterated over that starting value, i.e. x_1 = f(x_0), x_2 = f(x_1) = f(f(x_0)), x_3 = f(x_2) = f(f(f(x_0))), ...
            -This function is typically of the form a*x^2 + b
            -f(x) must be modulo d, where d is a factor of n (that we don't yet know).
           2. Compute the absolute difference between an even iterate (e.g. x_2, x_4, x_6, ...) and that iterate's half (e.g. |x_2 - x_0|, |x_4 - x_2|, etc.).
           3. Compute the greatest common divisor between the absolute difference calculated in step 2 and the number you're factoring, (usually using Euclid's algorithm).
            -If the greatest common divisor is 1, repeat steps 2 and 3 with the next pair of iterates (where one has twice the index of the other).
            -If the greatest common divisor is some integer greater than 1, that integer is a factor of the inputted number.
            -If the greatest common divisor is the inputted number itself, the algorithm fails and a new recursive sequence (with a new function) needs to be used.
    -Derivation: The idea behind the algorithm is the premise that if an algorithm to factor large integers quickly can't be found, then try exploiting Euclid's algorithm to find the greatest common factor of two integers, which is of course also a factor of both integers. Rather than randomly select integers to input into the greatest common divisor algorithm (with the number to be factored as well), the method would converge faster given some recursive sequence. Since the function is modulo d, at some point we will arrive at two inputs a = b (mod d), which implies that f(a) = f(b) (mod d). When this happens, the entire sequence will begin looping back on itself (this is where the algorithm's name comes from; the numbers progress sequentially and then loop back to a starting point, which graphically looks like the Greek letter rho). 
            Therefore, we will eventually arrive at a point when the sequence has passed through a number of loops; call this value x_k. Since x_k occurs after a number of loops, k is an integer multiple of l, where l is the length of the loop. Therefore, 2k is also an integer multiple of l. Therefore, x_(2k) = x_k (mod d). This means that the difference between x(2k) and x_k is an integer multiple of d.
    -http://www.csh.rit.edu/~pat/math/quickies/rho/
-Euclid's algorithm: An algorithm to find the greatest common factor between two numbers. The idea behind Euclid's algorithm is that the greatest common factor of two numbers is equal to the greatest common factor of the smaller number and the difference between the smaller number and the larger number. If these successive differences are computed over and over, eventually the two numbers will be equal, and that number is the greatest common factor. 
-User input in Java: To allow users to input information, use the Scanner class. This class is a simple text scanner that allows the user to read in and parse primitive data types, as well as Strings. 
    -Constructor: Call the constructor to first create a Scanner object that can then call methods in the Scanner class (the methods are non-static).
        -To pass an InputStream: Scanner s = new Scanner(System.in)
        -To pass a file: Scanner s = new Scanner(new FileReader("myFile"))
    -Methods: The following methods in the Scanner class return various kinds of tokens; a token is a string of text that leads up to a whitespace (a blank, space, or return). Thus, spaces and new lines act as whitespaces and separate tokens.
        -int nextInt(): Returns the next token as an int; if the next token isn't an int, the InputMismatchException is thrown.
        -int nextLong(): Returns the next token as a long; if the next token isn't a long, the InputMismatchException is thrown.
        -float nextFloat(): Returns the next token as a float; if the next token isn't a float, the InputMismatchException is thrown.
        - double nextDouble: Returns the next token as a double; if the next token isn't a double, the InputMismatchException is thrown.
        -String next(): Returns the next token as a String.
        -String nextLine(): Returns the rest of the current line; does not jump to new lines. If this method is called at the end of a line, it will return a blank String, not the next line.
        -void close(): Closes the scanner.
    -Boolean methods: Methods used to verify information about the input.
        -boolean hasNextLine(): Returns true if the scanner has another line left in the input.
        -boolean hasNextInt(): Returns true if the scanner has another int left in the input.
        -boolean nextFloat(): Returns true if the scanner has another float left in the input.
    -toCharArray(): A method that is called on a String and returns a character array whose length is that of this String and whose elements are corresponding characters in the String. This can be very useful for parsing, searching through, or otherwise manipuLating inputted Strings.
        -Located in Java.lang.String
-Net neutrality: A fundamental though controversial principle of the Internet that all Internet service providers (ISPs) and the governments that control access to the web ought to treat all data equally irrespective of its sender, recipient, content, platform, mode of transport, etc. The principle is meant to be an extension of the idea of a common carrier (Any entity that transports goods on behalf of other entities, and is responsible for loss or damage of said goods during transit; common carriers provide a public good and therefore cannot discriminate. Examples: public airplanes, trains, buses, etc.).
    -Manipulation of the last mile: The "last mile" is the final section of data transportation from the provider's source to the user's device, and is the part that most influences what the user actually perceives, e.g. copper wires connecting landlines to telephone exchanges, cell towers, cable television utility poles, etc. The bandwidth (The difference in upper and lower cutoff frequencies for a signal processor; it acts as a kind of "opening" that lets data through, and data larger than the "width" of this opening is elongated and squeezed through, as it cannot pass through all at once) of the last mile limits the total bandwidth of the data that reaches the user. Proponents of net neutrality argue that net neutrality makes it impossible for ISPs to exploit last mile infrastructure to block or filter content, or even block out competitors who are supplying the user data.
    -Tiered service model: A business model in which a range of prices are attached to one product, which varies correspondingly in quality. This is opposed to the traditional model of one price for one product. Examples include cell phone plans (e.g. more money buys more data, texts, and call minutes). Proponents of net neutrality argue that telecommunication corporations intend to use their control over the transport of data to the consumer to enforce a tiered service model, as well as to create artificial scarcity, which drives the price up, and force consumers to purchase undesirable services in conjunction with the basic data transport service.
        -Comcast has intentionally slowed peer-to-peer (P2P) communications when it attempted to block certain attempts for high-speed users to share content with each other in an effort to save money (file sharing on high speed connections uses more bandwidth that isn't available to other subscribers).
    -Legality: A federal appeals court has struck down the provision that the Federal Communications Commission (FCC) has the authority to regulate Internet traffic the way it does telephone traffic. Nonetheless, regulations exist the promote net neutrality, though enforcing it entirely wouldn't be possible.
    -Interconnect companies: Interconnect companies, such as Level 3, are companies that provide data transport service in between the provider and the consumer. The extra transit often slows down speeds. 
        -Netflix often accounts for 50% of video traffic on the web at any given time, and its streaming services consume very large amounts of bandwidth. Under the principle of net neutrality, Comcast would be forced to treat Netflix's data the same as it treats anyone else's, even though Netflix drives costs up. Comcast eventually slowed down Netflix's speeds, until Netflix agreed to pay extra in order to obtain a direct connection between its servers and Comcast's, cutting out Interconnect companies and speeding up data transport. Netflix's CEO, Reed Hastings, finds this practice exploitative and unfair and that all Internet traffic ought to be treated the same, whereas Comcast argues that it's only fair to compensate for the extra costs that come with hosting Netflix.
-Color theory: The study of color, mixing colors, etc. Each color has three overarching attributes.
    -Main attributes:
        1. Lightness: How bright or luminous a color is, physically interpreted to mean how much sunlight is cast on a colored object, or how bright the pixels transmitting a color onto a screen are.
        2. Saturation: (A.K.A. chroma) The perceived intensity of a color, on a scale of muted dull gray to a complete vivid color. It is also a measure of how much each of the three primary colors differ by proportion in the final color; the closer the proportions of each primary color used, the less saturated (and more grayscale) the color is. Saturation depends on both light intensity and the color's distribution across the spectrum of wavelengths, with more saturated colors being "purer" and using fewer and fewer wavelengths (with a single wavelength, or one color, being the most saturated possible).
            -One definition of saturation is "the proportion of pure chromatic color in the total color sensation", given by the equation S_(ab) = saturation = C_(ab) / sqrt( (C_ab)^2 + L^2 ) * 100%, where C_(ab) = chroma and L = lightness.
            -Another definition takes the square root of the colorfulness divided by the brightness.
        3. Hue: The actual color spectrum that the color lies in (e.g. red, orange, blue, green, etc.). It can be defined as how "red, blue, and green" a color is. 
            -Computing hue using RGB values: tan(hue) = (sqrt(3) * (G - B)) / (2R - G - B), where R, G, and B, represent the red, green, and blue values, respectively, and run from 0 to 255. 
    -Color wheel: The color wheel is flawed in its attempt to represent the colors as a circular progression that doubles back to itself, when in reality the colors progress linearly as the wavelength of the light progresses.
    -Other terms:
        -Shade: A variation in hue produced by the addition of black.
        -Tint: A variation in hue produced by the addition of white. 
        -Intensity: The brightness or dullness of a hue. 
    -Color systems: A methodology for producing a final work in all color from scratch.
        -Subtractive color: The most basic and intuitive color system, which starts with white and darkens to black as more and more colors are added. This is used in painting, printing, etc.
        -Additive color: Begins with black and ends with white; more and more white and tint is added until the result is white. This is used in computer screens. 
-Maxwell's equations: A set of four PDEs that govern the field of classical electromagnetism. They describe the behavior and nature of electric and magnetic fields and how each affects the other, and how both are affected by charges and currents. The equations:
    1. Gauss' Law: The total flux of the electric field generated by a source charge out of a given surface is directly proportional to the amount of charge in the source. This implies that electric flux is essentially independent of the shape of the surface or any other characteristic, depending only on the source charge's magnitude. It also implies that more flux in one part of the closed surface causes less flux in other parts, since the source charge's magnitude is unchanging.
        -Integro form: surface integral over closed surface of E (dot) dS = Q/(epsilon), where E = electric field (a vector field), dS = surface differential (vector-valued), Q = source charge, epsilon = permittivity of free space = 8.854187817... E(-12)
        -Differential form: del (dot) E = (rho) / (epsilon), where del (dot) E = divergence of the electric field and rho = charge density.
    2. Gauss' Law for Magnetism: The total flux of the magnetic field out of a given closed surface is always zero. This is because whatever source of flux exits the surface out of one pole of the magnetic must eventually re-enter the surface when it joins the opposite pole, meaning all external fluxes cancel out. This also implies that magnetic monopoles cannot exist; each magnetic must have two poles.
        -Integro form:  surface integral over closed surface of B (dot) dS = 0, where B = magnetic field
        -Differential form: del (dot) B = 0, where del (dot) B = divergence of the magnetic field
    3. Faraday's Law of Induction: Changing magnetic fields induce voltages that generate a current; this current's direction and magnitude are such that the magnetic field generated by the new current (since current generates magnetic fields as well) cancels out the initial change in the initial magnetic field. This makes sense from the standpoint of conservation of energy: magnetic fields induce currents and currents induce magnetic fields, but if a change in a magnetic field created a current that then induced another magnetic field, and the two magnetic fields reinforced each other rather than canceling, then new magnetic energy has been created, since the sole action of changing a magnetic field has lead to an overall stronger magnetic field (i.e. the compound of the initial and induced magnetic field). Thus, the new magnetic field must act to cancel the initial change.
                    The question of why magnetic fields induce currents in the first place (and vice versa) is more fundamental than this law. 
        -Integro form: line integral over closed curve of E (dot) ds = - d/dt( double integral over region bounded by closed curve of B (dot) dA ), where ds = curve differential
        -Differential form: del (cross) E = - dB/dt, where del (cross) E = curl of the electric field and dB/dt = partial derivative of the magnetic field with respect to time
            -It makes sense to consider the curl of E since this phenomenon happens in circuits, which begin at a voltage source and loop back to it, curling about the magnetic field in the middle.
    4. Ampere's Circuital Law: When a current induces a magnetic field, the amount by which that magnetic field circulates about the current is proportional to the total amount of current (which is the bound current (charge density in materials that prevent motion) and free current).
        -Integro form: path integral over closed curve of B (dot) ds = (mu)(double integral over region bounded by closed curve of J (dot) dA), where mu = vacuum permeability = 4 pi E(-7), J = current density vector
        -Differential form: del (cross) B = (mu)(J + (epsilon)(dE/dt))
            -Meaning: All magnetic fields have positive curl, since magnetic field lines always loop from the north pole to the south pole and through the magnet and again through the north pole. In fact, the stronger a magnetic field is, the tighter the loops are; the curl of B is a measure of the magnetic field's strength. This equation is thus analogous to Gauss' Law; Gauss' law provides a way to measure the amount of electric field there is (which is done through flux), and this law provides a way to measure the amount of magnetic field there is (which is done through how tight the magnetic field lines are looping). This strength is proportional to the total current in the system. The total current is the sum of the free current and bound current, which is why there are two terms on the right side of the equation. The stronger the total current is, the stronger the generated magnetic field is.
                -Free current: Free current is the moving charge within a conductor (usually a wire) that creates an electric field; this can be measured by the current density J in the conductor.
                -Bound current: Current not in a conductor, e.g. in an insulator or in the air, etc. These charges are not moving, but they still contribute to the electric field. Though current is defined as the rate of change of the charge, this is really the rate of change of the electric field (a measure of the charge), and therefore the bound current, which is the contribution of the non-moving charges to the total current, is proportional to the change in the electric field.
    -Note: A consequence of the third and fourth laws is that a changing electric field generates a magnetic field. But since the electric field is changing, the magnetic field is changing. However, a changing magnetic field generates a (changing) electric field, which generates a new (changing) magnetic field, which generates a new (changing) electric field, and so on. The same is true for an initial changing magnetic field. This infinite back-and-forth produces ripples in the electric and magnetic fields (which are really two sides of the same electromagnetic field) which we perceive as light.
        -The "acceleration" of the field is given by the Laplacian of the field: 
            1. (del)^2 E = -(mu)(epsilon)(d^2/dt^2 (E)) where (del)^2 is the Laplacian operator, and d^2/dt^2 is the second partial derivative with respect to time.
            2. (del)^2 B = -(mu)(epsilon)(d^2/dt^2 (B))
         These equations describe the motion of the waves in the electric and magnetic fields.
-Texas sharpshooter fallacy: A common logical fallacy in which similarities in several variables are emphasized and differences are ignored, implying the perception that random chance is not at work. The fallacy gets its name from the analogy of a Texan who shoots randomly at a barn door; if he shoots truly randomly, some of his shots will cluster together, and if targets are then painted around the clusters, it will appear that the random shooting was the work of a skilled sharpshooter.
    -Examples: Many people point out the similarities between President Lincoln and President Kennedy as spooky and odd. Both were elected exactly 100 years apart, both were shot and killed by assassins; Lincoln's shooter shot Lincoln in a theater and fled to a warehouse, while Kennedy's shooter shot Kennedy in a warehouse and fled to a theater. Lincoln had a secretary named Kennedy, and Kennedy had a secretary named Lincoln. Both were succeeded by a man named Johnson; both Johnson's were born 100 years apart. However, this story neglects the countless number of differences in the lives of Lincoln and Kennedy. There are so many variables that could be accounted for when comparing the entire lives of the two people that, by random chance, a few are bound to overlap. When those few overlaps are highlighted and emphasized while the overwhelmingly larger mound of disagreeing variables
           is ignored, it appears to be a causal link rather than pure randomness.
    -In general, when a very large number of variables are considered, random chance dictates that clusters will inevitably pop up. If those clusters are emphasized and the far greater amount of discrepancies neglected, the two variables can be made to appear correlatory rather than purely random.
-Random variable: A variable whose value is subject to probability rather than some input; in a probability distribution for a random variable, a possible outcome is inputted into the distribution and that outcome's probability is outputted.
    -Law of large numbers: As a particular random experiment is performed over and over, the average of all trials thus conducted approaches the expected value.
        -Intuition: Any random experiment has associated with it some random variable, X, with a mean mu and variation sigma^2. After conducting some integer number of trials, say n, of the experiment, we have n identically distributed random variables, X1, X2, ..., Xn. The mean of those random variables is mu = (X1 + X2 + ... + Xn)/n. The variability of mu is given by the variability of the sum of all the random variables (i.e. the numerator of the mean) divided by n^2 (since variability scales quadratically with scalar multiplication; this is due to the definition of variability, which uses the squares of variables). Variability scales linearly with for random variables, however, i.e. if Var(X) = sigma^2, then Var(X1 + X2 + ... + Xn) = Var(X1) + Var(X2) + ... + Var(Xn) = n*sigma^2. Therefore, the variation in the mean of n random variables = (n*sigma^2)/(n^2) = sigma^2 / n. 
              As n approaches infinity, the variation in the mean goes to 0, implying that the results approach the expected value.
-Aleph numbers: The aleph numbers are a set of numbers that describe the cardinality (the number of elements in a set) of sets; for all intents and purposes, infinite sets. The cardinality of finite sets is obvious, but the cardinality of infinite sets is more complicated. If every element in a set can by systematically listed, or counted, then we say that its size is listably, or countably, infinite. This is because we can count all the elements in the set systematically, and although it's still an infinite set, it can be counted. Formally, this is true if there exists a bijection (a one-to-one function) between the natural numbers (used for counting) and the set in question. This form of infinity, countable infinity, is denoted as aleph zero. Aleph one refers to the cardinality of uncountably infinite sets. The set of real numbers, for example, is uncountably infinite because it's impossible to systematically list the real numbers. No matter how many real numbers you list, it's possible to find another real    number that isn't any of the previously listed numbers. Alternatively, it's impossible to write down an ordered list of the real numbers (even though the set is totally ordered) because the moment you write down two real numbers, one can produce another real number between the two. This form of uncountable infinity is denoted aleph one. Theoretically, the aleph numbers exist from aleph zero, aleph one, aleph two, aleph three, etc. for any natural number. We can define the n-th aleph number recursively using the concept of a power set (the power set of a set S, denoted P(S), is the set of all subsets of S). By Cantor's Theorem, for a set S, P(S) has greater cardinality than S. Therefore, we can define aleph-2 as P(real numbers), aleph-3 as P(any set with cardinality of aleph-2), ..., aleph-n = P(any set with cardinality aleph-(n - 1)). 
    -Cantor'd diagonal proof: This proof details a method for finding a real number that is different from every real number on some list. Take the real numbers between 0 and 1. If I write a list of, say 10, infinitely long decimals (e.g. 0.3243223242214124...) then to find a real number different from the ten I've written down, I simply change the first digit of the first number, the second digit of the second number, all the way up to the tenth digit of the tenth number. This new number must be different from all the numbers listed. This process generalizes even if I have an infinitely long list of real numbers. Therefore, the set is uncountably infinite.
    -Algebraic number: Any number that is the root of a polynomial with integer coefficients (or, equivalently, rational coefficients). All rational numbers are algebraic, and some irrational numbers are algebraic.
    -Transcendental numbers: Non-algebraic numbers; almost all of the real numbers (in the sense that the set of transcendental numbers is an uncountable set whereas the set of algebraic numbers is countable) are transcendental.    
-Hyperreal number: Denoted *R (as opposed to R), the hyperreals are a system of numbers used to describe infinity or infinitesimal quantities. Formally, they are the set of numbers greater than the quantity (1 + 1 + 1 + ... + 1). The hyperreals satisfy the transfer principle, which states that all true (first-order) statements in R hold for *R. This includes statements such as the law of commutativity (i.e. x + y = y + x), the fact that sin(n*pi) = 0 for all integers n, etc. Hyperreal numbers are expressed as sequences. Sequences that converge to 0 are expressed as infinitesimal hyperreals, sequences that converge to non-zero values are finite hyperreals and are infinitely close to some real number but not quite equal to that real number, and divergent sequences are infinite hyperreals. Algebra over hyperreals is thus performed on corresponding components of the associated sequences for a hyperreal number. A real number r is expressed in hyperreal form as the sequence {r, r, r, ...}.
    -Non-standard analysis: A field of mathematics that provides a logically rigorous structure for infinitesimal and infinite numbers. This technique is used to simplify many results in calculus, which relies on epsilon-delta and limits to settle the philosophical debate over infinite quantities. An infinitesimal number is defined as a non-zero element that is smaller than the reciprocal of the set of natural numbers, i.e. 0 < x < 1/5, 1/6, 1/7, ... implies x is infinitesimal. At no point in the sequence is an element actually 0, and so the sequence's limit is positive (greater than 0), but the sequence's limit is also smaller than any real number you can think of (since you'll always reach the reciprocal of a bigger number).
        -Example: Proving product rule for differentiation using hyperreals: 
            1. Let y = u*v
            2. For an infinitesimal hyperreal dy, y + dy = (u + du)(v + dv)
            3. Therefore, dy = (u + du)(v + dv) - y = (u + du)(v + dv) - uv = u*dv + v*du +  du*dv
            4. Therefore, dy/dx = (u*dv)/dx + (v*du)/dx + (du*dv)/x = (u + du)(dv/dx) + (v)(dv/dx) = u*(dv/dx) + v*(du/dx) since du is infinitesimal.
    -Standard part function: A function over the finite hyperreals. All finite hyperreals are infinitely close to some real number (i.e. the difference between the hyperreal and its associated real number is infinitesimal). The standard part function outputs this associated real number. The standard part of a hyperreal H is denoted st(H).
-Pando: A giant colony of aspen trees. The entire colony of trees shares a huge root system and each aspen is genetically identical to the next. This is because the entire colony is actually one organism, beginning with a single aspen tree, that cloned itself and sprouted another aspen in its root system, and so on. It is estimated to be 80,000 years old and weighs 6 million kg. The colony spans 106 acres of land with 40 thousand trees.
-Annus mirabilis papers: Four groundbreaking papers published by Einstein in 1905, a year considered to be Einstein's "miracle year" (i.e. annus mirabilis).
    -Photoelectric effect: One paper was on the photoelectric effect, and introduced the idea of quantized energy transfer and photons, as well as an explanation of the experiment that the amount of energy electrons have when they are excited varies with the intensity of the light shone upon the metal containing the electrons rather than the brightness.
    -Brownian motion: This paper explained the random motions that particles suspended in a fluid undertook. It explained that the motion was a result of thermal interactions between the quickly moving atoms and molecules of the fluid and the particles immersed in that fluid. 
    -Special relativity
    -Mass-energy equivalence
-Intuition for matrices: A function (or transform) from R^m to R^n is classically considered to be a specified correspondence that takes m real numbers as inputs and corresponds to them a set of n real outputs. However, the function can also be considered a transform that inputs m-dimensional vectors and transforms them into n-dimensional vectors, with the vectors in question reLating to each other through the actual transform's specifications (e.g. squaring, cosine, etc.). 
    -Consider the transform T: R^m -> R^n | T( [x_1, ..., x_m] ) = [f_1(x_1, ..., x_m), ..., f_n(x_1, ..., x_m)]. It follows that for any T, a matrix A (of order n by m) can be found such that, instead of writing T( [x_1, ..., x_m] ) = [f_1(x_1, ..., x_m), ..., f_n(x_1, ..., x_m)], one can write T( [x_1, ..., x_m] ) = A * [x_1, ..., x_m] (in this situation "*" denotes the matrix product); then A is called the matrix that induces T. Thus, all matrices can be considered nothing more than linear (they are linear functions when the entries of the matrix are always numbers) functions of vectors, since everything about a transform is included in the induction matrix - the number of columns indicate the number of vectors that were inputted (this makes even more sense with the fact that matrices can be considered sets of column vectors) and the number of rows specifies the number of outputs. 
     Moreover, it can be shown that for a transform T, the each column of the induction matrix A is simply the image of the corresponding standard basis vectors under the transform T. It makes sense that the matrix that represents the transformation in space is the set of transformed basis vectors; since the set of basis vectors spans the entire space (in the simplest way possible), the set of transformed basis vectors, in a way, spans the entire transformed space. This gives a strong geometric interpretation, since the set of standard basis vectors "define" a rigid, simple, and standard space, and the set of transformed basis vectors that make up a matrix's columns represents the new distorted space, and vectors from the first space are mapped, distorted, and transformed according to the warping of the space that the matrix performs. 
    -Matrix multiplication (ie multiplying two matrices) can thus be seen as function composition. 
    -Eigenvectors: An eigenvector is a vector associated with a matrix such that when the eigenvector is multiplied by the matrix, the resulting vector is a scalar multiple of the original eigenvector. That is, for a matrix A and vector x, there exists a scalar lambda such that Ax = (lambda)x. There can be multiple eigenvectors to a matrix, and the scalar lambda is the eigenvalue for that specific eigenvector. 
        -Intuition: Matrices are akin to linear transformations, and since any Euclidean space is a vector space built from vectors, a matrix (which inputs and outputs vectors) can be thought of as a transformation that distorts space by rotating and scaling the vectors that build the original space. Note that if the matrix is filled with only constant/numerical entries, the transformation is completely linear in that only simple scales and rotations occur. Then, an eigenvector is a vector that is completely fixed in rotation under the linear transformation. The eigenvectors of a linear transformation provide a concise description of the transformation as a whole because they constrain and limit the transformation from any arbitrary and vague transformation (it could literally be anything in the infinite space of possible linear transformations) to a specific linear transformation that satisfies certain boundary constraints; it MUST leave certain specific vectors (the eigenvectors of course) untouched. By analogy, the roots of a polynomial are instrumental to the description of the polynomial itself (which is of course a mapping/transformation in itself) since any polynomial can be built up from only the roots. Out of all possible polynomials that could exist, knowing the roots of a polynomial similarly constrain what the polynomial can be by forcing it to pass through certain points, have a certain degree, etc. Eigenvalues can analogously be thought of as extremely rough measures of the distortion a linear transformation imbues on a space, and the corresponding eigenvalues tell you about the orientation of the distortion. 
-LSD: (Lysergic acid diethylamide)  A hallucinogenic drug. LSD is highly potent, with 20 micrograms (1 millionth of a gram) and a common dose of around 100 - 200 micrograms. A toxic dose is thousands of times the threshold, and so reports of overdosing are extremely uncommon. The drug is most commonly administered orally, through paper blotter, through geLatin, or by pill, and sometimes intravenously or intramuscularly. Although trip intensity is proportional to dosage, duration is relatively constant. 
    -Initial effects begin 20 - 40 minutes after consumption. These effects include mainly euphoria and disorientation. After this phase, hallucinations begin; these reach a peak at 5-6 hours after consumption. 
    -Mental effects: One typically feels detached from one's ego, and can quickly alter between states of consciousness. Auditory and visual hallucinations begin. Some are merely intense illusions, such as walls "breathing", or unmoving objects "flowing" or moving. Mild synesthesia can occur, with senses mixing together, e.g. tasting colors or hearing temperature, etc. The trip's overall experience is highly variable, and depends mainly on the drug itself but also heavily on the setting of the drug, as well as one's state of mind and expectations. It is rare for full blown visual hallucinations to occur; more often, reality is severely distorted instead. 
    -Comedown: Effects typically wear down after 6-8 hours, and tend to vanish completely after a night's sleep. Thoughts while on the drug tend to be ineffable and transcendent, and cannot be put into words. 
-NFPA 704: (Standard System for the Identification of the Hazards of Materials for Emergency Response) A standard in the National Fire Protection Association that defines the fire diamond, a diamond (or square rotated 45 degrees) split up into four sub-diamonds. Each sub-diamond has a color that defines the type of hazard (color order: blue, red, yellow, white are the colors for sub-diamonds on the left, top, right, and bottom respectively) and numbers that define how hazardous the substance is.
    -Red: The red sub-diamond represents flammability of the substance. Flammability levels:
        -0: The material will not burn under typical fire conditions. 
        -1: Materials that burn at a temperature of above 200 degrees F
        -2: Materials that burn between 100 degrees F and 200 degrees F
        -3: Materials that burn below 100 degrees F and can be ignited under all ambient temperature conditions. 
        -4: Materials that burn below 73 degrees F and vaporize quickly
    -Blue: Represents health hazards. Levels:
        -0: No health hazards
        -1: Exposure causes minor injury or irritation
        -2: Intense of prolonged exposure can cause temporary incapacitation    
        -3: Even short exposure can cause serious injury, both temporary or residual
        -4: Very short exposure can cause death or major injury
    -Yellow: Represents reactivity. Levels:
        -0: Stable, even under reactive conditions, and does not react with water
        -1: Stable at standard temperatures and pressures, but can become unstable in extreme conditions
        -2: Normally stable, but becomes unstable at elevated temperatures and pressures; instability involves violent chemical change and reacts violently or explosively with water
        -3: Capable of detonation, but requires a strong ignition source, high temperatures, or electric shock
        -4: Easily detonates or explodes even at standard temperatures and pressures.
    -White: Special notice symbols. 
        -OX: The substance is an oxidizer, and thus allows other substances to burn even without air supply.
        -W (with a strikethrough): Reacts dangerously with water.
        -SA: Asphyxiant gas that can suffocate. 
        -COR: A corrosive substance
        -ACID or ALK: Strongly acidic or basic substance, or alkaline substance (to be more specific)       
        -BIO: A bio-hazard
        -POI: Poisonous substance
        -RA, RAD, or radiation symbol: Radioactive substance
        -CYL or CRYO: A cryogenic substance, i.e. a substance at an extremely dangerously low temperature
-Functional equation: An equation whose solution is a set of functions, and relates the function's various arguments to each other.
    -For example, the functional equation f(x + 1) = x*f(x) is solved with f(x) is the gamma function. The equation f(x + y) = f(x)f(y) is solved by all exponential functions.
-Welding: The process by which two metals are joined together by melting sides of each metal down, mixing the resulting molten metal (though only part of the metal is melted, the part that is being joined), and allowing it to cool, which produces a joined metal piece.
    -Cold welding: A process in which metals of different composition spontaneously weld upon contact in the absence of heat. It is often achieved through immense pressure instead, and is a common problem in outer space. 
    -Soldering: Differs from welding, but is a weaker method for binding metals together. Instead of melting parts of the two metals to be joined, a third filler metal, which has a lower melting point than the joining metals, is used to join the metals. The filler metal is melted, and the two joining metals are attached through the filler metal (each metal is dipped into the molten filler metal) when the filler metal cools.
-Convection: A process of heat transfer in which thermal energy is imparted to the particle's of a fluid, and that fluid's molecules' motion carries that thermal energy to another location. An example is an oven; the air is heated next to heating coils, and the heated air in turn heats the food in the oven.
    -Thermal radiation: Convection is an alternate to the more direct heat transfer form of radiation, which is direct electromagnetic contact between the heat source and the recipient.
-Sleep inertia: A physiological state in the human body that temporarily onsets after awakening from sleep. It includes feelings of grogginess and impaired physical and mental dexterity. 
-Just world fallacy: The irrational cognitive bias that the world behaves in a predictable fashion and adheres to a code of morality, i.e. that the world is just and fair. This bias leads subjects to draw baseless conclusions from observed data in an attempt to identify the data with their perception of the world as just. This tends to manifest itself as people blaming victims of tragedies for having done something wrong to deserve the tragedy, even when there is absolutely no evidence to the theory, or that someone upon whom a random reward has been bestowed has somehow done something to deserve it.
    -Studies: One study involved 72 women watching a woman being solving puzzles who was subjected to electric shocks each time she answered incorrectly (the electric shocks were actually fake). Although the observing women initially expressed discomfort at watching the woman receive electric shocks, as the experiment continued and the observers could do nothing about it, they began to deride the woman, saying she deserved the shocks and making negative comments about her appearance and intelligence.
    -Theory: There is not much theory behind this phenomenon, but many say it is a by-product of the human ability for long-term planning and the effective method of setting and reaching a goal. The capacity for long-term planning requires a fundamental belief that one can have a predictable impact on the world. Furthermore, humans at heart psychologically reject the notion that there is an element of random chaos to the world as this idea brings helplessness; this psychological rejection and the capacity for long-term planning together may give rise to the just-world fallacy.
-US currency security measures: About $3 out of every $10,000 in the US is counterfeit. Anti-counterfeit measures in the currency include very fine lines, microprinting, color shifts, watermarks, special threads, and special paper.
    -Fine lines: In hard colors on currency, the ink is actually spread in extremely fine lines that are visible under close inspection (such as under a decent magnifying glass) and incredibly hard to reproduce without a very high quality printer.
    -Microprint: Many lines of ink are actually lines of text that appear to be solid colors to the naked eye. Text this fine and small is very hard to reproduce.
    -Color shift: Special inks are used in certain areas of the currency to allow the ink to change colors when tilted in various ways under light.
    -Watermarks: An ingrained image or text in the currency visible under light; it is created by varying paper density in specific ways and patterns.
    -Threads: Special security threads are embedded in the cash (not printed on it), with "USA" and the currency's denomination printed on it. These threads also glow under a UV light, with a specific color corresponding to a denomination.
    -Paper: Very special paper of a quarter linen and three quarters cotton is used for printing upon, and supply of this paper is very strictly controlled. 
-Factorial number system: (A.K.A. factoradic) A sequence of n digits under the factorial number system is less than the value of n! and can be converted to a permutation of n very easily. 
    -Definition: The system is based on place values, with the i-th digit from the right having a place value of i! which means that the digit in place i must be less than i (since otherwise a higher factorial would be used). Thus, the rightmost digit is always 0, and the next can be 0 or 1, and the next can be 0, 1, or 2, and so on.  Whereas decimal is a geometric number system (the place values form a geometric series that depends on exponents, i.e. 10^0, 10^1, 10^2, and so on), factoradic follows the factorial sequence (i.e. 1, 1, 2, 6, 24, 120, ...).
        -Converting to decimal: The i-th digit should be multiplied by i!, and then all the digits should be summed together to obtain the final decimal value. For example, the number 351010 in factoradic is equal to 3*5! + 5*4! + 1*3! + 0*2! + 1*1! + 0*0! = 3*120 + 5*24 + 1*6 + 0*2 + 1*1 + 0*1 = 463 in decimal. 
        -Converting from decimal: Divide the number by the highest factorial possible, say n! (such that (n+1)! is greater than the number); the quotient is the leftmost digit. To obtain the next digit from the left, divide the remainder by (n-1)!; the quotient is the next digit, and to obtain the next digit repeat the process until you reach 0!. 
    -Application: The system has the downside that as numbers get larger and larger, new digits must constantly be invented to denote the larger number. The system is very useful for finding permutations of lexicographically ordered sets. 
        -Lexicographical order: For a collection of sets A_1, A_2, ..., A_n, the Cartesian product of those n sets yields a set of n-tuples, i.e. ordered pairs of n elements. Those elements can be ordered according to a lexicographical system. Consider the n-tuples a = (a1, ..., an) and b = (b1, ..., bn). Under lexicographical ordering, the rule is that b is greater than (i.e. ordered higher lexicographically than) a if and only if a1 < b1 OR a1 = b1 and a2 < b2 OR a1 = b1, a2 = b2, and a3 < b3, OR ... 
        -Factorial number system: To find the nth permutation of a set of elements, simply writhe Ute n in factorial notation. Then assign each element in the set to be permuted a digit, starting at 0 and increasing to 1, 2, 3, and so on. The first digit of the factoradic representation of n is the position of the element in the set that is the first element in the nth permutation. Then remove that element from the set and repeat. Thus, the second digit reveals the position of the element in the set that comes next in the nth permutation, and so on.
            -Example: Suppose we want to find the 22nd permutation of the set (a, b, c, d). 22 is represented in factoradic as 3200. The element "d" has a position of 3 (remember to start numbering at 0), so our new set is (a, b, c). The element "c" has position 2. In the new set (a, b), the element "a" has position 0, and in the last set (b), the element "b" has of course the position 0. Thus, the 22nd permutation of (a, b, c, d) is (d, c, a, b).
-Google search process: Before searches are made, Google crawls and indexes the web. Then, when a search is entered, algorithms extract various forms of information from the search query and match it with web pages in the index. Other algorithms and sometimes manual labor work to keep spam from showing up in the search results. 
    -Crawling and indexing: When a Google search is made, the Internet itself is not being searched per se, but rather Google's index of the Internet (the inability of Google to index the entire web gives rise to the Deep web). Google finds a few webpages (which are likely to be broad and full of hyperlinks, and are diverse from each other) to begin with (these initial webpages are called seeds) and use spider algorithms (i.e. Web crawlers) to add all the hyperlinks on the page to the crawl frontier (a list of URLs to which the spider will visit), and once it visits those URLs it repeats the process indefinitely. Because a spider can only store a finite number of URLs on the crawl frontier, it must prioritize the order in which it visits and downloads URLs. 
    -Algorithms: Over 200 factors about a website are indexed for every webpage in the index, and when a search query is made, algorithms figure out which webpages the searcher is most likely searching for based on the query and how well the 200 factors match up. These include:
        -Answers: Does the site display immediate information that answers a question, such as the weather or a sports score?
        -Autocomplete: Based on the search query, this completes and predicts what the searcher is looking for. It sorts through ambiguous terms and the structure of the query. 
        -Freshness: Uses the latest and most recently updated information and sites.
        -Spelling: Consults lexicons to look for spelling errors and what the most likely correct word is. 
        -Synonyms: Also searches webpages for synonyms to key words in the search query.
        -Knowledge graph: Accesses a database of real world objects (even people) and the connections between them to better understand what the searcher is looking for.
        -Quality: The quality and reputation of a website is also taken into account. An important algorithm Google uses is called PageRank, which ranks webpages on importance by looking at how many hyperlinks lead to that page (among other factors). More trustworthy and reputable sites are displayed first. 
        -User context: Takes the user's region, language, culture, etc. into account to provide more specific and relevant results. 
-Pedigree collapse: A phenomenon in genealogy in which the progression of ancestors falls short of exponential, as it should be, when organisms that mate have ancestors in common. Normally, when two organisms mate, two separate lineages are merging; this is why the number of ancestors of any given organisms doubles with each generation, since every generation back each individual organism joins two lineages. This gives rise to the intuition that an organism's ancestry doubles with each generation. However, this is clearly flawed because this implies that someone alive today would have 2^30 (about 1 billion) ancestors 30 generations ago (about the High Middle Ages), and this number exceeds the world population at that time. However, this isn't the case because as you go back further generations, more and more people (with or without knowledge of the fact) mate with people they are related to, which reduces the number of ancestors. 
            If two first cousins meet and mate, their grandkids will have 6 grandparents rather than 8. 
-Mitochondrial Eve: The most recent matrilineal (descent through mothers and daughters) common ancestor of all modern humans. She lived about 100,000 to 200,000 years ago and is the woman from whom all humans are descended from (on their mother's side). All current mitochondrial DNA is descended from hers, differing from hers only in mutations that have occurred over the years. 
-Multinomial theorem: An extension of the binomial theorem that goes over the expansion of a polynomial raised to any integer power. 
    -Formula: (x_1 + ... + x_m)^n = sum over (k_1 + k_2 + ... + k_m = n) of (n choose (k_1, k_2, ..., k_m)) * product from t = 1 to t = m of (x_t)^(k_t)
        -The index of summation, i.e. k_1 + ... + k_m = n, means to sum over all possible collections of m natural numbers that sum to n, and the multinomial coefficient (n choose (k_1, ..., k_m)) is an extension of the traditional combination, and is equal to (n!)/((k_1)! * (k_2)! * ... * (k_m)!)
            -Example: Consider (x + y + z)^3. Applying the formula, we have: (x + y + z)^3 = (3 choose (3, 0, 0))*(a^3*b^3*c^0) + (3 choose (2, 1, 0))*(a^2*b^1*c^0) + (3 choose (2, 0, 1))*(a^2*b^0*c^1) + (3 choose (1, 0, 2))*(a^1*b^0*c^2) + (3 choose (1, 1, 1))*(a^1*b^1*c^1) + (3 choose (1, 2, 0))*(a^1*b^2*c^0) + (3 choose (0, 3, 0))*(a^0*b^3*c^0) + (3 choose (0, 2, 1))(a^0*b^2*c^1) + (3 choose (0, 1, 2))*(a^0*b^1*c^2) + (3 choose (0, 0, 3))*(a^0*b^0*c^3) = (3!/(3!*0!*0!))*(a^3) + (3!/(2!*1!*0!))*(a^2*b) + (3!/(2!*0!*1!))*(a^2*c) + (3!/(1!*0!*2!))*(a*c^2) + (3!/(1!*1!*1!))*(a*b*c) + (3!/(1!*2!*0!))*(a*b^2) + (3!/(0!*3!*0!))*(b^3) + (3!/(0!*2!*1!))*(b^2*c) + (3!/(0!*1!*2!))*(b*c^2) + (3!/(0!*0!*3!))*(c^3) 
-Random sampling: To investigate some attribute of a set (in statistics, that set is referred to as the population), typically some form of an average of proportion is necessary. The most precise way to do this is to employ a census (a survey of the entire population), but this can often be very difficult in practice due to constraints of time, money, and other resources, and some mathematical populations are infinite and therefore completely resistant to a census. Therefore, the method of random sampling is employed to sacrifice certainty in results for ease and practicality, and in some cases necessity. The results of a random sample are estimates of the population characteristics.
          
    -Random sample: A subset of the population in question that is composed of randomly selected members of that population. It is absolutely vital that the sample be composed of randomly selected participants, as this assures against the risk of confounding variables, patterns of selection, and other forms of bias that cause the sample to inaccurately represent the population. The larger the size of the sample (the number of participants), the more accurate it will be. Random samples are meant to represent the population, in all its variety, by proportionally scaling down the various proportions of the population.
                    
        -Sampling distribution: There are many different possible samples of a given size for a population, distinguished by each sample's members (specifically, there are N choose n = N! / ( (n!)(N - n)! ) possible samples, where N = population size and n =sample size). Each specific sample in that set of possible samples has a certain mean, x. The function that maps each element in the set of all possible values of x to that element's frequency (ie how many samples of a given size have that element as their mean) is the sampling distribution for the sample of that above mentioned size. The center of the sampling distribution is the population mean. The foundation of this idea is that the means of the samples of a given size are not just arbitrary numbers; rather, they are the values of a random variable, x.
          
        -Central limit theorem: This theorem, proved by Laplace, states that the sampling distribution of a set of samples of a given size for a population is not any arbitrary function. It is always an approximation of the Gaussian distribution. Mathematically, it states that for the set of samples of size n (n is a natural number) from a population: as the sample size approaches infinity, the sampling distribution approaches the Gaussian distribution with mean x and a standard deviation of (sigma)/(sqrt(n)), where x is the center of the sampling distribution (and hence the population mean) and sigma is the standard deviation of the population. A particularly amazing nuance of the theorem is that no matter how the population distribution is shaped, be it bimodal or horribly skewed, the sampling distribution will always approach the Gaussian distribution.

            -Confidence interval: Since the statistics (means, proportions, etc) of samples from a population are merely points on the sampling distribution graph (which is centered at the true population mean), the results from Brandon sample do not tell us conclusively the population parameters. The idea of confidence interval is to say with that there is a certain probability (the confidence level) that the population parameter is in some specified interval, which we find using the sample's statistics.  The random sample will have some mean which we denote x, and we will use x to estimate the population mean, which we denote mu. We know that mu is the center of the sampling distribution, and that x is on the sampling distribution. Since the sample was random, x is at a random position on the sampling distribution. From the 68-95-99.7 heuristic, we know that about 68% of variation in the Gaussian distribution is captured within one standard deviation of the mean. From the central limit theorem, we know that the sampling distribution is approximately Gaussian (the approximation improves as the sample size increases) and so the 68-95-99.7 heuristic does indeed apply, as does the extension of the heuristic we will soon explain. If 65% of the variation in the sampling distribution (approximately) is captured within one standard deviation of the mean mu, there is a 65% chance that x is within one standard deviation of mu. Of course, mu is not known so this doesn't help much; however, if x is within one standard deviation of mu, then mu is within one standard deviation of x, and so we can say that there is a 65% chance that the true population mean is within one standard deviation of the sample's mean. The only question that remains now is how to use the standard deviation if the sampling distribution's standard deviation is unknown. Here we again must use an estimate for the standard deviation. If we denote the population standard deviation sigma, then the standard deviation of the sampling distribution is (sigma)/sqrt(n), and so to estimate this standard deviation without knowing sigma, we simply use the sample's standard deviation, which we denote s, and approximate using s/sqrt(n). This process is known as a One-Mean z-Interval
          
        -Confidence: However many standard deviations we want to use in our attempt.to.capture the population mean is what controls the confidence level. The higher our confidence level, the greater the probability that the population mean is actually inside the specified interval. However, a higher confidence level means using more standard deviations to capture the population mean on either side of x, and therefore the interval is much wider. Thus, with confidence intervals there is always a trade-off between certainty and precision. At one extreme, one can be 100% confident that the mean is between 0 and infinity. At the other, one can have a tiny interval with a length of less than 0.00001, thus with extremely high precision, but with horribly low confidence. The only way to improve both confidence and precision is by increasing the sample size.
          
            -Process: Once the sample mean x and sample standard deviation s are known, select a confidence level C. Common levels are 95% and 99%. Use the inverse Normal distribution to find how many standard deviations to use by inputting C into the inverse Normal function and dividing by two (since it is two-tailed). Divide by standard by sqrt(n) to find the standard deviation and multiply the standard deviation by the number standard deviations to use (ie the output of the inverse Normal function) to obtain your margin of error (ME). The interval is then x +/- ME, and it is said that we are C% confident that the population mean is between x - ME and x + ME.
          
            -Proportions: When a population proportion (number of successes/total) rather than a mean is desired, the same theory still applies, but standard deviation is now found by the formula s = sqrt( (p)(1 - p)/n ). This process is known as a One-Proportion z-Interval
-Hypothesis testing: Hypothesis testing is a statistical procedure used to test if a hypothesized change in a population is due to random chance or not. Whereas confidence intervals are used to estimate population parameters, hypothesis tests are used when current population parameters are already known, and one is interested in a possible change in that parameter. Thus, hypothesis testing is most commonly used to test the successes of new techniques or mechanisms in various fields of industry, and whether statistical improvements are due to the new method or are merely random chance. They are also commonly used to determine changes in a population due to a stimulus and whether the change is due to the stimulus or is random variation, e.g. presidential election polls. 
 
                 -Process: Since a change is being determined, the default position in the population is the status quo, ie that a change has not occurred. Only if there is sufficient evidence that circumstances have changed is it logical to switch from the default position to the changed position. Hypothesis testing quantifies this principle and applies statistical theory to help. First, a null hypothesis is assumed as true (this is the default position or status quo referenced above) since it is simply the hypothesis that no change has occurred and is hence the base case.  An alternate hypothesis provides the alternative to the null hypothesis and is the object of investigation in hypothesis testing; we wish to see if there sufficient evidence to warrant a switch from the null to the alternative. Second, the sample's statistics are computed. Third, the population parameter, which we know due to the null hypothesis, is used to computed the sampling distribution's standard deviation, and our temple's statistic's z-score is calculated. Fourth, since the central limit theorem guarantees that the sampling distribution will be approximately Normal for large sample sizes, the area under the Gaussian distribution between the z-score and either positive infinity (if the z-score is positive) or negative infinity (if the z-score is negative). This is the p-value and is the probability that, assuming the null hypothesis is true, we would receive the results that we did in our random sample. If this probability is sufficiently low (what defines "sufficiently low" is up to you; probabilities at 5%, 1%, or less than 1% are commonly used), we can reject the null hypothesis in favor of the alternate hypothesis and assume that there is strong enough evidence; otherwise, if the probability is high enough to happen by chance decently often, we fail to reject the null hypothesis. If the null hypothesis is rejected and the population parameter has indeed changed, it is often a good idea to use the random sample drawn for the hypothesis test to perform a confidence interval for what the true population parameter actually is.
          
            -Choice of lowest possible p-value: It is imperative that before the statistical procedure is carried out, a number is predetermined for how low the p-value must be (at least) for the null hypothesis to be rejected. This choice of p-value, sometimes called the alpha level, depends highly on context, importance of the issue, and existing evidence to support the null hypothesis. For example, applying hypothesis testing to see if there is a disease in an area would have a high alpha level (the null hypothesis is that there is no disease), since we would much rather treat a few healthy people (false positive) than not treat sick people (missed negative). For other scenarios a lower p-value is needed. Furthermore, if the null hypothesis is long-standing and well supported by scientific evidence, we require a very small p-value to reject it. If someone claiming to be a psychic performs on a hypothesis test with a p-value of 1%, we would not be convinced even though 1% is plenty low for other situations.

-Random walk: A hypothetical probabilistic scenario in which as marker begins at the point x = 0 on the real number line, and with has a 59% probability of moving forward +1 units or moving backwards -1 units. At each position, the marker will again move one step forward or backward with equal probability. Then there exists a random variable to represent the total distance from x = 0 after any given number of steps, and we denote the distance after n steps D_n. Since the distance can be backward as well as go forward, so we will be concerned with (D_n)^2. 
          
    -Expected value: The expected value of D_n is not 0 as one would intuitively guess. It turns out that D_n has an expected value of sqrt(n), ie after n turns, the marker will be, on average, a distance of sqrt(n) steps away.
                    
    -Proof: First, consider our measure of absolute distance, (D_n)^2. The number of steps on turn n can be defined recursively, since D_n = D_(n-1) + 1 or D_n = D_(n-1) - 1, each with probability 0.5; therefore, (D_n)^2 = (D_(n-1))^2 + 2*D_(n-1) + 1 and (D_n)^2 = (D_(n-1))^2 - 2*D_(n-1) + 1, each with 0.5 probability. The expected value of (D_n)^2, denoted <(D_n)^2>, is (D_(n-1))^2 + 1. Since (D_1)^2 = 1, it follows by induction that <(D_n)^2> = n, and therefore <D_n> = sqrt(n).
          
    -Relation to Brownian motion: The random motions of atoms are a form of 3-dimensional random walk. 

-Intuition for the differential in the integral: In integration, there always exists a differential multiplied by the function being integrated. The reason for its existence is in the definition of integration. Essentially, an integral is meant to be a completely continuous summation of all function values between two bounds, and to achieve this continuous summation we begin with an approximation and then take the limit. To approximate a summation of f(x) over all x in an interval, we first sum f(x) over n equally spaced x_i in the interval, ie the approximation is f(x_1) + f(x_2) + ... + f(x_n). The problem with this approximation is all the missing space in between any x_i and x_(i+1), which is unaccounted for in the approximation. Therefore, we account for this missing space by multiplying each element in the approximate summation with the distance between that element and the next; this "compensation" distance is delta x, and in the limit it becomes dx as the number of elements becomes infinite.

-Intuition for tensors: Scalars (rank 0 tensors) describe raw quantity such as mass or temperature, but in many cases a single raw quantity is not enough to describe the full phenomenon. Vectors (rank 1 tensors) are essentially collections of scalars, because there is a raw quantity involved in each individual dimension. This is why scalars such as temperature are single numbers: only a single dimension is required to express them. For example, as wind speed require multiple dimensions to be described (in the example of wind speed, 3 spatial dimensions are required, and in the example of 4-vectors from General relativity 3 spatial and one temporal dimension is required), and so each dimension necessary has a scalar associated with it. Extending this idea, there exist certain phenomena which require multiple dimensions, just like vectors, except each dimension, which we shall call meta-dimensions, contains its own set of dimensions. For example, internal stress in a steel beam is represented with a tensor, because perpendicular to the x-direction there is a force represented by a vector, and there are similar force vectors in and y- and z-directions. Thus, 3 spatial dimensions (meta-dimensions by our convention) are required to describe the tensor, and each of those dimensions requires a vector that itself requires a scalar in each of the 3 spatial dimensions. Extending this idea, there could exist phenomena that must be expressed in several dimensions each of which is described by a tensor, and so on for rank-3, rank-4, and so on tensors. Notationally, since rank-0 tensors (scalars) are single numbers, rank-1 tensors (vectors) are 1-D columns or rows of numbers, and rank-2 tensors are 2-D arrays of numbers, it would be appropriate for rank-3 tensors to be written in a 3-D cube of numbers, such as an cube with 27 spaces for entries for 3 by 3 by 3 tensor of rank-3. Higher rank tensors of rank-n would correspondingly be written in n-dimensional hypercubes, though this is obviously impossible in our 3-dimensional universe.ons. Thus, for each of the three spatial dimensions, an additional three spatial dimensions is required for a complete representation. The stress tensor has 9 components: each of the 3 spatial dimensions is described by a vector, which itself has each of the 3 spatial dimensions being described by a scalar. Such a rank-2 tensor in 3-dimensional space is represented as a 3 by 3 matrix. This idea can be extended of course if there exist phenomena requiring meta-meta-dimensions, meta-meta-meta-dimensions, and so on, which will be represented with rank-n tensor.
-Logic: The tool for distinguishing between true and false based on a set of initially assumed truths.
-Derivation of L'Hopitl's Rule: This theorem (partially) states that if the limit as x approaches a of f(x)/g(x) is indeterminate as 0/0, then the limit is equal to the limit as x approaches a of f'(x)/g'(x).
          -Proof: First, write f and g in terms of their Taylor series centered at a; then we have the limit as x approaches a of ( f(a) + (x-a)*f'(a) + (x-a)^2*f''(a)/(2!) + ... ) / g(a) + (x-a)*g'(a) + (x-a)^2*g''(a)/(2!) + ... ). If the original limit produces the indeterminate form 0/0, then it follows that f(a) = g(a) = 0, and once these two terms have vanished from the above limit, we can factor out and cancel a single (x-a) term from the numerator and denominator. This leaves the limit as x approaches a of ( f'(a) + (x-a)*f''(a)/(2!) + (x-a)^2*f'''(a) + ... ) / ( g'(a) + (x-a)*g''(a)/(2!) + (x-a)^2*g'''(a) + ... ) = f'(x)/g'(x). Q.E.D.
-Derivation of the gamma function: The gamma function is an extension of the factorial function; the function n! has a domain of all natural numbers n, where as the gamma function interpolates the factorial function as well as stretches the domain to the entire real line. The function is defined gamma(x) = integral from 0 to infinity of (t^(x-1)*e^(-t)) dt. 
          -Derivation: The derivation uses differentiation under the integral sign. First, notice that the integral from 0 to infinity of (e^(-tx))dt = 1/x. Then, differentiating the integral with respect to x, we find that the integral from 0 to infinity of (t*e^(-tx))dt = 1/(x^2). Differentiating again, it is seen that the integral from 0 to infinity of (t^2*e^(-tx))dt = 2/(x^3), and the integral from 0 to infinity of (t^3*e^(-tx))dt = 6/(t^4). Differentiating n times, we find that in general the integral from 0 to infinity of (t^n*e^(-tx))dt = (n!)/(x^(n+1)). Setting x = 1, we find that n! = integral from 0 to infinity of (t^n*e^(-t))dt 
-Some nations in history, such as Great Britain during the American Revolutionary War, or the United States during the Civil War, Grace used the tactic of injecting huge amounts of counterfeit currency into the enemy's economy so as to cause the value of genuine currency to plummet, making it more difficult for the enemy to financially and economically sustain war.
-Predictive analytics: A large methodology for analyzing data and attempting to draw conclusions, find patterns, and predict future events from it. 
    -Social security number: Although the social security number of an individual is meant to be hidden and only revealed when absolutely necessary, researchers have found minute patterns in the assignment of social security numbers, and able to deduce the first 5 digits of a SSN from an individual's publicly available birth information, such as hometown and date of birth, with a 60% rate of accuracy. As the last 4 digits are public anyways, this research shows how much information there is out there for any one person. 
-Iridium flare: A satellite flare is a temporary and very bright glow seen from the surface of the Earth when reflective surfaces on satellites reflect sunlight directly onto the Earth. Iridium flares is such a flare from the Iridium communication satellite system.
    -Iridium satellite constellation: A large group of 66 satellites providing data coverage for most of the Earth's surface. All satellites are in low earth orbit (altitude of ~485 miles) moving at about 17,000 mph. The system is mostly used by the US military to allow global and reliable communication for its personnel.
-Gramogram: Words that sound like letters.
    -Examples: Array (RA), envy (NV), pea (P), enemy (NME), decay (DK), etc.
-Sprouts: A paper game that has very interesting and complex mathematical roots. The game works by first beginning with at least two, but as many as desired, initially placed (in any arrangement) dots on a piece of paper. Players will then draw lines connecting two dots, making the lines as curved, long, and intricate as they wish, without crossing any existing lines. Then, that player places a new dot on the line he just drew at any spot along the line. This thus adds another dot into play, and this dot can also be used to draw lines to or from. However, no dot may have more than three lines protruding from it; when it does, it is completely saturated and can no longer be used to draw lines to or from. Note that the act of placing a dot on a line already causes two lines (both halves of the line you just placed the dot on) to be protruding from the dot, leaving room for one more line to be drawn from or to the dot. It is acceptable for a line to start at and terminate at the same dot, ie a closed loop; this is counted as one protruding line. Players continue doing this until one is unable to connect any two free dots without crossing existing lines. 
-Shadow blister effect: The effect in which as two shadows get closer and closer to each other, they appear to bulge outwards and merge before they actually collide. This is because of overlapping penumbra (the part of the shadow along the edges where only part of the light and not all is obscured) create a completely dark shadow. Neither penumbra is dark enough to be a shadow itself, and therefore is not perceived by the brain as a shadow, but when the two overlap they do form what the brain considers a true shadow, and so to humans it appears that the shadows bulge towards each other randomly. 
-Agnotology: The study of ignorance.    
-Green wave (Switzerland traffic): A phenomenon in which some Swiss cities implement traffic algorithms such that cars who drive exactly 45 km/hr will be guaranteed to catch all green lights and will thus arrive at their destination before a speeding car. This is meant to discourage speeding.
-Shonhage-Strassen algorithm: An extremely fast multiplication algorithm for very large (millions of digits) numbers. It has a complexity of O(n*log(n)*log(log(n))).
-Shor's algorithm: A quantum algorithm that can theoretically factor large composite integers into their constituent primes in less than polynomial time (specifically, O( (log(N))^3 ). This is extremely efficient, much more so than any classical factoring algorithm (none of which come close to polynomial time) and so would be able to break RSA encryption. As of yet, the largest number to be factored by a quantum computer is 143.
-Greedy algorithm: A type of algorithm that is an effective problem solving heuristic and works by quickly makes decisions at each step of the process by analyzing what is best for the solution at that specific moment and step, not thinking future consequences though it does take past mistakes into account to improve future performance. However, it never backtracks or alters past choices, even if they were inefficient in retrospect. It hopes that the locally optimum choice will effect a globally optimum solution. Although only some and not all problems are solved most efficiently by greedy algorithms, greedy algorithms nonetheless approximate the most efficient solution quite well, and are much easier to design and implement than the true solution. 
-Quarter square multiplication: A multiplication method in which the product of two numbers is found not by computing their multiplication but by taking the difference between the floor of the square of the sum divided by 4 and the square of the difference divided by 4. This works because for two number x and y, x*y = 1/4 * (4xy) = 1/4 * ((x^2 + 2xy + y^2) - (x^2 - 2xy + y^2)) = 1/4 * (x + y)^2 - 1/4 * (x - y)^2 = floor( (x + y)^2 / 4 ) - floor( (x - y)^2 / 4 ), where the floor of a real number is that number with its decimal portion rounded down or truncated. The reason flooring is introduced is, is either x + y or x - y are odd, then the other is two and so the fractions will cancel out anyways, and if they are even, they are divisible by 4 or else the fractions will again cancel out.
-Process for electing the President of the United States: 
    -Primaries and caucuses: First, a candidate must be nominated  to represent the political party. This is done in primaries and caucuses, in which potential candidates attempt to garner the support of party leaders and activists. At the national party convention, delegates from each state cast votes for the person who will represent their political party; a majority is required to win the nomination, and thus this voting process can happen several times. Once this candidate is chosen, he chooses a running mate. 
    -General election: After the primaries and caucuses, whatever party differences fostered by the nomination process are set aside and the entire party unites to get their candidate elected. All candidates campaign right up until Election Day.
    -Electoral college: However, voters do not vote directly for the president; this was to ensure that an uninformed decision by the generally stupid populace is not made. Instead, a body called the Electoral college is convened every election year (and dissolved for the next four years) and filled with electors. Each state gets as many electors as it has representatives in the House of Representatives (to satisfy the principle of proportionality) plus the two it gets as a flat rate due to the Senate (to satisfy the principle of equality). Voters cast votes for groups of electors that are pledged to vote for the candidate in question, and not directly for the candidate himself. The electors then convene on the Monday after election day and cast their pre-pledged votes. In theory, this is about the same as a direct election by the people, since the people elect electors who then vote for president, but legally an elector can vote for someone other than to whom he is pledged (such an elector is known as a faithless elector), though this is rare. 
        -The electoral college comes with many problems, first and foremost that it allows a candidate to win the presidency without winning the popular vote. It is also subject to gerrymandering. 
-Mortage: A long-term loan of very large sums of money designed to allow the debtor the purchase a home without having the financial capital to do so outright. This loan must be repaid, on top of interest on the loan. 
    -Down payment: Typically, to assuage some of the risk involved in lending such a large sum of money and to also verify that the debtor will be able to pay off the loan with his current salary, an initial fraction of the entire loan is required to be repaid upfront. Typically a down payment is about 20% of the price of the house (50% before the Great Depression, but lowered to 20% to invigorate the dying economy), and this sum of money must be paid immediately. 
    -Mortage payments: The structure of a mortgage payment is somewhat complex, and is not structured as a simple monthly fee of some of the principle plus interest. The monthly fee depends on the size of the loan and the term of the loan (how long it is to be repaid over). 
        -Structure: There are two main things to pay for a mortgage: the principle (the amount actually borrowed) and the interest on the principle. There is a set amount for the monthly payment, but different parts of the amount are allocated towards the principle and towards the interest. For example, even if $600 is paid, $500 may go towards repaying the principle and only $100 towards the interest. In the beginning of the mortgage, the payments go almost completely towards interest, and the allocation ratio slowly reverses until towards the end, the payments are almost completely principle. 
        -Monthly payment formula: Payment = (Principle * Interest Rate) / (1 - 1/( (1 + Interest Rate)^(number of payments) ) ). The principle is how much money was borrowed, the interest rate is how much interest is paid per payment (make sure to divide yearly interest rates by 12 if you're paying monthly payments), and the number of payments is how many total payments will be made (on a 30 year mortgage that is 360). 
-Stable marriage problem: A mathematical problem of finding a mapping between two sets (of equal cardinality) of elements in which each element has a ranked preference for which element in the other set it would most prefer to be paired with. It is thus analogous to a group of men and a group of the same number of women, in which all the men have a list of which women they would prefer to marry in what order, and the women have an analogous list. It has been proven that given such a problem, a "stable mapping" (a correspondence in which all elements are paired with their highest possible preference on their list) always exists.
    -Definition of "stable mapping": For a mapping from set A to set B, a stable mapping is one in which the BOTH of the following two conditions are NOT satisfied: (1) an element of A prefers to be matched to some element of B over the element in B to which it is currently paired, and (2) that element in B (that the element from A wants to be matched to over its current pairing) also wants to be paired with the element from A over the element to which it is currently paired.
    -Gale-Shapley algorithm: This algorithm iterates a sequence of steps over and over until a stable mapping is achieved. Each iteration has two main steps: First, each unpaired element in A pairs with the element in B that it prefers the most. Second, each element in B severs whatever pairing it has with its element from A if that element in A is not its first choice. In the next iteration, each unpaired element in A pairs with the element in B it most prefers unless it has already paired with that element (in which case it moves to the next most preferred element in B). The elements in B (even if they're already paired) again reject the pairing with every element in A that isn't its first choice. This guarantees that eventually a stable mapping will be achieved. 
-Zermelo-Fraenkal set theory: A system of axioms developed in the early 1900s to reconcile many paradoxes in set theory at the time. It is a widely accepted definition of modern set theory. It is concerned primarily with the mathematical idea of a set, perhaps the most fundamental mathematical construct that exists. A set is a collection of objects that all satisfy some list of axioms. All elements in a set have an intrinsic property called the membership property, which is merely the property that they are sub-categorizations of their containing set. This formalization of sets requires an axiomatic system, which Zermelo-Frankael (ZF) set theory provides. It is extremely important, as the foundations of set theory provide a foundation for literally all of mathematics.
    -Axioms: 
        1. Axiom of existence: There exists a set which is completely empty, i.e. a set without any elements. This axiom establishes that at least one set exists, mathematically. 
            -It follows that there is only one such empty set (called rather appropriately the "empty set" or "null set").
        2. Axiom of extensionality: This axiom provides a way to distinguish sets, and establish uniqueness as a property of sets. It states that for any two sets X and Y, if every element of X is also an element of Y, then X = Y. 
        3. Axiom Schema of Comprehension: This axiom schema was defined to look at the existence of uniqueness of other sets in relation to defined sets. It essentially brings up the idea of a subset of a larger set, and asserts that the subset is also a set. Formally: For an arbitrary mathematical object x, let P(x) denote some property of x; for all sets A, there exists another set B such that x is an element of B if and only if x is an element of A and x satisfies P(x). 
            -It follows that the above set B is unique. 
        4. Axiom of pairing: The essence of this axiom is that any two sets have a pair. Given any two sets A and B, there exists a third set C whose members are exactly A and B. It follows from the axiom of extensionality that this set C is unique. Formally, for any two sets A and B, there exists a set C such that x is an element of C if and only if x = A or x = B. 
        5. Axiom of Union: This axiom states that for any set A that contains as elements some other sets, there exists a set B such that the elements of B are exactly the elements of the sets that are the elements of A. In essence, this states that the union of a set is a set. With the axiom of pairing, these two axioms imply that for any two sets, there exists a third set that is the union of those two sets. 
        6. Axiom of Power Set: This axiom states that given a set A, there exists a set P(A) such that any element of P(A) necessarily shares all of its elements with A, i.e. a set B is an element of P(A), if and only if every element of B is an element of A. 
            -Power set: The power set of a set A is thus the set of all subsets of A.
        7. Axiom of infinity: This axiom guarantees the existence of at least one infinite set. 
        8. Axiom Schema of Replacement: If P(x, y) denotes some property such that for every x there exists a unique Y for which P(x, y) holds, then for every set A there exists a set B such that every element in A has a counterpart element in B such that P(x, y) holds for those two elements. 
    -Axiom of choice: The axiom of choice assumes that it is mathematically valid to make an infinite number of arbitrary choices. Without the axiom of choice, it is still possible to, given a collection of non-empty sets, select an element from each set if there are only a finite number of sets. In fact, even if there are an infinite number of sets, it is still valid to select one element form each set if one can define an unambiguous and well-defined rule for choosing the elements. For example, given a collection of sets such that each set contains some amount of natural numbers, one can select one element from each set by systematically choosing the smallest element from each set. However, when we consider a collection of non-empty and uncountably infinite (so no such well-defined and systematic rule for selection exists) sets, we require the axiom of choice to select an element from each set, as doing so means we are making an infinite number of arbitrary choices at once. 
        -Banach Tarski paradox: This paradox asserts that it is possible to partition a mathematical sphere (ie the collection of points equidistant from a fixed center point) into a finite number of pieces and reassemble two such mathematical spheres using only the initial partitions, and applying only the simple transformations of rotation and translation. This is a paradox because it doubles the total volume without introducing any new objects whatsoever. The paradox comes from deep foundations in number theory; the mathematical set of points making up a mathematical sphere have infinite cardinality, and as such are so infinitely complex that no subset of this sphere has a measurable volume. 
            -Analogy with natural numbers: The theorem is called a paradox because it clashes with our intuitive notion of mass and volume. However, physical objects are never infinite (as they have finite atoms that compose them) where as mathematical objects are, and infinite objects rarely behave intuitively. The cardinality of a mathematical sphere is greater than that of the natural numbers, but a useful parallel can still be drawn. The natural numbers are infinite in size, and it seems intuitively obvious that the set of natural numbers is bigger than (daresay exactly twice as big as ) the set of even numbers. However, since every even number x can be mapped to a unique natural number 2x, each element in the set of even numbers has exactly one counterpart element in the set of natural numbers, and so the sets are the same size. We have thus duplicated the size of the natural numbers using only those natural numbers. 
            -The paradox is only possible if we accept the axiom of choice. 
-General Leibniz rule: A rule that generalizes the product rule for differentiation and pairs the transformation of differentiating products (of only two functions) with the idea of binomial expanding sums (of two variables). The rule is that the nth derivative of the product of functions f and g = sum from k = 0 to k = n of (n choose k)*(f^(k) * g^(n - k)), where f^(k) denotes the k-th derivative of the function f. 
-Quaternions: Quaternions were formulated by Hamilton in his search for a three-dimensional geometric equivalent to the complex numbers (which are represented geometrically in a two-dimensional plane, called the Argand plane). 
    -History: The algebraic structure of the complex numbers was well understood at the time of the formulation of the quaternions, and it was well known that complex numbers could represent 2-D rotations. In fact, a consequence of this is the geometric intuition for multiplication by -1 manifesting itself as a 180 degree rotation, and as such a multiplication by i (sqrt(-1)) as a 90 degree rotation into a number line orthogonal to the reals. Hamilton wanted a way to similarly represent a 3-D rotation with a 3-D number system, but it was proven that such a system could not exist. However, it turned out that a 4-D number system could work, at the price of commutativity of multiplication. The central motivation behind the quaternions was to extend the complex numbers. 
        -Proof that a 3-D number system with 3 bases cannot exist: Many proofs exist, but a simple one is as follows. We will prove by contradiction that a field in 3-D generated by 3 bases cannot exist because it is impossible for the multiplication operation to be closed. First, assume there exists a field D generated by {1, i, j}, such that i^2 = j^2 = -1. We will look at all possible products of ij and prove that each is impossible, and therefore j cannot be an element of D and so D cannot exist. Since D is generated by {1, i, j}, it follows that ij must be proportional to each of the generators (and their opposites). 
            1. First, check the case that ij = +/- 1. This implies that iij = +/- i, and therefore that j = -/+ i. If j is a multiple of i, then they are not linearly independent and so j cannot be a basis for D. 
            2. Now check if ij = +/- i. This implies that iij = +/- i^2 = -/+ 1, which implies j = +/- 1. Again, j is proportional to another generator of the field (namely, 1), and so isn't linearly independent with it. Thus, j cannot be a generator of D.
            3. Third, check if ij = +/- j. This implies that ijj = +/- j^2 = -/+ 1, which implies that i = +/- 1. This shows that this time i is proportional to another generator of D (namely, 1) and so isn't linearly independent with it. Thus, i cannot be a basis for D.
            4. Lastly, it is trivial to see that ij = 0 cannot be the case, for them either i or j would be equal to 0, which contradicts our assumption that i^2 = j^2 = -1. 
         We have gone through all possibilities for the product, and none are consistent with a properly defined 3-D field. Thus, such a field cannot exist.
    -Construction: The quaternions were meant to extend the complex plane, which is defined as a field C such that C: {a + ib | a, b are elements of R, i^2 = -1}. We want a new field, H, such that C is a subset of H and that the operations of field H are compatible with those of C. Note that C can be thought of as a vector space instead of a field (different ways of looking at the same thing), in which case the space is generated by the bases {1, i}. As shown above, H cannot be a degree 3 vector space (ie a space with 3 bases/generators) but it can be degree 4. We can define H: {a + ib + jc + kd | a, b, c, d are elements of R, i^2 = j^2 = k^2 = ijk = -1}. Equality and addition are defined very intuitively (two quaternions are equal if each of their a, b, c, d are equal, and to add them one simply combines like terms) but multiplication is tricky because the field of not commutative. One must multiply out the quadnomial product given by multiplying two quaternions, being careful to avoid commutativity. We also define the norm of a quaternion: if q = a + ib + jc + kd, the norm of q is denoted with a q with a bar over it, and is equal to a - (ib + jc + kd). The magnitude of a quaternion is given by its product with its own norm, as in the complex plane. 
    -Geometric intuition: Although the condition that i^2 = j^2 = k^2 = ijk = -1 seems to imply that i = j, or j = k, and so on, this is not the case (especially since multiplication is not commutative). Each i, j, and k are unique elements that separately generate different orthogonal parts of the space, just as x, y, and z do in three-dimensional Cartesian space. With the complex plane, multiplying by i rotated the number line 90 degrees into the second dimension. With the quaternion field, multiplying by i still rotates 90 degrees into the second dimension, but multiplying by j rotates 90 degrees into another second dimension that is orthogonal to both the dimension rotated into by multiplication by i and the real number line. Multiplication by k rotates the real number line into yet another second dimension, orthogonal to each of the previous three. Each of the quaternion units simply represents the basis for one of each of the four dimensions of the vector space, and the set together {1, i, j, k} generates the entire space under the special condition that i^2 = j^2 = k^2 = ijk = -1. All identities for multiplication in the quaternion field can be derived from this special condition. 
    -Relationship to the 3-D vector cross product: In three dimensions, we can view vectors as the imaginary component of a corresponding quaternion. Specifically, a vector <x, y, z> can be represented as q = xi + yj + zk (this is where the i, j, k unit vector notation originated, in fact). The cross product of two vectors then corresponds to the imaginary part of the product of their corresponding components.
    -Octonions: (***INCOMPLETE***)
        -Relationship to the 7-D vector cross product: In the same way that 3-D vectors can be represented as the imaginary part of corresponding quaternions and the 3-D cross product is represented as the imaginary component of the product of two quaternions, 7-D vectors can be viewed as the imaginary component of a corresponding octonion. The 7-D cross product is then viewed as the imaginary component of the product of the corresponding octonions for each of the vectors.
-Equatorial coordinate system: A coordinate system used to define locations in the sky relative to observers. 
    -Terminology: 
        -Relative landmarks: The landmarks of the celestial sphere that depend on the observer's position on earth. These points are always seen in the same apparent positions no matter where the observer is, but will be at different parts of the objective celestial sphere depending on the observer's location. Observers in different locations will have different celestial spheres.
            -Horizon: Where the earth and sky meet. It is the lowest line of sight possible, and is aimed at the boundary between the visible celestial sphere and the part of the sphere blocked by the earth.
                -Local horizon: The horizon as seen by an observer; it may include disfigurements such as mountains, buildings, trees, etc.
                -Idealized horizon: The horizon as would be seen if the earth's terrain were completely flat. 
            -Altitude: The angle between it and the horizon. 
            -Cardinal points: The points on the horizon that are due north, south, east, and west. 
            -Zenith: The point in the celestial sphere directly overhead the observer. 
            -Meridian: The great circle that passes through the North point, the South point, and the zenith. It lies on the celestial sphere. 
        -Fixed landmarks: Landmark points on the celestial sphere that are fixed on the celestial sphere, and thus will appear in different positions in an observer's view depending on where the observer is. 
            -Celestial equator: The equator of the celestial sphere; it is the projection of the earth's equator onto the celestial sphere. No matter where an observer is on the earth, the celestial sphere will pass through the East and West points on the horizon. 
            -Celestial poles: Projections of the earth's north and south poles onto the celestial sphere. 
                -Polaris: The north star. It is a star that is almost directly above the earth's north pole, and so moves very little throughout the year. The angle between polaris and the horizon is the measuring observer's latitude on earth. 
    -Declination: The upward angle between the celestial equator and the position; how far up you must look to see the star.
    -Right ascension: The angle along the celestial equator that a star's declination's great circle intersects. It is measured eastward along the celestial equator, with a zero point defined as the vernal equinox point on the celestial sphere (it lies on the celestial equator, of course). 
        -Vernal equinox: The point of intersection between the celestial equator (an extension of the earth's equator into a circle of infinite radius) and the path of the earth around the sun, ie the sun's ecliptic. Note that since the earth's axis is tiled (at an angle of 23.5 degrees), these two circles are not co-planar.
-Littlewood's law: A probabilistic argument that asserts that events normally classified as "miracles" by the general population can be statistically expected to occur to the average individual at a frequency of about once per month.
    -Process: A "miracle" is defined as an event with an occurrence probability of 10^(-6), ie one in a million. In the ~15 hours that a human is awake throughout the day, he will be "alert" for about 8 of them, and in those 8 hours, that human will experience and process a single event per second, conservatively speaking. This means that after 35 days, a human will have experienced about one million events, one of which is statistically guaranteed to be a miracle (in the long run). This reasoning suggests that what we consider exceptional or rare is actually rather commonplace, given the sheer magnitude of how many possible events can occur throughout time.
-Halting problem: The problem of whether an algorithm with a given input will terminate in finite time (as opposed to running forever). Alan Turing proved that an algorithm that solves the halting problem for any arbitrary inputted algorithm with arbitrary input cannot exist. There are no resource limitations on the computer, as long as the amount of time used is not infinite (the memory used can be). 
-Anatomy of a car crash: Survivors of traumatic vehicle accidents are often able to recall the event in extraordinary detail, and say that during the crash, it felt as though time was slowing down. However, Ford recently conducted research in the anatomy of a car accident, and found that in reality, the entire event is over before the conscious human brain even registers it; it is only later as subjects replay the event in their minds that they remember it at all. Ford provided a breakdown of the process that occurs during a car crash, including how the vehicle's safety mechanisms respond (they possess 5-star safety ratings). Ford performed 90 crash tests and 5000 simulations, and used a supercomputer to sort through the data and conclude a millisecond breakdown of the event. 
    -Reconstruction of a stationary Ford Falcon XT Sedan being struck in the driver's door by a projectile traveling at 50 km/hr. Note that 1 millisecond = 1 ms = 0.0001 seconds.
        1. 0 ms: The projectile makes contact with the side door at 50 km/hr.
        2. 1 ms: A pressure wave travels throughout the door's metallic structure and is detected by the door pressure sensor.
        3. 2 ms: The abrupt impact of the projectile produces interaction with the car's A-, B-, or C-pillar (see below), depending on which part of the car's side is struck. An accelerometer sensor detects the sudden acceleration. 
            -Automobile pillar: A vertical closed steel beam/structure that runs from the floor panel of the car up to the bottom of the window and greenhouse area of the door; it is welded to the bottom of the car and the underside of the door window.
        4. 2.5 ms: The kinematic acceleration wave and pressure wave reach the car's center and are detected by the central computer as crash vibrations.
        5. 5 ms: The central computer activates the crash computer, which cross-references the detected shock waves and acceleration against sub-crash events (e.g. an insignificant collision such as a basketball bouncing off the car or a shopping cart colliding with the car). The crash computer begins the algorithmic process of assigning a value to the crash to quantify the severity. 
        6. 5.5 ms: The door's intrusion structures begin absorbing and redirecting the energy from the collision. 
        7. 6.5 ms: The crest of the shockwave front passes through and is detected by the pressure sensors. 
        8. 7 ms: The crash computer, which began computing the severity of the crash in (5), confirms that the collision is a severe crash. It begins evaluating options and responses.
        9. 8 ms: Crash computer first sends a signal to fire the side airbags, which magnify the duration of the change in momentum of the passenger and combat the acceleration, reducing chances of blunt force trauma to the anatomy of the passenger.
        10. 8.5 ms: The automobile pillar (A-, B-, or C-pillar depending on specific point of impact) begins to implode on itself as it absorbs force; energy is transferred to the cross-car load path located beneath the passengers' seats.
        11. 9 ms: The side airbag system, triggered in (9), fires.
        12. 15 ms: Further energy is diverted to the roof as the pressure wave envelops the car.
        13. 17 ms: Cross-car load path reaches the level of maximum tensile stress and pressure. The airbag has now made contact with the passenger and push the body away from impact zone.
        14. 20 ms: Car door and automobile pillar (same one in (10)) begin pushing inwards onto the passenger's seat as a result of the force.
        15. 27 ms: A pusher block located in the center of the passenger's seat automatically moves the passenger's center of mass away from point of impact. At this point, the impact velocity has effectively halved. 
        16: 30 ms: All energy from the crash has now been fully imparted onto the vehicle; the car has absorbed all of the energy. 
            -The passenger briefly experiences an enormous force equivalent to 12 g (12 times the force of his own weight, on himself).
        17: 45 ms: As the door and pillar continue to push towards the passenger's seat, the seat and passenger together move with the now deformed side structure.
        18. 50 ms: Crash computer sends signal to unlock car doors, and the passenger safety cell rebounds the doors away from the passenger. 
        19. 70 ms: Airbag begins to deflate and the passenger moves back towards the center of the car.
            -At this point, the crash is classified as complete.
        20. 300 ms: The passenger's conscious central nervous system registers the collision. 
-Rogaine: Known pharmaceutically as Minoxidil, rogaine is an anti-hypertensive (counteracts hypertension (high blood pressure)) vasodilator (widens the diameter of blood vessels). It also has the effect of stopping and reversing hair loss. 
-Bouba and Kiki effect: An experiment was carried out several times in which participants were shown two shapes, a jagged star shape and a rounded bloblike shape. They were asked to name one object "Kiki" and the other "Bouba"; which object was which name was up to them. Although results of 50% naming one object Kiki and 50% naming the other Kiki were expected, around 95 to 98% of the population named the jagged shape Kiki and the rounded blob shape Bouba. It was theorized that the strong and harsh "K" sound in Kiki has connotations of sharpness and fortitude, which are traits embodied by the jagged shape, where as "Bouba" sounds more weak and flexible with the long "o" sound in the middle and the soft "B" in the beginning.
    -Implications: The effect shows that the evolution of language was not a random process; words and sounds were associated to objects not arbitrarily, but rather based on some intrinsic link to the object's appearance and characteristics.
    -Picture for Bouba and Kiki: http://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Booba-Kiki.svg/1280px-Booba-Kiki.svg.png
-Algebra over a field: A vector spaced together with a bilinear product.
    -Bilinear product: A function from two vector spaces (technically, the Cartesian product of two vector spaces) into a third vector space, all with the same base field, such that when elements of either vector space composing the domain are held fixed, the function reduces to a linear operator. Mathematically, for a mapping B: V x W -> X, where V, W, and X are all vector spaces with the same base field, v is mapped to B(v, w) for all elements w in W, and w is mapped to B(v, w) for all elements v in V. This means that if we hold w constant, the mapping from v to the function value B(v, w) is linear, and if we hold v constant, the mapping from w to the function value B(v, w) is linear. 
-Types of clouds: The way a cloud is classified depends primarily on its height. There are ten basic types of clouds, first classified as low, middle, or high height, and then sub-classified by composition.
    -High clouds: Clouds that form above 20,000 feet in altitude. At temperatures as low as they are at this altitude, the clouds are mainly made up of ice crystals. 
        -Cirrus: Rather transparent and detached clouds that appear as delicate, thin, white filaments in the sky.
        -Cirrostratus: Transparent clouds with a whitish veil feature. They have a fibrous and smooth appearance. These clouds are long and extensive, often taking up the entire sky. 
        -Cirrocumulus: Thin, white, patches of layered clouds that do not have shading. These clouds are degraded versions of the above two and are composed of very small elements.
    -Mid-height clouds: Clouds that form between 6,500 feet and 20,000 feet in altitude. These are mainly made up of water droplets.
        -Altostratus: Gray or bluish fibrous sheets of clouds that can totally (or partially) cover the sky. They are thin, and tend to allow sunlight to pass through them. Sometimes altostratus can reach very near the ground, causing precipitation.
        -Altocumulus: White patches of layered clouds composed of laminae (rounded rolls or plates) that appear diffuse and fibrous. This is the most common mid-level cloud.
        -Nimbotratus: A continuous and large rain cloud that takes up the entire sky. They are formed when altostratus clouds thicken and thicken into rain clouds, and are dark gray clouds diffused by falling rain or snow. These clouds are thick enough to disallow sunlight from passing through. 
    -Low clouds: Clouds that form under 6,500 feet in altitude. These are also made up mainly of water droplets. 
        -Cumulus: Detached and dense clouds with sharp outlines. These clouds develop vertically as well sa horizontally, and have bulging tops in the shape of domes and towers. The bases of the cloud, most often seen by observers on the earth, are completely dark and horizontal, though the sunlight tops appear brilliant. 
        -Stratus: A gray cloud layer which can produce light rain or snow grains (and possibly ice prisms) if thick enough. 
        -Cumulonimbus: The thunderstorm cloud. This cloud is thick and dense, and rises vertically to form a huge tower. Whereas the top is fibrous and smoothed, the bottom is flattened into a plume, akin to cumulus. These clouds sometimes produce rain, and can also produce hail or tornadoes.
        -Stratocumulus: Grayish or whitish patches of clouds which follow a tessellation pattern in the sky (typically honeycomb), and appear as rounded masses. They are not fibrous, but are composed of small elements.
-Entrapment: The practice of a law enforcement official using his power to compel a person to commit a crime that the person would normally not have committed. It is a generally accepted defense against criminal liability if it can be proven. 
-Propositional logic: A logical system that operates on the level of sentences, ie the smallest unit of reasoning in propositional logic is a sentence. Specifically, it is a proposition (a sentence that is either true or false); it is not concerned with substructures within the sentence. The logic is used to determine the truth or falsehood of a sentence based on a set of propositions known to be true or false. 
    -Propositions: A proposition must be rigorously quantifiable and have a truth value, though it cannot be true and false simultaneously. 
        -Examples: "2 + 2 = 5", "3 = 3", "Grass is green" are all propositions (with truth values false, true, true, respectively), but "close the door", "is it hot outside?", and "x = x" are not propositions. The last is not a proposition because the term "x" has not been rigorously quantified, and the degree of uncertainty in what "x" actually is, ie the essence of "x", means it is not a proposition. Thus, although "3 = 3" is a proposition because it is known with complete certainty what "3" is, "air equals air" is not, because there is a degree of uncertainty (a mass of air, or the concept of air, ... ?, etc.).
    -Elements of propositional logic: Though propositions are the most basic unit of propositional logic, larger and more complex sentences can be constructed using various propositions and the idea of a connective to link them together logically.
        -Connective: A logical unit used to modify or link propositions in a logical framework. The five basic connectives of propositional logic are: NOT (negation; reverses the truth value), AND (constructs a sentence, from multiple propositions, whose truth value depends on those of the propositions constructing it (strictly so)), OR (an operator used to build a sentence from multiple propositions; the sentence's truth value depends weakly on those of the propositions constructing it), imply, biconditional.
            -NOT: For a statement P, NOT P = True when P = False, and NOT P = False when P = True.
            -AND: For statements P, Q, the statement "P AND Q" is True when both P = True and Q = True.
            -OR: For statements P, Q, the statement "P OR Q" is True when P = True and Q = False, when P = False and Q = True, and when P = True and Q = True.
            -IF-THEN: For statements P, Q, the statement "IF P THEN Q" is true when P = True and Q = True, when P = False and Q = True, and when P = False and Q = False.
            -IFF: (short for IF-AND-ONLY-IF) For statements P, Q, the statement "P IFF Q" is True when P = True and Q = True, and when P = False and Q = False.
        -Converse and contrapositive: 
            -Converse: For the proposition "IF P THEN Q", the converse is the proposition "IF Q THEN P". 
            -Contrapositive: For the proposition "IF P THEN Q", the contrapositive is the proposition "IF NOT Q, THEN NOT P". The contrapositive is always true when the statement is true.
    -Tautology: A proposition that is always true.
     Contradiction: A proposition that is always false. 
     Contingency: A proposition that is neither a tautology nor a contradiction. 
-Predicate logic: A type of logic stronger than propositional logic, as it allows for the use of variables and also addresses the idea of partiality (eg. The statement "Not all integers are even" is equivalent to "Some integers are even"). 
    -Predicate: A verb phrase template that describes a property of something or a relationship between something (often represented by variables). 
        -Examples: The sentences "The car Tom drives is blue", "The sky is blue", and "This book is blue" all come from the template "is blue", which describes a property. Thus, it is a predicate.
        -Use: Predicates are useful because they allow for the use of variables, which can be used to create "template" sentences into which specific cases can be substituted. Such a template can even be considered a function.
            -Example: The sentences "John gives the book to Mary", "Jim gives bread to Tom", and "Jane gives a lecture to Margaret" are all of the form "x gives y to z", where x, y, and z are variables. This template can be considered a function G(x, y, z). 
        -A predicate with variables in it is not a proposition, for the reasons discussed in "Propositional logic", but they can be made propositions by either (1) assigning a value to the variable, or (2) quantifying the variable using a logical quantifier.
            -Quantifiers: There are two kinds of quantifiers. The first is the existential quantifier, "THERE EXISTS", which modifies a variable by evaluating the sentence to True when a substitution for the variable that makes the statement True exists. There is also the universal quantifier, "FOR ALL", which evaluates the sentence to True if the sentence is True for every possible substitution for the variable being modified. 
-Lojban: A language that was constructed in 1987 to be completely syntactically unambiguous, and is based on rigorous predicate logic. It is accepted as the language with the most complete grammar of all languages. Each word of the language has exactly one grammatical interpretation, and multiple words can relate to each other in at most one grammatical way; no other language can claim this.
    -Vowel: A sound produced with an open vocal tract without a build-up of air pressure above the glottis (twin foldings of mucous membrane stretched horizontally in the larynx, along with the space in between them). A consonant is the sound formed with a constriction in the vocal tract.
-Asset forfeiture: (A.K.A. civil forfeiture) The legal procedure by which a government confiscates a person's assets, sometimes in the absence of a conviction, charges, or even evidence. In the US, many officers of the law have confiscated money and other assets without charging the previous owner with a crime, purely out of highly improbable suspicion. This practice has caused civil forfeiture, though instrumental in the fight against crime (specifically against the drug trade), to come under severe criticism. Legally speaking, it is the assets themselves that are on trial and not their owners, as ridiculous as this sounds. Cases literally have names akin to "United States v. Twenty-Five Hundred Dollars" or "United States v. Thirty Thousand Pounds of Shark Fins".
-Blackbody radiation: A blackbody is an idealized (and non-existent) object with perfect absorption, ie it absorbs 100% of electromagnetic radiation (ie light) incident upon it, reflecting 0% of it. Thus, at room temperature, it would appear completely black, hence the term "blackbody". Blackbody radiation is the thermal radiation emitted by a blackbody by virtue of the fact that it has a positive temperature. Although blackbodies do not actually exist (but they are useful approximations for many real objects; they are used as first approximations for stars, and black holes are near perfect blackbodies (if not for Hawking radiation)), any matter with positive temperature (ie above absolute zero; that is, all objects that exist) emit thermal radiation, just because of the fact that they have thermal energy in their structure at all. 
    -Explanation: The thermal radiation emitted by all objects with positive temperature is a consequence of the radiative distribution of entropy, and therefore of the second law of thermodynamics. It is the spontaneous conversion of thermal energy to electromagnetic energy. As this energy has to come from somewhere, the object's temperature will asymptotically cool down and approach thermal equilibrium with its surroundings (this process is called radiative cooling). When the object is in thermal equilibrium with its surroundings, it is still losing energy through blackbody radiation, but it is getting energy back from its surroundings at an equal rate. At this point, neither the object nor the environment cools any further since energy is simply exchanged back and forth through radiative cooling and thermally heating.
    -Blackbody spectrum: Blackbody radiation is not emitted in a single wavelength of light at once, depending on temperature. Rather, for a given constant temperature, the radiation emitted follows a distribution of wavelengths, that peaks at a specific wavelength. All blackbodies emit radiation at all wavelengths, but their peak wavelength (ie the wavelength they emit most of their radiation at) depends on the temperature. Thus, the blackbody spectrum is a family of distributions that depend on the temperature of the body; each temperature corresponds to a particular distribution. As temperature rises, the distributions tend to get taller, meaning more radiation is emitted overall, and the peak wavelength also shrinks (ie the maximum of the distribution shifts left, towards lambda = 0, as temperature rises).
        -Planck's law: A body's blackbody spectrum depends only on its temperature, and not on any other characteristics of the body. Specifically, B_(lambda) = radiation for wavelength lambda = (2hc^2 / (lambda)^5)) * (1 / (exp(hc / ((lambda)*k_B*T)) - 1))
        -Wien's displacement law: The peak wavelength of a distribution and the temperature that corresponds to that specific distribution are inversely proportional. Thus, (lambda_max)*(temperature) = constant = 2.8977721 * 10^(-3)
    -Equipartition theorem: (of classical physics) Stated simply, the principle that each degree of freedom in a gas, resulting from various modes of motion by the particles of the gas (eg vibration, rotation, translation, etc.), contribute equal amounts of energy to the total thermal energy and temperature. 
-Intuition for product rule for differentiation: The product rule can be thought of as a consequence of the distributive property of multiplication. In differential notation, appending a d(...) to an argument denotes an infinitesimal change in the argument, ie a differential for that quantity. Notice that for functions f and g of real parameter t, d(fg)/dt = ((f + df)(g + dg) - (fg)) / dt = ((fg + g*df + f*dg + df*dg) - fg) / dt = g*(df/dt) + f*(dg/dt) + (df*dg)/dt = g*(df/dt) + f*(dg/dt) since the last term, the product of two differentials, is so small it can be neglected.
-Black death: An extremely deadly pandemic that erupted in Europe from 1346 to 1353. The plague began in Eastern Asia, but quickly spread to Europe via the Silk Road. It was mainly carried overseas through rats and fleas that snuck onboard ships. The plague reduced the world population from 450 million to between 350 and 375 million people; approximately one in three people in Europe died, and it took 150 years for Europe's population to fully recover. 
    -Causes: The pandemic was caused by the bacterium Yersinia pestis.
    -Symptoms: The bacterium causing the pandemic affected the infected with one of three possible plagues. The plagues differ in location of infection: the bubonic plague infects the lymphatic system, the pneumonic plague infects the respiratory system, and the septicaemic plague, infects the blood stream.
        -Bubonic plague: The most common type of plague, the Bubonic plague had broken out 800 years earlier (Plague of Justinian) in the Byzantine empire, and it continued to occur recurringly in Europe until the beginning of the 1900s. It causes the infected to have buboes (hence the name "Bubonic"), or enlarged lymphatic glands that blacken over time as the skin decays. It has a 50% fatality rate, and kills within 3 to 7 days.
        -Pneumonic plague: The Y. pestis bacterium is transmitted in droplets through coughing; this mode of transmission leads to the pneumonic plague, which has the symptoms of severe coughing with hemoptysis (coughing up blood). Initial signs include fever, headaches, fatigue. These symptoms quickly progress to chest pain, bloody sputum, and shortness of breath. Untreated infected die in 2 to 4 days from shock or respiratory failure.
        -Septicemic plague: This plague is mostly spread by flea bites; it causes internal bleeding, low blood pressure, nausea, organ failure, shock, difficulty in breathing, gangrene, bleeding, etc. 
-Spanish flu: The flu pandemic lasted from January of 1918 to December of 1920, right after World War I, and was one of the most devastating natural disasters in human history. The H1N1 influenza virus was first indentifiedd in the US military base Fort Riley in Kansas, but quickly spread worldwide. About one third of the population was infected, with 10 to 20 percent of the infected dying; thus, the virus claimed the lives of between 50 and 100 million people. 
    -H1N1 virus: A type of the influenza A virus; is it called H1N1 because it contains the glycoproteins (protein structures with saccharaide chains and polypeptide side chains, often used for signal transduction) Haemagglutinin and Neuraminidase (type 1 of each, hence H1N1). The first protein, Haemagglutinin, clumps blood cells together and allows the virus to bind to the cells, whereas the second protein, Neuraminidase, acts as an enzyme and helps the virus move through infected cells and bind to new host cells. 
    -Killing mechanism: The virus actually kills by causing the body's own immune system to overreact; thus, healthy adults with strong immune systems are actually more adversely affected than children and elderly, who have weaker immune systems. The virus kills by causing a cytokine storm (A.K.A. cytokine cascade or hypercytokinemia). Normally, cytokines are released as a part of the immune response; when they are released, they signal T-cells and macrophages to surge to the site of release/infection. The cytokines also have the side effect of stimuLating the release of even more cytokines, resulting in a positive feedback loop that is regulated by the immune system. In a cytokine storm, this feedback mechanism is not regulated, and too many immune cells accumulate in one place. If this occurs in a key area, such as the lungs, heart, or liver, it can lead to rapid organ failure and death.
    -Name: The pandemic is called the "Spanish flu" not because of any connection to Spain, but because during the pandemic, most the countries in World War I (when the virus erupted) were censoring the release of the virus. Neutral Spain was not, and so the relatively high amount of media attention funneled towards the virus earned it the name "Spanish flu".
-Moravec's paradox: The counterintuitive observation in the fields of artificial intelligence, robotics, and computer science that higher cognition and reasoning turn out to require small amounts of computational power and are relatively easy to implement, compared to the enormous computational power required and the extraordinary difficulty involved with algorithmic implementation of tasks normally considered basic and easy. Whereas tasks considered complex such as engineering air travel, solving games of chess, performing well on intelligence or creativity tests, analyzing stock behavior, solving entrepreneurial problems, etc. are relatively easy to implement and execute. However, seemingly basic tasks that a one year old human picks up, such as facial recognition, responding to questions and holding a (meaningful) conversation, walking across a room, catching a ball, etc. are among the most difficult problems in computer science, engineering, and general science that have ever been encountered. Many consider this result to be the most important conclusion drawn in the field of artificial intelligence to date. In general, it appears the unconscious human tasks are far more complex than conscious or neocortical reasoning tasks. 
    -Cause: The most basic human tasks that are so difficult for us to implement only appear easy and effortless to humans because the evolutionary process of natural selection has had billions of years to, through random variation and trial and error, perfect the skill. All these basic skills are taken care of by the unconscious brain, which has evolved over billions of years to efficiently solve problems that are objectively (compared to abstract reasoning problems) incredibly difficult. Therefore, it follows that the difficulty in implementing a task solved by biology should be roughly proportional to the amount of time the task has been evolving. 
    -Examples: Skills that have been evolving for millions of years, such as recognizing a face, moving around in space, judging peoples' motivations, catching a ball, recognizing a voice, setting appropriate goals, paying attention to things that are interesting; anything to do with perception, attention, visualization, motor skills, social skills etc. are actually very difficult problems but have evolved for a very long time to be solved efficiently. 
-Roche limit: The closest distance an astronomical object, held together by gravity (and probably in the shape of a perfect sphere), can get to another celestial body without disintegrating. If the body gets too close to another celestial body with a very strong gravitational attraction, then the more massive body's tidal forces on the smaller body will overwhelm the smaller body's own self-attraction force of gravity, causing the smaller body to rip apart. 
    -Rings: When small bodies of rock that are orbiting a planet get too close to the planet, the planet's tidal forces on the moon overcome the moon's own gravitational self-attraction that holds it together, and therefore the moon breaks apart. As the pieces of the moon closest to the planet continue orbiting faster than those furthest from the planet, the difference in velocity eventually leads to uniform rings. This is how Jupiter and Saturn formed their rings.
-N-back test: A common and rather effective test for both testing and improving working memory. The test works by presenting a sequence of stimuli, and asking the subject to indicate whenever a particular stimulus is repeated after exactly n steps. For example, if the stimuli are presented auditorily, a subject might listen to random letters (from the alphabet) being spoken at a fixed rate (eg 1 letter every 3 seconds), and is tasked with indicating whenever a letter is repeated after n steps. So, if n = 3, then in the sequence (a, g, i, o, l, I, f, f, r, o, F, R, p, t, e, j, a, a, q, w, A, h, r), the capital letters would be those that the subject is supposed to indicate, because those letters also occurred exactly 3 letters prior.
    -Dual n-back test: The same n-back test, except instead of one sequence, two sequences of different sensory inputs (usually one auditory and one visual) are shown simultaneously, and the subject must indicate whenever the current visual stimulus had occurred n steps prior, AND when the current auditory stimulus occurred n steps prior. Commonly, the auditory sequence is the random alphabetic sequence described above, and the visual sequence is a sequence of blocks in a grid (whenever a block appears in the same place on the grid that a block had appeared n steps prior, it is meant to be indicated by the subject). By using two different stimuli simultaneously, the task becomes much more difficult. The test can be made additionally difficult by using different "n" values, eg asking when the same auditory stimulus repeats after 3 steps but when the visual sequence repeats every 5 steps.
    -Testing of working memory: If n = 1, then the subject can simply remember the previous stimulus and replace it with the next stimulus each time, and so only has to verify whenever the same stimulus is repeated consecutively, a trivially easy task. However, when n > 1, more stimuli must be remembered simultaneously each time, and the appropriate stimulus must be replaced with each new stimulus, and the subject must cross reference even more stimuli simultaneously. This puts a large amount of strain on short term memory, working memory, and processing speed. 
    -Memory improvement: In addition to testing certain aspects of cognition (namely, short term and working memory), the test has shown to improve these aspects with time and practice. 
-Homemade hydrophobic solution: A hard and smooth surface, such as glass, can be converted into a hydrophobic surface by first spraying on spray-on clear coat (to close any micro-fissures, cracks, and imperfections in the glass as well as act as a binding agent for the antiperspirant) and then spraying on aerosol antiperspirant, and waiting about an hour to dry. 
-Lucky Luciano: (Full name Charles "Lucky" Luciano; born Salvatore Luciana) Widely accepted as the father of modern organized crime. Luciano established the first Commission (the sub-government within the American mafia; see below). He partitioned New York into the modern Five Families. Luciano grew up in New York after his parents immigrated to the US when he was 10, and started his first gang in boarding school as a teenager; instead of getting involved in petty crime like other gangs, Luciano offered Jewish students protection from Italian and Irish gangs at a price of ten cents per week. During Prohibition in 1921, Luciano began his own bootlegging operation, along with his friends (that he met through the Five Points Gang) Frank Costello and Vito Genovese. They were financed by Arnold "the Brain" Rothstein, for whom Luciano was working at the time. Rothstein groomed and mentored Luciano, saving his reputation amongst the criminal community (after Luciano ruined it by botching a drug deal) and teaching him to dress properly. By 1925, Luciano had a net income of $4 million from his bootlegging operation, with a gross income ot $12 million and $8 million in bribery costs for the police and polciticians, bleeding money, contract killings, etc.
    -Rise to power: After Rothstein was murdered, Luciano went back to work for Joe Masseria, for whom he was working before he went to Rothstein. Although he quickly moved up the ranks in Masseria's organization, Luciano and his other young ganster friends did not agree with the traditional, "Mustache Petes", old world Mafia principles that Masseria (and his rival, Salvatore Maranzano) adhered to; he believed in upholding "honor", "dignity", and "pride", and refused to work with non-Italians. As the rivalry between Maranzano and Masseria (which started when Maranzano refused to pay commission to Masseria) escalated into the the Castellammarese War. These Old World principles were also uneducated and sloppy; Luciano and his friends wanted to use the skills Rothstein had taught them, as well as new techniques of their own, to escalate their gang into a criminal empire. Luciano and his young mobster friends began to be known as the Young Turks. They followed the ideology that the rampant greed and conservatism of their bosses was keeping the gang poor, and hoped to form allegiances with other Jewish and Irish gangs, and merge resources. Luciano made a deal with Maranzano to betray and have Masseria murdered in exchange for control of Masseria's gang. The tactic worked, and Luciano was in control of Masseria's gang. Maranzano divided New York into the Five Families. Although he promised equality among the gangs, he later declared himself the most powerful and inflated his income at the expense of other gangs. Luciano accepted the changes at first, but quietly planned to murder Maranzano. Maranzano suspected Luciano's motives and attempted to murder him, but the attempt failed and Luciano then successfully had Maranzano murdered. After his assassination, Luciano became the most powerful mobster in America. He abolished the "Capo Di Tutti Capi" (literally, "boss of all bosses") regime among the gangs, realizing that it caused violence and friction. Luciano did not want to declare himself the most powerful mob boss and end up making himself a target. Luciano was later imprisoned for a number of years, but struck a deal with the US government during World War II. He provided valuable Italian intelligence to the US, as well as control of New York City waterfronts, which he was still in control of even in prison. In exchange, the US government commuted his sentence and deported him to Italy. 
    -The Commission: A "Board of directors" used by various Mafia families across the United States as a governing body amongst organized crime, and to mediate conflicts between families. It was the successor of the "boss of all bosses" hierarchical structure, which often lead to conflicts and inter-Mafia violence. The Commission meets every five years, though its last known meeting was in 1985. Its main purpose is to oversee all organized criminal activities in the United States. Lucky Luciano first proposed the idea of a Commission, and was appointed its Chairman. The Commission consisted of seven crime families: New York's Five Families (Luciano, Vincent Mangano, Tommy Gagliano, Joseph Bonanno, and Joe Profaci), Chicago Outfit boss Al Capone, and Buffalo family (settled in Buffalo, New  boss Stefano Magaddino.
-Differentiation under the integral sign: An integration technique that applies to definite integrals. The method essentially takes the initial integration problem, generalizes the function to a higher dimensional function by introducing a parameter, uses calculus on the parameter to make the function easier to integrate, and then eliminates the parameter by choosing a special value for it such that the desired answer is obtained. The problem is solved by first being made more general, solved using easier and more general techniques, and then re-specializing to derive an answer to the old problem. 
    -Theorem: Leibniz discovered the theorem upon which this idea rests: (d/dt)*(integral of f(x, t) dx) = integral of d/dt(f(t, x)) dx. Note that on the right side, the derivative with respect to t is a partial derivative. The idea behind the technique is best illustrated through examples of usage.
    -Example 1: Integrate sin(x) / x from 0 to infinity.
        1. First, generalize the inner function f(x) = sin(x)/x to the function f(x, t) = e^(-tx)*sin(x)/x. 
        2. Let G(t) = integral from 0 to infinity of f(x, t) dx = integral from 0 to infinity of e^(-xt)*sin(x)/x dx.
        3. Differentiate both sides with respect to t: d/dt(G(t)) = d/dt(integral from 0 to infinity of e^(-tx)*sin(x)/x dx), which implies G'(t) = - integral from 0 to infinity of e^(-tx)*sin(x) dx.
        4. Now we can simplify the easy integral on the right side: - integral from 0 to infinity of e^(-tx)*sin(x) dx. We repeatedly integrate by parts until we obtain: G'(t) = - ( (t*sin(x) + cos(x))/(1 + t^2) ) from x = 0 to x = infinity. This implies G'(t) = - 1/(1 + t^2).
        5. Now we integrate G'(t) to obtain G(t): G(t) = integral of G'(t) dt = integral of (- 1/(1 + t^2)) dt = - arctan(t) + C, t > 0.    
        6. Now back-substitute our original definition of G(t) from step (2): integral from 0 to infinity of e^(-xt)*sin(x)/x dx = - arctan(t) + C.
        7. Let t = infinity. The function e^(-xt)*sin(x)/x = 0 when t = infinity because the exponential term decays to zero and the sin(x)/x term is bouned by -1 and 1. The right hand side simplifies to - arctan(infinity) + C = -pi/2 + C. Setting the left and right sides equal, 0 = -pi/2 + C. Therefore, C = pi/2.
        8. Now that we've solved for the arbitrary constant, we go back to our relation: integral from 0 to infinity of e^(-tx)*sin(x)/x dx = pi/2 - arctan(t). Recall that the original problem we're trying to solve is to integrate sin(x)/x from 0 to infinity, so we must choose a value for t that will simplify the right hand side to the integral we want. This value is, of course, t = 0.
        9. Setting t = 0, e^(-xt) = 1, and therefore integral from 0 to infinity of sin(x)/x dx = pi/2 - arctan(0) = pi/2.
    -Example 2: Integral (x^2 - 1)/ln(x) dx from 0 to 1.
        1. Let I(t) = integral from 0 to 1 of (x^t - 1)/ln(x) dx.
        2. Then: d/dt(I(t)) = I'(t) = d/dt(integral from 0 to 1 of (x^t - 1)/ln(x) dx) = integral from 0 to 1 of d/dt((x^t - 1)/ln(x)) dx = integral from 0 to 1 of (ln(x)*x^t)/ln(x) dx = integral from 0 to 1 of x^t dx = (1/(t + 1) * x^(t + 1)) from x = 0 to x = 1 = 1/(t + 1).
        3. Now, integrate with respect to t so we can recover I(t): I(t) = integral of I'(t) dt = integral of 1/(t + 1) dt = ln(t + 1) + C.
        4. To eliminate the arbitrary constant, notice that if we let t = 0, then the integrand in I(t) = integral from 0 to 1 of (x^t - 1)/ln(x) dx simplifies to 0; therefore, I(0) = 0. Thus, I(0) = ln(0 + 1) + C = 0. Since the natural log of one is zero, C = 0.
        5. Therefore, I(t) = ln(t + 1). Now we simply choose the appropriate value for t. Notice that I(2) is the integral we wanted to solve for in the first place.
        6. Therefore, integral from 0 to 1 of (x^2 - 1)/ln(x) dx = I(2) = ln(2 + 1) = ln(3). 
    -As can be seen from the examples, it can be tricky to figure out what parameter to introduce. The idea is to chose a parameter such that, when the function is differentiated with respect to the parameter, the integral simplifies to something that is easily integrated with respect to x.
-Rorschach test: A psychological test created by Hermann Rorschach designed to understand a person's underlying thought processes and personality traits, even without the subject volunteering this information for whatever reason. The test consists of a series of inkblots, which form rather abstract and nebulous black-and-white images. Subjects are asked what scenes, pictures, or likenesses they can see within the inkblots. Based on these answers, psychologists use psychological interpretation and/or complex algorithms to extract meaning about the subject. 
-Thematic apperception test: A very widely studied and used projective psychological test (a personality test in which the subject responds to an ambiguous question that has no real answer, so that the response itself and its specific traits can be used to analyze the subject) that shows subjects ambiguous pictures of various scenes and asks the subjects to tell a story about them. It is claimed that based on the responses, a subject's subconscious or hidden motives and character traits can be extracted, as well as the way in which he views the social world as a whole.
-Turing machine: The idea of a Turing machine was formulated when mathematicians began to ask the question "How much mathematics can be done if we, instead of using mathematical formalism, aptitude, and intuition, restrict ourselves to a simple and immutable set of instructions, ie an algorithm". A Turing machine is the simplest possible such mechanism, and is in fact the basis of all computing as it is the simplest computer. Despite its simplicity, any and every computer program in existence can be simulated on a Turing machine.
    -Formulation: Consider a (perhaps infinitely) long tape with numbered (not necessarily ordered) boxes. Each box contains a 0, a 1, or nothing.  A Turing machine can read a single square at once, and moves up and down the tape reading squares. The machine can alter the symbol in the square - this alteration typically is meant to depend on the symbol itself, thus qualifying the process as an algorithm - but no information about any other squares can influence the behavior of the machine at any one square. Theoretically, a piece of initial tape represents some problem, and the machine can continue following some algorithm that alters the symbols on the tape until finally the tape is left with a sequence of symbols that corresponds to a solution. 
    -Turing completeness: A computer program is Turing complete if it can complete a task that a Turing machine can complete. A Turing machine can complete any task a computer can do, but the converse is not true. Thus, a Turing complete computer program is held by programmers as the highest possible standard of soundness a program can achieve, aside from perhaps efficiency. 
-Beta of a plasma: For a plasmic cloud (a collection of matter in the state of plasma, ie very hot ionized particles), the beta value is a state variable for the system. It is defined as the ratio of plasma pressure (the pressure exerted by the particles in a plasma on the walls of the container, ie the average force exerted by all the particles in the plasma due to their collisions divided by the area over which the force is exerted) to magnetic pressure (the energy density (amount of energy per unit volume) of a magnetic field; this is analogous to the pressure exerted by any gas of particles, but the force exerted is carried by magnetic field lines instead of particles). Note that any matter in a plasmic state will induce a magnetic force by virtue of the motions of all the ionized/charged particles. 
-Matrix diagonalization: A diagonal matrix is a square matrix which only has non-zero entries on its main diagonal. Diagonal matrices have connections to eigenvalues because, like eigenvalues, multiplying a diagonal matrix by another matrix (of the same order) simply scales each column of the matrix by the corresponding entry on the diagonal matrix's main diagonal, the same way that eigenvectors are simply scaled when multiplied by a matrix. A matrix is thus always similar to the diagonal matrix that has the matrix's eigenvalues as entries on its main diagonal. 
    -Matrix similarity: Two matrices are similar when they both represent the same mathematical linear operator, but simply act under different coordinate systems (ie they use different bases). Formally, matrix A is similar to matrix B if there exists a matrix P (the change-of-basis matrix between the bases of A and B) such that AP = PB, or equivalently, A = P*B*(P^(-1)). So, when a matrix is similar to a diagonal matrix (which has the matrix's eigenvalues as entries on its main diagonal), the corresponding change-of-basis matrix P is the matrix whose columns are the matrix's eigenvectors (or, more technically, the bases of each eigenspace of the matrix's eigenvalues).
-Malaria: An infectious disease caused by a genus of parasitic protozoa (uniceullar eukaryotes with animalistic behavior (eg spontaneous and purposeful motion) of the genus plasmodium. The plasmodia spread the disease to vertebrate hosts using a mosquito vector; the entire disease is mosquito-borne. The mosquito's saliva contains the parasites, and these parasites enter the human body through the bite a mosquito makes when it feeds. The parasites settle in the human's liver, where they begin to reproduce. No vaccine currently exists, and the parasite often becomes resistant to antimalarial medications.
    -Symptoms: Symptoms appear about a week to three weeks after the disease has been contracted. Initially, symptoms can include headaches, fever, joint pain, nausea and vomiting, jaundice, blood in the urine, eye damage (to the retina), and seizures. 
    -Fatality: It is estimated, using genomic sequencing and migration patterns, that of all humans who have ever lived in the history of the species, about half have died due to malaria.
-Blood-brain barrier: A semi-permeable membrane of very high selectivity, located at every single artery carrying blood to the central nervous system except some parts of the hypothalamus, some capillaries near the posterior pituitary gland, the pineal gland (a small endocrine gland that produces melatonin (a hormone affecting sleep patterns and the Circadian cycle)), and the choroid plexus (a plexus (a complex and branched network of nerves) in the brain's ventricles that produces cerebrospinal fluid (a clear fluid whose purpose is to buffer the brain from immunological damage, and also helps regulate blood flow in the brain)). The barrier's purpose is to regulate which substances in the blood can go to, and therefore affect, the brain. It exists to provide an additional layer of protection to the brain against potentially harmful substances. 
    -Structure: In the blood-brain barrier, endothelial cells (flattened tiny cells that form the outer membrane (ie endothelium) of blood vessels) are connected to each other at tight junctions of very high electrical resistivity. The endothelial cells are supported by astrocytes (star-shaped cells in the brain (in fact, they are the most common type of cell in the brain); in the blood-brain barrier, astrocyte activity affects cerebral blood flow). These tight junctions allow water and lipid-soluble molecules to pass (through (passive) diffusion), but do not allow the entry of very large molecules, lipophillic molecules, and potential neurotoxins. Even many microscopic objects, such as bacteria, cannot pass. This protects the brain from many common infections, making cerebral infections very rare; if they do occur, however, they are very serious and often deadly without treatment (since antibodies and many medications are unable to cross the blood-brain barrier). 
-Dynamic optimization: (A.K.A. dynamic programming) A problem-solving methodology that involves breaking down large and complicated problems into a series of simpler sub-problems, with the assumption that the solutions to the sub-problems can be combined for a solution to the sum of the sub-problems (ie the original problem). Often, this methodology of breaking a problem down into smaller problems is recursively applied to the sub-problems of a problem until the sub-problems encountered have trivial solutions. The procedure requires two properties of the problem to be present: (1) overlapping subproblems, and (2) an optimal substructure.
    -Assumptions:
        1. Overlapping subproblems: This property confirms the repetition of subproblems throughout the series of subproblems. So, when a problem is recursively broken down into subproblems, and then those subproblems are broken down into further simpler subproblems, and so on, when some of those subproblems are duplicates of each other (ie the same problem comes up multiple times in the recursive process of breaking each problem down further). When this occurs, the same subproblem does not need to be re-solved every time it is encountered, as a naive and inefficient recursive implementation would do. Rather, a subproblem could be solved the first time, and then have its result stored in some cache. Then, when the same subproblem was encountered again in the future, instead of recomputing it, its solution can merely be searched for in the cache. When the overlapping subproblems happen to be subproblems high up in the tree of problems being broken down into further problems, this will save enormous computational resources, as the entire sub-tree of subproblems associated with a repeated subproblem will not need to be traversed; the value can simply be looked up.
            -Memoization: The process of storing the solution to every subproblem in a cache as the solutions are computed is known as "memo-izing" the problem. 
            -Example: Consider the recursive implementation to find the n-th Fibonacci number: fib(n) = 0 if n = 0, 1 if n = 1, and fib(n - 1) + fib(n - 2) otherwise. When computing fib(n) for n > 2, we fill first need to compute fib(n - 1) and fib(n - 2). However, in computing fib(n - 1), we will further need to compute fib(n - 2) and fib(n - 3). Notice now that we will be encountering the problem of computing fib(n - 2) twice: once to help compute fib(n - 1) and again to add to fib(n - 1) to result in fib(n). Instead of computing it twice, which will exponentially increase the amount of work required, we can store the value of fib(n - 2) in a cache so that when we encounter the problem again, we can simply retrieve its value instead of re-computing it.
        2. Optimal substructure: A problem has an optimal substructure if its optimal solution can be (efficiently) derived from the solutions of some set of subproblems of the problem. Informally, a problem possesses optimal substructure if the problem can be broken down into several subproblems and the optimal solutions to those subproblems can be combined efficiently to yield an optimal solution to the initial, parent problem. If a problem has optimal substructure, either dynamic programming or a greedy algorithm can produce a solution. If it can be proven, usually by induction, that locally optimal solutions to each subproblem will build on each other to produce a globally optimal solution, and will not be counterproductive in the long-term. Alternatively, if it can be proven through induction that a greedy algorithm is the best choice at each step of the problem, it should be used. When this is not the case but the problem with optimal substructure also displays overlapping subproblems, dynamic programming should be used. When neither of these conditions is satisfied, there is not systematic process to find the solution. 
    -Bellman's equation: For an optimization problem, the relationship between the values of the solutions to a problem's subproblems and the solution to the problem itself is given by a Bellman equation. 
-Cochlear implants: A small device meant to be surgically implanted above the ear and into the inner ear to aid the deaf or extremely hard of hearing in receiving sound. The quality of sound is still significantly lower than that perceived by the non-deaf population, but the implants can still help understand speech, listen to music, etc.
    -Parts: The implant can be separated into internal and external components. External components are implanted outside the body, above the ear. They include a microphone to pick up sound, a speech processor to filter sound for conversation and audible speech, and a transmitter, which is held in position by a magnet behind the earlobes and transmits power and sound signals from the sound processor to the internal components. The transmission is done through the electromagnetic induction of current. The internal components include a receiver to pick up the sound signals from the transmitter, and a stimulator to convert the signals into electrical impulses and send them to the electrodes via an internal, implanted cable. Both the receiver and stimulator are embedded in the bone under the ear. The electrical impulses are picked up by an array of 22 electrodes that are wound about the cochlea (a spiraling cavity in the inner ear). These electrodes pick up the electrical impulses from the stimulator and send the impulses directly to the brain through the auditory nerve system.
-Techniques for visualizing higher-dimensional data: Methods of graphically displaying multivariable data with over three dimensions. It is impossible to represent a Cartesian n-dimensional space for n > 3, as we live in a 3-dimensional universe (macroscopically, at least; see String theory), but various coordinate transformations and projections exist to aid the visualization process. 
    -Parallel coordinates: To draw a parallel coordinates plot for n-dimensional data, draw a 2-dimensional Cartesian plot, with two orthogonal axes. On the (positive) horizontal axis, draw n parallel vertical lines, each perpendicular to the horizontal axis. Each of these n lines will represent a different dimension. Then, to plot a point in n-dimensional space, we can label the vertical axis with a range of values, and on each parallel line, place a point on the line whose height corresponds to the value on the vertical axis that the line's dimension takes upon. Do this for every parallel line, and connect all the points. The resulting polygonal line is a representation of the n-dimensional coordinate. 
        -Example: To plot the point (10, 15, 6, 12, 3), we would first create a horizontal and a vertical axis. We would then draw five vertical parallel lines, and label them "dimension 1", ..., "dimension 5", respectively (what the lines are labeled depends on what the point represents; eg if each dimension represents the amount of a certain carbohydrate in a food, we might label the lines "glucose", "fructose", "maltose", etc.). Label the vertical axis with an appropriate range, such as 1 through 20. Then, mark the first coordinate, 10, on the first line at a height corresponding to the value a of 10 on the vertical axis. Do the same for each of the coordinates, and connect the points. This line represents the line. 
        -Limitations:
            -Notice that so far, we can only represent n-dimensional points and not n-dimensional functions (though a naive way of representing this would be to simply have n vertical parallel lines representing the n inputs and have an additional m vertical parallel lines after representing the output, and then displaying several graphs for various input points). However, we can still compare different points in n-dimensional space by plotting multiple polygonal lines in the same parallel coordinates graph.
            -When comparing different n-dimensional points, it can be difficult to compare line segments between two parallel lines to line segments between two other parallel lines, because there are other parallel lines in between. Thus, to compare different dimensions directly, you may need to switch around the order of the parallel lines relative to each other. 
        -Normalization: Often, the values on the vertical axis are normalized, meaning every coordinate is represented relatively, as a z-score, or percentile, etc. 
    -Andrews plot: Each n-dimensional point is transformed into a curve by taking its projection onto a particular Fourier series: the vector <1/(sqrt(2), sin(t), cos(t), sin(2t), cos(2t), sin(3t), cos(3t), ...>.
-Runaway greenhouse effect: A process in which a planet's Greenhouse effect (the process by which a planet's atmosphere heats by allowing the sun's optical radiation to pass through, but traps all the infrared radiation that is emitted by the planet after it absorbs and re-emits the sun's radiation) causes the surface temperature of a planet to approach the boiling point of water. When this happens, a positive feedback loop is created, since as more and more water is evaporated, the extra water vapor more effectively traps radiation, raising the temperature further, causing more water to boil, and so on until all the oceans have boiled away and the temperature reaches equilibrium. It is also possible, if the surface temperature reaches a few hundred degrees Celsius, for chemical reactions cause the carbon dioxide in rocks to exit into the atmosphere. The rocks would then sublimate.
-Polymorphic functions in Python: In Python, a polymorphic function is a function that takes an argument and does something to it, but it differs from a normal function because its argument can be multiple types of data. Regular functions can be written to operate on, say, integers or strings, but after they're defined this way, they can ONLY operate on integers and strings, respectively; they are bound by their definition to operate only on specific kinds of data. Polymorphic functions can operate on multiple kinds of data; the behavior they have on different kinds of data must be specified within the class of the data type itself, ie the polymorphic function in question (which already exists in its most general form in the object class) must be redefined within the class of the data type that you want the function to act on. Polymorphic functions always begin and end with two underscores.
    -Examples of polymorphic functions are:
        1. __len__(self): This function, equivalent to the notation "len(obj)" for an object obj, defines a measure of length for the object.
        2. __getitem__(self, index): This function, equivalent to the notation "obj[index]", returns the object at the inputted index in some collection.
        3. __add__(self, other): This function, equivalent to the notation (self + other), adds two objects.
        4. __mul__(self, other): Similar to the __add__(self, other) function, this returns (self * other).
-Physics booklist: A list of some of the best books to read to get into various subjects within the field of physics. URL: http://math.ucr.edu/home/baez/physics/Administrivia/booklist.html#classical-mechanics
    -Classical mechanics:
        1. Herbert Goldstein: Classical Mechanics, 2nd ed, 1980.
        2. Introductory: The Feynman Lectures, vol 1.
        3. Keith Symon: Mechanics, 3rd ed., 1971 undergrad. level
        4. H. Corbin and P. Stehle: Classical Mechanics, 2nd ed., 1960
        5. V.I. Arnold: Mathematical methods of classical mechanics, translated by K. Vogtmann and A. Weinstein, 2nd ed., 1989. 
        6. R. Resnick and D. Halliday: Physics, vol 1, 4th Ed., 1993
        Excellent introduction without much calculus.  Lots of problems and review questions.
        7. Undergrad level.  A useful intro to classical dynamics.  Not as advanced as Goldstein, but with real worked-out examples.
        8. A. Fetter and J. Walecka: Theoretical mechanics of particles and continua
        9. Kiran Gupta: Classical Mechanics of Particles and Rigid Bodies (1988)
    -Classical electromagnetism: 
        1. Jackson: Classical Electrodynamics, 2nd ed., 1975
        2. Purcell: Berkeley Physics Series Vol 2.
        3 Reitz, Milford and Christy: Foundations of Electromagnetic Theory 4th ed., 1992
        4. Feynman: The Feynman Lectures, Vol. 2
        5. Lorrain & Corson: Electromagnetism, Principles and Applications, 1979
        6. Resnick and Halliday: Physics, vol 2, 4th ed., 1993
        7. Igor Irodov: Problems in Physics Excellent and extensive collection of EM problems for undergrads.
        8. William Smythe: Static and Dynamic Electricity, 3rd ed., 1968
        9. Landau, Lifshitz, and Pitaevskii: Electrodynamics of Continuous Media, 2nd ed., 1984
        10. Marion and Heald: Classical Electromagnetic Radiation, 2nd ed., 1980
    -Quantum mechanics: 
        1. QED: The strange theory of light and matter Richard Feynman.
        2. Cohen-Tannoudji: Quantum Mechanics I & II&, 1977.
        3. Liboff: Introductory Quantum Mechanics, 2nd ed., 1992
        4. Sakurai: Modern Quantum Mechanics, 1985
        5. Sakurai: Advanced Quantum Mechanics 1967
        6. J. Wheeler and W. Zurek (eds.): Quantum Theory and Measurement, 1983
        7. C. DeWitt and N. Graham: The Many Worlds Interpretation of Quantum Mechanics
        8. H. Everett: Theory of the Universal Wavefunction
        9. Bjorken and Drell: Relativistic Quantum Mechanics/ Relativistic Quantum Fields
        10. Ryder: Quantum Field Theory, 1984
        11. Guidry: Gauge Field Theories: an introduction with applications 1991
        12. Messiah: Quantum Mechanics, 1961
        13. Dirac: 
            1. Principles of QM, 4th ed., 1958
            2. Lectures in QM, 1964
            3. Lectures on Quantum Field Theory, 1966
        14. Itzykson and Zuber: Quantum Field Theory, 1980
        15. Slater: Quantum theory: Address, essays, lectures.
        16. Pierre Ramond: Field Theory: A Modern Primer, 2nd edition. Volume 74 in the FiP series.
        17. Feynman: The Feynman Lectures, Vol. 3
        18. Heitler & London: Quantum theory of molecules
        19. J. Bell: Speakable and Unspeakable in Quantum Mechanics, 1987
        20. Milonni: The quantum vacuum: an introduction to quantum electrodynamics 1994.
        21. Holland: The Quantum Theory of Motion
        22. John von Neumann: Mathematical foundations of quantum mechanics, 1955. 
        23. Schiff: Quantum Mechanics, 3rd ed., 1968
        24. Eisberg and Resnick: Quantum Physics of Atoms, Molecules, Solids, Nuclei, and Particles, 2nd ed., 1985. 
        25. David Saxon: Elementary Quantum Mechanics
        26. Bethe and Jackiw: Intermediate Quantum Mechanics
        27. P.W.Atkins: Quanta: A Handbook of concepts
        28. James Peebles: Quantum Mechanics (1993)
    -Statistical mechanics and entropy: David Chandler: Introduction to Modern Statistical Mechanics, 1987
        1. R. Tolman: Prinicples of Statistical Mechanics. Dover
        2. Kittel & Kroemer: Statistical Thermodynamics
        3. Reif: Principles of statistical and thermal physics.
        4. Felix Bloch: Fundamentals of Statistical Mechanics.
        5. Radu Balescu: Statistical Physics
        6. Abrikosov, Gorkov, and Dyzaloshinski: Methods of Quantum Field Theory in Statistical Physics
        7. Huw Price: Time's Arrow and Archimedes' Point
        8. Thermodynamics, by H. Callen.
        9. Statistical Mechanics, by R. K. Pathria
        10. Hydrodynamic Fluctuations, Broken Symmetry, and Correlation Functions, by D. Forster
        11. Introduction to Phase Transitions and Critical Phenomena, by H. E. Stanley
        12. Modern Theory of Critical Phenomena, by S. K. Ma
        13. Lectures on Phase Transitions and the Renormalization Group, by N. Goldenfeld
    -Condensed matter: 
        1. Charles Kittel: Introduction to Solid State Physics (ISSP),
        2. Ashcroft and Mermin: Solid State Physics,
        3. Charles Kittel: Quantum Theory of Solids.
        4. Solid State Theory, by W. A. Harrison 
        5. Theory of Solids, by Ziman.
        6. Fundamentals of the Theory of Metals, by Abrikosov
        7. Many-Particle Physics, G. Mahan.
    -Special relativity: 
        1.Taylor and Wheeler: Spacetime Physics 
        2. Relativity: Einstein's popular exposition.
        3. Wolfgang Rindler: Essential Relativity.  Springer 1977
        4. A.P. French: Special Relativity
        5. Abraham Pais: Subtle is the Lord: The Science and Life of Albert Einstein
        6. Special Relativity and its Experimental Foundations Yuan Zhong Zhang
    -Particle physics: 
        1. Kerson Huang: Quarks, leptons & gauge fields, World Scientific, 1982.
        2. L. B. Okun: Leptons and quarks, translated from Russian by V. I. Kisin, North-Holland, 1982.
        3. T. D. Lee: Particle physics and introduction to field theory.
        4. Itzykson: Particle Physics
        5. Bjorken & Drell: Relativistic Quantum Mechanics
        6. Francis Halzen & Alan D. Martin: Quarks & Leptons,
        7. Donald H. Perkins: Introduction to high energy physics
        9. Close, Marten, and Sutton: The Particle Explosion 
        10. Christine Sutton: Spaceship Neutrino
        11. Mandl, Shaw: Quantum Field Theory
        12. F.Gross: Relativistic Quantum Mechanics and Field Theory
        13. S. Weinberg: The Quantum Theory of Fields, Vol I,II, 1995 It's the usual Weinberg stuff: refreshing, illuminating viewpoints on every page.  Perhaps most suitable for graduate students who already know some basics of QFT.  Unfortunately, this book does not conform to Bjorken-Drell metric.
        14. M.B. Green, J.H. Schwarz, E. Witten: Superstring Theory (2 vols)
        15. M. Kaku: Strings, Conformal Fields and Topology
        16. Superstrings: A Theory of Everything ed P.C.W. Davies
        17. A Pais: Inward Bound 
        18. R.P. Crease, C.C. Mann: The Second Creation 1996
        19. L. Lederman, D. Teresi: The God Particle: If the Universe Is the Answer, What Is the Question? 2006
    -General relativity:
        1. Meisner, Thorne and Wheeler: Gravitation W. H. Freeman & Co., San Francisco 1973
        2. Robert M. Wald: Space, Time, and Gravity: the Theory of the Big Bang and Black Holes.
        3. Schutz: A First Course in General Relativity.
        4. Weinberg: Gravitation and Cosmology 
        5. Hans Ohanian: Gravitation & Spacetime (recently back in print)
        6. Robert Wald: General Relativity
        7. Clifford Will: Was Einstein Right? Putting General Relativity to the Test
        8. Kip Thorne: Black Holes and Time Warps: Einstein's Outrageous Legacy
    -Mathematical methods:
        1. Morse and Feshbach: Methods of Theoretical Physics.  This book used to be hard to find, but can now be bought at feshbachpublishing.com.
        2. Mathews and Walker: Mathematical Methods of Physics.  An absolute joy for those who love math, and very informative even for those who don't.  [This has been severely disputed!--ed]
        3. Arfken: Mathematical Methods for Physicists Academic Press
        4. Zwillinger: Handbook of Differential Equations. Academic Press
        5. Gradshteyn and Ryzhik: Table of Integrals, Series, and Products Academic
        6. F.W. Byron and R. Fuller: Mathematics of Classical and Quantum Physics (2 vols) 
    -Nuclear physics:
        1. Preston and Bhaduri: Structure of the Nucleus
        2. Blatt and Weisskopf: Theoretical Nuclear Physics
        3. DeShalit and Feshbach: Theoretical Nuclear Physics
        4. Satchler: Direct Nuclear Reactions
        5. Walecka: Theoretical Nuclear and Subnuclear Physics (1995)
        6. Krane: Introductory nuclear physics
    -Cosmology:
        1. J. V. Narlikar: Introduction to Cosmology.1983 Jones & Bartlett Publ.
        2. Hawking: A Brief History of Time 
        3. Weinberg: First Three Minutes
        4. Timothy Ferris: Coming of Age in the Milky Way and The Whole Shebang
        5. Kolb and Turner: The Early Universe.
        6. Peebles: Principles of Physical Cosmology. Comprehensive, and on the whole it's quite a good book, but it's rather poorly organized.  I find myself jumping back and forth through the book whenever I want to find anything.
        7. Black Holes and Warped Spacetime, by William J. Kaufmann III.
        8. M.V. Berry: Principles of Cosmology and Gravitation
        9. Dennis Overbye: Lonely Hearts of the Cosmos The unfinished history of converge on Hubble's constant is presented, from the perspective of competing astrophysics rival teams and institute, along with a lot of background on cosmology (a lot on inflation, for instance).  A good insight into the scientific process.
        10. Joseph Silk: The Big Bang
        11. Bubbles, voids, and bumps in time: the new cosmology edited by James Cornell.
        12. T. Padmanabhan: Structure formation in the universe
        13. P.J.E. Peebles: The large-scale structure of the universe
        14. Andrzej Krasinski: Inhomogeneous Cosmological Models
        15. Alan Lightman and Roberta Brawer: Origins: The lives and worlds of modern cosmologists, 1990
    -Astronomy:
        1. Hannu Karttunen et al. (eds.): Fundamental Astronomy.
        2. Pasachoff: Contemporary Astronomy
        3. Frank Shu: The physical universe: an introduction to astronomy
        4. Kenneth R. Lang: Astrophysical formulae: a compendium for the physicist and astrophysicist
    -Numerical methods:
        1. Johnson and Rees: Numerical Analysis Addison Wesley
        2. Numerical Recipes in X (X=c,fortran,pascal,etc) Tueklosky and Press
        3. Young and Gregory: A survey of Numerical Mathematics Dover 2 volumes.
        4. Hockney and Eastwood: Computer Simulation Using Particles Adam Hilger
        5. Birdsall and Langdon: Plasma Physics via Computer Simulations
        6. PIC simulation applied to plasmas.  Source codes shown.  First part is almost a tutorial on how to do PIC.  Second part is like a series of review articles on different PIC methods.
        7. Tajima: Computational Plasma Physics: With Applications to Fusion and Astrophysics Addison Wesley Frontiers in physics Series.
    -Fluid mechanics:
        1. D.J. Tritton: Physical Fluid Dynamics
        2. G.K. Batchelor: Introduction to Fluid Dynamics
        3. S. Chandrasekhar: Hydrodynamics and Hydromagnetic Stability
        4. Segel: Mathematics Applied to Continuum Mechanics Dover.
    -Nonlinear dynamics, complexity, chaos:
        1. Prigogine: Exploring Complexity
        2. Guckenheimer and Holmes: Nonlinear Oscillations, Dynamical Systems, and Bifurcations of Vector Fields Springer
        3. Lichtenberg, A. J. and M. A. Lieberman (1982): Regular and Stochastic Motion.  New York, Springer-Verlag.
        4. Ioos and Joseph: Elementary Stability and Bifurcation Theory.  New York, Springer.
        5. Heinz Pagels: The Dreams Of Reason
        6. M. Mitchell Waldrop: Complexity
    -Optics:
        1.Max Born and Emil Wolf: Principles of Optics: Electromagnetic Theory of Propagation
        2. Sommerfeld
        3. Allen and Eberly: Optical Resonance and Two-Level Atoms.
        4. Goodman: Introduction to Fourier Optics.
        5. Quantum Optics and Electronics (Les Houches Summer School 1963 or 1964, but someone has claimed that Gordon and Breach, NY, are going to republish it in 1995), edited by DeWitt, Blandin, and Cohen- Tannoudji, is noteworthy primarily for Glauber's lectures, that form the basis of quantum optics as it is known today.
        6. Sargent, Scully, & Lamb: Laser Physics
        7. Yariv: Quantum Electronics
        8. Siegman: Lasers
        9. Shen: The Principles of Nonlinear Optics
        10. Meystre & Sargent: Elements of Quantum Optics
        11. Cohen-Tannoudji, Dupont-Roc, & Grynberg: Photons, Atoms and Atom-Photon Interactions.
        12. Hecht: Optics 
        13. Practical Holography by Graham Saxby, Prentice Hall: New York; 1988.
    -Mathematical physics: 
        1. Yvonne Choquet-Bruhat, Cecile DeWitt-Morette, and Margaret Dillard-Bleick: Analysis, manifolds, and physics (2 volumes)
        2. Jean Dieudonne: A panorama of pure mathematics, as seen by N. Bourbaki, translated by I.G. Macdonald.
        3. Robert Hermann: Lie groups for physicists, Benjamin-Cummings, 1966.
        4. George Mackey: Quantum mechanics from the point of view of the theory of group representations, Mathematical Sciences Research Institute, 1984.
        5. George Mackey: Unitary group representations in physics, probability, and number theory.
        6. Charles Nash and S. Sen: Topology and geometry for physicists.
        7. B. Booss and D.D. Bleecker: Topology and analysis: the Atiyah-Singer index formula and gauge-theoretic physics.
        8. Bamberg and S. Sternberg: A Course of Mathematics for Students of Physics
        9. Bishop & Goldberg: Tensor Analysis on Manifolds.
        10. Flanders: Differential Forms with applications to the Physical Sciences.
        11. Dodson & Poston: Tensor Geometry.
        12. von Westenholz: Differential forms in Mathematical Physics.
        13. Abraham, Marsden & Ratiu: Manifolds, Tensor Analysis and Applications.
        14. M. Nakahara: Topology, Geometry and Physics.
        15. Morandi: The Role of Topology in Classical and Quantum Physics
        16. Singer, Thorpe: Lecture Notes on Elementary Topology and Geometry
        17. L. Kauffman: Knots and Physics, World Scientific, Singapore, 1991.
        18. C. Yang and M. Ge: Braid group, Knot Theory & Statistical Mechanics.
        19. D. Kastler: C-algebras and their applications to Statistical Mechanics and Quantum Field Theory.
        20. Courant and Hilbert: Methods of Mathematical Physics Wiley
        21. Cecille Dewitt is publishing a book on manifolds that should be out soon (maybe already is).  Very high level, but supposedly of great importance for anyone needing to set the Feynman path integral in a firm foundation.
        22. Howard Georgi: Lie Groups for Particle Phyiscs Addison Wesley Frontiers in Physics Series.
        23. Synge and Schild
    -Atomic physics:
        1. Max Born: Atomic Physics
        2. Gerhard Herzberg: Atomic spectra and atomic structure, Translated with the co-operation of the author by J. W. T.Spinks.  New York, Dover publications, 1944
        3. E. U. Condon and G. H. Shortley: The theory of atomic spectra, CUP 1951
        4. G. K. Woodgate: Elementary atomic structure, 2d ed. Oxford: New York: Clarendon Press, Oxford University Press, 1983, c 1980
        5. Alan Corney: Atomic and laser spectroscopy, Oxford, New York: Clarendon Press, 1977
    -Low temperature physics:
        1. The Theory of Quantum Liquids, by D. Pines and P. Nozieres
        2. Superconductivity of Metals and Alloys, P. G. DeGennes A classic introduction.
        3. Theory of Superconductivity, J. R. Schrieffer
        4. Superconductivity, M. Tinkham
        5. Experimental techniques in low-temperature physics, by Guy K. White.
-Galactic tide: The tidal force (a force exerted on a body that has differing strengths at different positions of the body, due to the body's own diameter causing a difference in distance from the force's source) experienced by celestial bodies due to another galaxy, typically the one that contains it. 
-Tetris effect: The syndrome in people in which so much attention and concentration is given in a relatively short amount of time to an activity that the activity begins to show up and pattern the person's thoughts. For example, after people play Tetris nonstop for several hours at a time, they may see images of a Tetris game in progress when they close their eyes, things that they see around them will remind them of or resemble Tetris objects or the game, etc, and it may show up in dreams. In fact, the very thought processes that exist in the game of Tetris (ie wondering if things can fit together and how, etc.) influence the thoughts people have in the rest of their life, eg seeing objects and immediately wondering if they fit together. The effect is short-lived and will dissipate unless the activity is continued. 
    -The effect can be positive; people (with no experience in Tetris) who played twelve 30-minute sessions of Tetris scored significantly better on spatial reasoning and pattern recognition tests than a control group, though it is unknown if the effects are permanent and how long they last.
    -Although the effect is named after Tetris, the effect applies to any activity, including (in my experience), chess, mathematics, programming, etc.
-Pseudo random number generator: An algorithm used to produce a sequence of values that appear random, and may even be uniformly distributed. However, as the name suggests, the values generated are not truly generated, and in fact often follow some formula. 
    -Linear congruential generator: This is one of the oldest and best pseudo-random generators. The generator is of the form of a recurrence relation: X_(n + 1) = (a*X_n + c) mod M, where m is the modulus (m > 0), a is the multiplier (0 < a < m), c is the increment (0 <= c < m), and X_0 is the seed (ie the starting value; 0 <= X_0 < m).
        -Hull-Dobel theorem: An LCG will have a full period (meaning the values generated will repeat at some point) if and only if: (1) c and m are relatively prime, (2) (a - 1) is divisible by all prime factors of m, and (3) if m is divisible by four, so is (a - 1). The maximum value for the period of an LCG is m.
        -Common parameters: 
            1. X_(n + 1) = (1664525*X_n + 1013904223) mod 2^32
            2. X_(n + 1) = (22695477*X_n + 1) mod 2^32
            3. X_(n + 1) = (1103515245*X_n + 12345) mod 2^31
            4. X_(n + 1) = (16807*X_n) mod (2^31 - 1) 
            5. X_(n + 1) = (25214903917*X_n + 11) mod 2^48
            6. X_(n + 1) = ((2^16 + 3)*X_n) mod 2^31, depending on seed value can be good or bad
    -Fibonacci generators: A relation given by: X_n = ((x_(n - i)) op (s_(n - j))) mod M, where 0 < i < j and "op" is an operator that can either be addition, subtraction, or multiplication. To seed this generator, we require an initial n random numbers, {s_0, ..., s_(n - 1)}, typically generated by another generator.
        -Limitation with subtraction generators: For a subtraction generator, if x_n < 0, redefine it as x_n := x_ + M.
        -Maximum period: (2^k - 1)*(2^(M - 1)) for addition or subtraction, (2^k - 1)*(2^(M - 3)) for multiplication.
            -A generator will have its maximum period if the following polynomial is primitive over the integers (mod 2): f(x) = x^i + x^j + 1.
    -Mersenne twister: The most widely used pseudo-random number generator; it is thusly named because its period its a Mersenne prime (specifically, the period is 2^19937 - 1). The PRNG is a very powerful one, and passes many tests for statistical randomness
        -Diehard tests: The algorithm is well reputed for passing the Diehard tests. The Diehard tests are a highly stringent and effective series of tests applied to a random number generator to test its quality and the effectiveness to which it emulates (if it is psuedo-random) randomness; it can, of course, be used to test the randomness of a truly random variable as well. The tests are:
            1. Birthday spacings: Based on the birthday paradox, this test involves the random selection of points throughout a large interval; if the selection was truly random, the points' distances to each other will follow an exponential distribution.
            2. Overlapping permutations: Generate a large amount random digits and break it down into subsequences of n digits. Each possible permutation of those n digits should occur uniformly (with probability 1/n!) if the digits generated are truly random.
            3. Parking lot test: Suppose we construct a circle if radius r. In a (100r) by (100r) grid, randomly place circles such that every time a circle is overlapped, one erases it and places another circle instead. After around 12,000 tries, including failed tries that overlapped, the number of circles successfully placed should follow a normal distribution. 
            4. Minimum distance test: In a large square, place a very large (around 80% of the size of the side length of the square) number of points; the minimum distance between any pair of points should be an exponentially distributed variable if the placing of the points were indeed truly random.
            5. Squeeze test: Start with 2^31. Multiply this number by randomly generated decimal values between 0 and 1, exclusive and round the result up. Continue doing this until the number reaches 1; the number of times needed to multiply 2^31 to reach 1 should be a random variable. Do this 100,000 times to generate 100,000 such instances of the random variable. The number of times the variable is either less than 7 or greater than 47 should be low, and is compared to an exponential distribution.
            6. Overlapping sums test: Add sequences of 100 randomly generated decimal values, each between 0 and 1, exclusive. The sum is a random variable that should be normally distributed.
            7. Walf-Wolfowitz Runs test: The counts of ascending and descending runs in a sequence of randomly generated numbers within some interval should follow a conditional distribution (a distribution of a random variable that assumes some other value for a related random variable) that is approximately normal (with mean (2N + M)/(N + M) + 1 and standard deviation (((mean - 1)(mean - 2))/(N + M - 1))^0.5).
            8. Other tests: Matrix rank test, various variants of a monkey test, count-the-1s test, random spheres test, craps test.
        -Explanation: (***INCOMPLETE***)
-Stellar evolution: The process by which a star is created, and how the star changes and evolves over time due to the innate processes of the universe's physics.
    -Birth: In clouds of gas (ie nebulae) patches of slightly higher density (due to normal statistical variation), pieces of matter become gravitationally attracted to one another and start to clump. As the clumping matter grows in size, its gravitational field increases in strength as well, and more and more matter clumps together. 
        -Nebulae: Gigantic (hundreds to thousands of lightyears in diameter) interstellar clouds of dust and ionized gas (mostly hydrogen and some helium). Despite their size, they have incredibly low density; they are less densely populated with matter than any vacuum created on earth, and a nebula the size of the earth be only a few kilograms of mass. Due to this low density, enormous stretches of matter coalesce into smaller stars (eg to form our sun, it took a cloud of gas 100 times larger than our solar system). A nebula can birth hundreds of stars.
        -Protostar: Although the temperature of the particles begins at that of vacuum space, 2.15 degrees K above absolute zero, within a few hundreds of thousands of years, enough matter has collected to form a flattened, rotating disk (the rotation arises from statistically random "net" directions of motion as the particles clump together, and as the mass grows in size, these tiny variations are amplified into a full on rotation; conservation of angular momentum then flattens the mass into a disk), and the center forms a sphere in which the high densities of matter create high enough pressures for the temperature to reach 2 million degrees Kelvin. This sphere is a protostar. 
            -Brown dwarves: At this stage, some protostars fail to collect enough matter for pressures to rise to the point of sustainable hydrogen-1 fusion. Specifically, a star needs at least approximately 0.08 Solar masses to begin its main sequence. Stars that fail to achieve this mass are failed stars, and are called brown dwarves. Their mass ranges from a few Jupiter masses to 80 Jupiter masses; some dwarves heavier than 13 Jupiter masses can fuse deuterium, and those heavier than about 65 Jupiter masses can fuse lithium as well. 
        -Main sequence: After about ten million years, enough matter has accrued in the sphere for the temperature to reach over 18 million degrees Kelvin. As more and more matter collects in the protostar, the sphere gets denser and denser, and temperatures continue to rise. Eventually, the gravity is so powerful that it can cause the star to implode on itself, collapsing towards its center of mass under its own gravity. However, temperatures are now so high at this point that the hydrogen atoms in the core of the star are moving fast enough for thermonuclear fusion to be initiated. The enormous energies and outward pressures exerted by the nuclear fusion counteract the inward gravitational force in a balanced state of feedback equilibrium. Whenever gravity overcomes the internal nuclear fusion, the star begins to collapse and shrinks, but this shrinking in volume raises density and therefore internal temperature, boosting the fusion processes within the star even stronger and allowing them to counteract gravity.               Conversely, if the fusion processes are two explosive, the star will begin to expand, but the increase in volume lowers density and therefore internal temperatures, slowing down fusion and again reaching equilibrium. 
            -Types of fusion:
                -Proton-proton chain reaction: (pp chain reactions) The dominant form of hydrogen fusion in lighter stars of under 1.3 solar masses. This fusion is simple, with two protons combining to form helium (see "nuclear fusion" above for details). At at least 4 million Kelvin, pp fusion begins.
                    -Process:
                        1. Two protons collide and bond.
                        2. One of the electrons collides into the nucleus, and merges with a proton to form a neutron. This releases a neutrino and a positron as by-products. The resulting atom is deuterium.     
                        3. Deuterium fuses with another proton to form Helium-3. This produces a gamma ray.
                        4. Two Helium-3 atoms collide to form Beryllium-6.
                        5. The Beryllium-6 decays into Helium-4 by releasing two protons.
                -CNO cycle: (Carbon-Nitrogen-Oxygen cycle) This form of fusion begins at about 15 million Kelvin, and at an internal temperature of at least 17 million Kelvin, the CNO cycle becomes the dominant form of energy for a star (as opposed to pp fusion). With CNO fusion, energy output rises dramatically as mass increases. 
                    -Cycle: First, two protons fuse, using carbon, nitrogen, and oxygens at catalysts. This produces a Helium nucleus (ie an alpha particle), two positrons, and two electron-neutrinos. At hot temperatures, carbon, oxygen, and nitrogen nuclei can capture protons faster than they can beta decay, allowing for more nuclear fusion pathways that are inaccessible at lower temperatures.
                -Triple alpha: The process by which helium nuclei fuse into carbon. 
                    1. Two He-4 atoms collide to form Beryllium-8 and releasing a gamma ray as a by-product.
                    2. The Be-8 atom collides with another He-4 atom, releasing another gamma ray as a by-product and forming Carbon-12.
                -Helium capture reaction: The process in which heavier (than carbon) elements form by capturing a helium nucleus. The process may halt at any of the following steps.
                    1. C-12 atom collides with a He-4 atom to form Oxygen-16.
                    2. O-16 collides with another He-4 atom to form Neon-20. 
                    3. Ne-20 collides with another He-4 atom to form Magnesium-24.
            -Duration: This equilibrium is the majority of a star's life. 
                -Red dwarfs: At the lower bound of stars, there exist small, cool stars of between 0.1 and 0.5 solar masses in mass, and with cool surface temperatures of less than 4000 degrees Kelvin. These are the most common types of stars, and because they burn their nuclear fuel so slowly, their main sequence stages (and therefore their entire lifetimes) last hundreds of billions or even trillions of years.
                -Blue stars: At the other extreme, there are very large and very hot stars of up to 20 solar masses with scorching surface temperatures of around 26,000 degrees Kelvin (though of course the core will be much hotter), and 10,000 times more luminous than the sun. Because these stars burn their fuel so quickly, these stars can last only a few millions years in lifetime.
                -Mid sized yellow stars (such as the sun) will remain on the main sequence for about 10 billion years. 
    -Maturation: Eventually, there will not be enough hydrogen nuclei left for the star to fuse and sustainably counteract its inward gravitational pressures. What happens after this point depends on the mass of the star.
        -Low mass: The smallest red dwarves (of around 0.1 solar masses) can remain on main sequence for up to 12 trillion years. Small red dwarves are fully convective, meaning the plasma within the star is constantly moving around. This means that there can be no buildup of helium in the core of the star, and so helium fusion is impossible. As such, these red dwarves will slowly, over another period of several hundreds of billions of years, collapse into a white dwarf, thpugh some of the more massive ones may become red giants first.
            -Radiative and convective zones: In a star, the radiative zone is the zone of the star in which the main heat transfer occurs through electromagnetic radiation. In the convective zone, the dominant form of heat transfer is convection (ie the mass movement of large aggregates of matter with high temperatures that then transfer their high temperature to other systems of matter through physical contact).
            -White dwarf: As a star or star's core continues to collapse inwards towards its center of mass under its own gravity, eventually it will become so dense that the implosion will halt. This is because the matter has been crushed to a density so high that the electrons in the atoms that compose the star are so close to each other that their sheer electromagnetic repulsion provides the outward pressure to counteract gravity; this pressure is known as electron degeneracy pressure. It's mass is around 1 solar mass, packed into a volume roughly of that of the earth; its only luminosity comes from its blackbody radiation and stored thermal energy. White dwarves are typically composed of carbon and oxygen, because they tend to form from red giants with carbon and oxygen cores that aren't massive enough to reach the temperatures required to fuse carbon and oxygen (about 1 billion Kelvin; after the outer layers of the red giant diffuse into a nebula, the core collapses into a white dwarf).
                -Chandrasekhar limit: If a white dwarf has a mass over 1.4 solar masses, it cannot be sustained by its electron degeneracy pressure, and will either explode in a supernova (Type 1a) or collapse into a black hole. Otherwise, it will gradually cool over time into a cold black dwarf. 
            -Helium flash: For stars between 0.5 and 2.5 solar masses, after the red giant phase, the core is composed of helium, and is held against gravity by degenerate pressure. More massive stars reach the temperatures sufficient for helium-fusion faster, before the helium has a chance to be crushed by pressure and become degenerate, and so do not have helium flashes. If, however, the star has a degenerate helium core and begins to collapse after it is depleted of hydrogen, the sudden increase in temperature leads to runaway helium fusion. This is because the normal feedback loop between nuclear fusion outward pressure and inward gravitational pressure is no longer in effect, since the core is supported by quantum mechanical pressure, not thermal pressure. So, the increase in temperature sparks unregulated fusion, which occurs very fast due to the very high density of the core. In a few seconds, the runaway helium fusion releases an amount of energy comparable to that released by the entire Milky Way. This 
                       process can also occur in white dwarves that suddenly accrete material.
        -Middle mass: Stars with masses between about 0.3 solar masses to 8 solar masses will become red giants. 
            -Red giant: Although the entirety of a star's composition is mainly hydrogen, only the hydrogen in the core of the star has enough gravitational pressure on it to undergo fusion; as a result, there is a large shell of hydrogen in the outer regions of the star (which is a sphere) that cannot fuse. However, when the core exhausts its supply of hydrogen to fuse, the star begins to collapse under its gravity. This collapse increases the density of the star to the point where these outer shells of inert hydrogen can begin fusing into helium. As these outer layers fuse, they expand drastically (mainly because there is a lot less gravitational pressure on them than there was on the core of the star during main sequence) into between 100 times and 1000 times the size of the sun (diameters of between 100 million km and 1 billion km, respectively). 
                -Evolutionary phases:
                    -Red-giant-branch phase: In this phase, stars have inert cores made of helium. These stars, like all red giants, have hydrogen-fusing outer shells. Some do not have sufficient mass to have fusion in the core, and they collapse into white dwarves and burn out. Those that do have enough mass (about one solar mass or above) will continue fusing hydrogen in their outer shells, but since this layer does not have the immense pressure of the entire star that the core had on it during the main sequence, the outer shells burn hydrogen faster and faster, driving temperatures up. Though it can take up to a billion years, eventually the temperatures will be sufficient to ignite helium fusion.
                    -Asymptotic-branch phase: After the red-giant-branch phase, the stars will either have had enough mass to fuse the helium at the core, in which case they will now have inert cores made of carbon and oxygen; if they did not have enough mass, they will collapse into white dwarves and quietly burn out. Like red-giant-branch stars, these stars have outer shells that are fusing hydrogen, but they also have enough mass to fuse helium in the core. Since helium is heaver than hydrogen, all the helium produced in the hydrogen-fusing outer layers will sink inwards towards the core. Eventually, there will be a carbon core (since, as with helium, the heavier carbon sinks inwards as the helium fuses into it), surrounded by an outer helium-fusing shell and and even further outer hydrogen-fusing shell.
                    -Post-asymptotic-branch phase: Middle mass stars are not massive enough to ever initiate full-fledged, large-scale, sustainable carbon fusion. As a result, the carbon core will eventually contract into a white dwarf and burn out, whereas the inflated outer shells will diffuse into a planetary nebula that is relatively rich in heavier elements that were produced in the star. 
                -Color: Due to the immense size and therefore surface area of the red giant, all its produced energy is spread over a very large area, causing its flux to be very low and producing a dull orange or reddish color, hence the name. 
        -High mass: After the core burns all of its hydrogen and hydrogen fusion begins in the outer shells, the star is already massive enough for helium fusion to begin immediately in the core. As the star continues to collapse, even electron degeneracy pressure is insufficient to prevent core collapse, so elements continue fusing into heavier ones, each fusion phase temporarily halting collapse. Helium fuses into carbon, which fuses into neon and oxygen, which fuse into silicon. In addition, when the temperature hits 1.1 billion degrees Kelvin, neon breaks down into oxygen and helium; the oxygen fuses into sulfur, silicon, and other heavier elements whereas the helium instantly fuses with the surviving neon into magnesium. Eventually, the temperature is high enough for spontaneous photodisintegration (an atomic nucleus is bombarded with an extremely high energy gamma ray, which it absorbs; it then decays by releasing a proton, neutron, or alpha particle), and so any nucleus can be partially broken down. The 
                released alpha particles instantly fuse with other nuclei to form heavier elements. 
            -Electron capture: Some stars with lower mass (but still too high to become white dwarves) will undergo core collapse before they begin fusing heavier elements. This is due to electron capture, a process in which nuclei with a superabundance of protons under immense external pressure absorb the electron in the lowest energy states, converting one of the protons in the nucleus to a neutron. 
            -Supernova: As elements continue fusing into heavier elements, the heavier elements sink inwards, creating several concentric outer shells of fusing elements (typically, starting outermost and going inwards, hydrogen, helium, carbon, neon, oxygen, silicon). However, eventually the core begins fusing elements into iron and heavier elements. The problem with iron nuclei is, they have large enough atomic radii for the range of the strong force to be weak enough, and have enough protons for the electromagnetic repulsion between two iron atoms to be strong enough, such that the combination of a weakened strong force and powerful electromagnetic force means that fusing two iron nuclei by overcoming the Coulomb barrier would require more energy to be input than the reaction would release. Thus, the fusion of iron cannot sustainably produce outward radiation pressure to counterbalance gravitational collapse. The resulting gravitational collapse leads to an extremely violent and energetic explosion of 
                    cataclysmic proportions; the energy released all at once by a supernova exceeds the entire sum of energy that the sun will release in its entire lifetime of 10-12 billion years. Supernovae briefly outshine the hundreds of billions of stars in their parent galaxies. 
                -Causes: There are two ways a supernova can be triggered. One way is for a star to undergo sudden gravitational collapse. The second is for the abrupt re ignition of nuclear fusion in a degenerate star (any star, including black holes, that relies on some form of degeneracy pressure to sustain itself from gravitational collapse).
                    -Degenerate stars: In compact stars, if a sudden amount of mass is accreted at once, causing it to exceed to Chandrasekhar limit, carbon fusion in the core may be reinitiated all at once, causing sudden runaway nuclear fusion that explodes and completely destroys the star.
                    -Gravitational collapse: If the mass is large enough, the sudden gravitational collapse of all the star's mass towards the iron core cause it to hit the iron core with tremendous force. This causes two things to happen: (1) the enormous implosive force of gravitational collapse pushes the core into a neutron star, and (2) the collapsing material hits the neutron star material and rebounds off it outwards with great force. The rebounding material causes most of the star's mass to be expelled at 30,000 km/s, or 10% the speed of light. As the star's mass rebounds into itself outwards, generating enormous shock waves, the entire star explodes violently. 
                -Types: There are two types of supernovae, type I and type II. Type I is further subdivided into three subclassifications. They are mainly classified by spectral lines.
                    -Type I: 
                        -Type Ia: This is known as a nova. There is a binary system of stars. One of the stars is a white dwarf, and if it gets close enough to the other star, its gravity will begin pulling mass off the companion star and onto itself. If the white dwarf accretes enough mass to exceed the Chandrasekhar limit, it will undergo reignited runaway nuclear fusion of its carbon core, exploding the star in a nova.
                        -Type Ib: In this type of supernova, the star has already lost its outer hydrogen envelope (ie outer shell).
                        -Type Ic: These are similar to Type Ib, but have additionally lost most of the helium envelope as well. In both Type Ib and Ic, there is no silicon spectral line. 
                    -Type II: This type of supernova results from the gravitational collapse of a massive star, and is distinguished by its containing hydrogen spectral lines. The sudden implosion the precedes the supernova happens in a matter of seconds; the outer core can reach velocities of up to 23% the speed of light. The inner core reaches temperatures of around 100 billion degrees Kelvin; 10^46 Joules of energy are released in a ten second long period of collapse. The implosion is stopped by neutron degeneracy pressure, and it rebounds. The resulting shock wave is powerful enough to accelerate the entire outer mass of the star well passed escape velocity.                      
                        -Type II supernovae are further sub-classified into Type II-L, Type II-P, Type II-n, and Type IIb supernovae, which all differ in their light curves (plots of luminosity against time).
                -Stellar remnants:
                    -Neutron stars: If a star's core exceeds the Chandrasekhar limit, it will collapse inwards, overcoming even electron degeneracy pressure, until the pressures are so immense that, through extremely rapid and prevalent electron capture, protons and electrons merge until the entire core is made entirely of neutrons. Due to the Pauli exclusion principle, two neutrons cannot occupy the same quantum state at the same time, and so there is an outward pressure that the neutrons exert on each other simply because they cannot compress any further. This provides the outward pressure necessary to fight further gravitational collapse. The resulting mass is a neutron star.
                        -Density: Neutron stars are among the most dense objects in the universe; they have radii as small as 10 km across, the size of a large city, but have a mass of several solar masses. A tablespoon of neutron star mass would weigh as much as the earth, equivalent to a Boeing 747 being crushed to the size of a grain of sand. They have densities in the neighborhood of 3.7 * 10^17 to 5.9 * 10^17 kg/m^3, which is comparable to the density of an atomic nucleus. 
                        -Magnetic field: Although neutron stars are predominantly (around 90%) neutrons, they will retain a small amount of trace protons or electrons, since it is not necessary for all the mass to turn into neutrons for the outward degeneracy pressure to fight gravitational collapse. The neutron stars are spinning so fast that even these trace amounts of charged particles produce extraordinarily strong magnetic fields (around 10^13 times the field strength of the earth's field).
                        -Rotation: Neutron stars can have extremely rapid rates of rotation (up to 43,000 rpm). 
                        -Pulsars: (portmanteau of "pulsating star") A rotating neutron star with a very strong magnetic field that gives off very strong bursts of electromagnetic radiation at its poles, similar to how a lighthouse rotates and gives off a beam of light in one direction.
                                -Magnetar: These are neutron stars that have magnetic fields of up to 10^16 times the strength of the earth's magnetic field.
                            -Maximum magnetic field: The maximum magnetic field any object can have is about 10^19 times the magnetic field of the earth; fields stronger than this have enough energy in their force fields to turn their sources (in this case, the neutron stars) into black holes.
                    -Black holes: If the mass of the stellar remnant core exceeds not only the Chandrasekhar limit, but also the Tolman-Oppenheimer-Volkoff limit (a neutron star with a mass between 1.5 to 3 solar masses), then the resulting neutron star will continue collapsing into a black hole. Additionally, if a stable neutron star gains enough mass to exceed to TOV limit, it can also destabilize and collapse into a black hole. 
                        -Massive enough stars, far more massive than the largest that exist today, may have existed at some point in the universe's history, and would collapse immediately into black holes (skipping the black hole phase entirely). They may also be destroyed entirely in extremely powerful supernovae, not even leaving behind a stellar remnant. 
                        -For more on black holes, see "black holes" above.
                -Hypernova: Supernovae that are much more energetic and luminous than standard supernovae.
                    -Collapsar model: If a star has a core with a mass between 5 and 15 solar masses, it will go supernova, but much of the ejected mass will fall back towards the core. Normally, this would form a black hole, but if the core is rotating quickly enough, the fall back into the black hole will produce relativistic jets (extremely powerful jets of plasma ejected at high speeds with lengths of up to several thousand or even several hundred thousand light years). 
                    -Pair-instability supernova: Pair production creates virtual particles in between fusing nuclei in the core, reducing the outward thermal pressure. This abruptly induces partial gravitational collapse which then includes dramatically accelerated runaway nuclear fusion that blows the star apart so violently that no stellar remnant is left behind.
-Lagrange multipliers: A method used to optimize a function (finding extrema) when the function is subject to additional constraints under which it can be optimized. This has applications in most real world optimization problems, in which the procedure to be optimized has additional constraints on it (usually conservation of resources or something similar). Specifically, we want a global extremum of a function with the constraint that our domain is restricted to a subset of the entire domain, and the nature of the subset depends on some relationship. Formally, for a function f: R^n -> R, such that the point (x1, ..., xn) is mapped to a single number f(x1, ..., xn), and we want to find an n-dimensional point that is a global extremum of f AND satisfies a constraint given by g(x1, ..., xn) = 0 for some function g.
    -Examples: We will begin with some basic mathematical examples; not that the theory applied here can be applied to real world functions and relationships as well. 
        -Example 1: In a simple one-dimensional case, suppose we want to maximize the function f(x) = 2 - x^2 subject to the constraint that x - 1 = 0. In this trivial example, we find that the domain of f is restricted to the set of points that satisfy x - 1 = 0, which is only one point: x = 1. So, we can substitute this point in to obtain the maximum value over the constraint: maximum = f(1) = 1.
        -Example 2: In a two-dimensional example, maximize f(x, y) = 2 - x^2 - 2y^2 subject to the constraint that the point (x, y) lies on the unit circle, ie we restrict the domain of f from the entire xy-plane to the unit circle in the xy-plane. This can be expressed as the constraint x^2 + y^2 - 1 = 0. We can find the maximum by solving for x^2 = 1 - y^2 abnd substituting into f: f(x, y) = 2 - x^2 - 2y^2 = 2 - (1 - y^2) - 2y^2 = 1 - y^2. Now we can maximize the function f(y) = 1 - y^2 over all possible values of y (which are, in accordance with the unit circle, any value in the range [-1, 1]); the maximum value is y = 0, which means x = 1 or -1. Thus, the maximum occurs at (-1, 0) and (1, 0) in the domain, and the minimum occurs at (0, 1) and (0, -1).
    -General solution: In a general n-dimensional case, it can be very difficult to solve problems using the above methods. Graphical intuition can help provide a general solution. First, consider contour lines of f; lines where, by definition, f does not change in value. We can keep hopping from contour line to contour lines, akin to checking various values of f, searching for a maximum. However, because the value is also constrained by the condition that it lies on the g = 0, we can contour hop but only check points that are also on the graph of g = 0, ie points where contour lines and the constraint graph intersect. Of these intersection points, the extrema will be located where the gradient vector of f is perpendicular to the constraint graph. This is because the gradient vector points in the direction of steepest ascent, and if we're at an extremum, there should not be any direction along the constraint graph that we're able to walk and still increase or decrease the value of f; if we could move along the 
               constraint graph at a point and still increase the value of f, then that point clearly wasn't a maximum. So, the gradient vector must have absolutely no component parallel to g = 0, ie it must be perpendicular to g = 0. 
        -Formalization: Now that we have our condition that the gradient vector of f must be perpendicular to the constraint graph at any extremum points subject to the constraint, we can mathematically formalize the idea. We must now view the constraint graph g = 0 as not only a graph in the domain of f that constrains f, but also as a contour line of some n-dimensional function g. The gradient vector of f is perpendicular to the contour g = 0 if and only if the contour lines of f and g are parallel/tangent at that point (again, if they weren't tangent, then I could guarantee another intersection between the contour line of f and the constraint graph where, if I walked, there would be a higher value for f than at the previous point, indicating that the point obviously wasn't a maximum), and this is only true if f and g have parallel gradient vectors at the point. If the gradient vectors are parallel, then there must exist a scalar lambda such that the gradient of vector is equal to the product of lambda and the 
                gradient of g. So, the system of equations we must satisfy is given by: (1) grad(f) = lambda * grad(g) and (2) g = 0, where f is the function to be minimized and g is the constraint relationship (that specifies how the domain of f is restricted for our extremum points). 
-Quadratic sieve algorithm: A general purpose (its runtime depends only on the size of the integer to be factored and not on any special properties of the integer) integer factorization algorithm. After the general number field sieve algorithm, it is the fastest factorization algorithm known to date, and remains the fastest overall for integers under 100 digits. It was designed as an improvement to Schroeppel's linear sieve.
    -Congruence of squares: The procedure of finding two perfect squares that are congruent modulo n; this is useful because it implies that (x^2 - y^2) has n as a factor, which means that if we can compute the greatest common factor of (x + y, n) and of (x - y, n) (since x^2 - y^2 = (x + y)(x - y)), then we will have two factors for n; finding the greatest common factor of two integers can be done efficiently using Euclid's algorithm. 
        -Dixon's algorithm: (***INCOMPLETE***) An early factorization algorithm based on congruence of squares is Dixon's algorithm. Dixon's algorithm is an improvement on Fermat's algorithm.
            -Fermat's factorization algorithm: This algorithm is most directly based on the congruence of squares. To factor an integer n, we select random x values over sqrt(n) and hope that x mod n is a perfect square, ie if y = x mod n, then sqrt(y) is an integer. If this is true, then then we can compute the greatest common factors of (x - sqrt(y)) and n to find a factor of n. 
                -Efficiency: It is impractical to search randomly for integers and hope that they're square modulo n is a perfect square. This algorithm is therefore rather inefficient. 
            -Smoothness of integers: A smooth integer is a number with only small prime factors; specifically, a number is called "B-smooth" if all of its prime factors are less than or equal to B.
-General number sieve algorithm: (***INCOMPLETE***)
-Theory of constraints: (***INCOMPLETE***)
-Intelligence quotient: (A.K.A. IQ) An attempted quantitative measure of human intelligence. Based on a mental test administered to human subjects and factors such as number of questions correct, time taken, and other more diverse factors (see creativity test above), a single number is assigned designed to measure the intellectual and cognitive capacity and how it influences perception, abstraction, creativity, learning, and overall ability. There is no generally accepted or completely accurate theory or even definition of biological intelligence to date. There is a significant positive correlation between IQ and factors such as financial success, lifespan, social status, and achievement (about a 0.7 correlation with achievement testing and measures of achievement), but the correlation is not strong enough to rule out confounding variables or make any conclusive statement. 
    -Stanford-Binet IQ Test: Most widely used IQ test. French psychologist Alfred Binet + Lewis Terman (Stanford University (1916)). Current version is the fifth edition, by Roid. Known as SB5.
        -Binet Scale: Below 20=Idiot, 20-49 = imbecile, 50-69 = moron, 70-79 = borderline deficient, 80-89 = dull, 90-109 = normal, 110-119 = superior, 120-139=Very superior, over 140 = genius or near genius
        -IQ = (Mental Age/Actual Age) x 100 This is a traditional definition that is no longer in use; IQ is much better measured as a standardization.
            -Fluid intelligence: The ability to think logically and to solve problems. It is independent of acquired knowledge and is more of a measure of innate ability. Includes peoples' abilities of inductive and deductive reasoning. Declines with age; rises and peaks until 26, then steadily decreases.
            -Crystallized intelligence: The ability to heuristically problem solve (i.e. use stored information to solve new problems). Represents an overall achievement rather than ability and is largely dependent on environment, memory, and motivation. Can be embodied in vocabulary and general knowledge. Improves with age.
    -IQ: Standard set to 100, standard deviation is 15-16. About 95% of the population has an IQ between 70 and 130. 
        -Heritability of IQ is still a matter of debate (nature vs nurture)
        -Process of assessing intelligence began with Francis Galton and James McKeen Cattell
        -Historiometry: analytical study of human progress. Pioneer: Francis Galton
        -Lewis Terman and Leta Hollingworth popularized the term of genius as one with high IQ. Terman: over 140. Hollingworth: over 180.
            -K. Anders Ericson: says that after a person's IQ is over 120, other qualities determine success. Exceptions: highly demanding intellectual fields, i.e. theoretical physics
    -Theories of intelligence: 
        -General factor (A.K.A. g) A theory proposed by Charles Spearman that formally correlated different IQ tests, designed to measure different hypothesized aspects of intelligence (such as vocabulary, spatial reasoning, etc.) together. He theorized that there existed an underlying mental ability in the brain that effected a very versatile and flexible form of cognitive perception, problem solving, and learning, and thus influenced performance across many unrelated fields. The theory was designed to explain the experimental observation that schoolchildren that performed well in one part of their academics or on a particular intelligence test tended to perform well on all of them, even seemingly unrelated ones. There are, of course, confounding variables such as motivation, ambition, etc. Spearman conjectured a single "general factor" existed that could explain general human ability. The validity of this theory is still controversial.
            -Testing: The nature of the theory implies that to accurately test for g, a test must correlate highly with every other kind of IQ test; as such, most of these tests are highly abstract and involve abstract reasoning. 
        -Cattell-Horn-Carroll: These three psychologists first defined fluid intelligence (the ability to solve diverse and original problems using reasoning) and crystallized intelligence (general knowledge and heuristic pattern matching or problem solving).
            -Three strata: It was thought to divide intelligence into three levels, or strata. At the top is the the general factor (mentioned above); below are ten broad types of intelligence, each describing a particular (albeit broad) ability, and each of these ten are subdivided further into specific forms of intelligence (eg vocabulary, design fluidity, etc.) totaling 70 different forms.
                -Second (broad) stratum: Fluid intelligence (Gf), crystallized intelligence (Gc), quantitative reasoning (Gq; understanding numerical values and quantities, and manipuLating symbols thereof in accordance with given guidelines), writing (Grw; a general ability for creative, expository, and other forms of writing fluidly and effectively, as well as reading skills), short-term memory (Gsm; obtaining and keeping information for a short period of time (a few seconds) and manipuLating or applying it), long-term memory (Glr; store and recall large amounts of information after indefinite periods of time), visual processing (Gv; visual thinking, perception, and analysis; related to long-term memory especially of visual objects), auditory processing (Ga; manipuLating, understanding, applying, and storing auditory stimuli), processing speed (Gs; performing cognitive tasks under time pressure), and reaction time (Gt; how quickly a cognitive reaction is elicited from a stimulus; this differs from Gs as this refers to 
                             immediate awareness and not time pressure).
-Parallel computing: Computational activity in which several calculations are carried out simultaneously instead of line-by-line; this follows the idea that many algorithms can be broken down into smaller sub-pieces which are carried out in parallel. Parallel computing is more power efficient and is key to higher-performance computers. Serial computation has limits in upward efficiency and speed, since increasing frequency reduces runtime but also increases power consumption. However, parallel computer programs have their own drawbacks, mainly in difficulty of creation. The biggest problems are communication between different processors, and synchronizing their computation.
-Spectral type of a star: Stars can be classified based on what wavelengths of the electromagnetic spectrum light is emitted at. The light emitted is dispersed into its various wavelengths (with absorption lines here and there); lines correspond to chemical elements that emitted the light through the fluorescence of excited electrons returning to their stable ground state, and the thickness or strength of the line corresponds to the concentration of abundance of the element. 
    -Morgan-Keenman system: A sequence from hottest to coldest that uses the letters O, B, A, F, G, K, M, L, T, and Y. Within each letter, a number from 0 to 9 subdivides the classification by temperature in descending order. A luminosity classification is combined with the spectral classification by appending the corresponding Roman numeral.
        -Luminosity classification: Class 0 = hypergiants, Class I = supergiants, Class II = bright giants, Class III = giants, Class IV = sub-giants, Class V = main sequence, Class VI = white dwarves, Class VII = brown dwarves. For the sun, the full classification (including spectral) is G2V.
        -Spectral classifications:
            -Class O: These stars are very hot and luminous, but are also very rare. They have effective temperatures of greater than 30,000 K and are blue in color. They are also very massive. 
            -Class B: Effective temperatures between 10,000 K and 30,000 K. They are a blue white color.
            -Class A: Effective temperatures between 7,500 K and 10,000 K. White color.
            -Class F: Effective temperatures between 6,000 K and 7,500 K. Yellow white color.
            -Class G: Effective temperatures between 5,200 K and 6,000 K. Yellow color.
            -Class K: Effective temperature between 3,700 K and 5,200 K. Pale orange color.
            -Class M: Effective temperatures between 2,400 K and 3,700 K. Light orange red color.   
            -Class L: Effective temperatures between 1,300 K and 2,400 K. Light red color.
            -Class T: Effective temperatures between 500 and 1,300 K. Brownish color.
            -Class Y: Effective temperature under 500 K. Black color.   
-Knowledge Graph: A very large organization of structured complex information and the relationships between different pieces of information used by the company Google to better answer search queries in ways that are more comprehensive and easy to understand to the user. The purpose of a search engine is to enable a user to gain access to sources of information that may answer a query he has, with the intent that upon presenting the available resources to the user, the user would traverse those resources to answer his query. This project attempts to streamline and evolve this process by attempting to directly answer the user's query instantly upon the query being searched, saving the user the time and energy of compiling the information himself from many different websites (Google instead does this). It is important to note that Google does not compile the information necessary to concisely answer the user's query through any kind of information generating algorithms; it simply analyzes available information 
          across many websites and compiles the most relevant using separate algorithms. 
    -Knowledge base: The Knowledge Graph is a knowledge base. A knowledge base, as mentioned above, is a very large database of organized information, both structured and raw, as well as how different categorizations of information pertain to and affect each other. It can be visualized a large tree with parent nodes containing many separate sub-categorizations of information, and each node is its own level of information complexity and behaves as a large complicated spider-web with different forms of information, their relationships to each other, links to additional websites for information, summarizations of the information, etc. 
    -Semantic search: A methodology employed by search engines that attempts to analyze search queries using linguistic, grammatical, and semantic cues to better understand what the intent and meaning behind a query is, as opposed to the most crude form of searching by simply searching for the presence of a string of text in a body of text. 
-Symbolic link: These computing constructs are used to store a file in multiple locations by storing the file in a set location and then creating another filler file in a new location; the filler file will point to the original file, so for all intents and purposes, it can be treated as if the file were in both locations at once. The source file in the original location is called the target; deleting the target destroys the symbolic link (since there's nothing to point to), but destroying the symbolic link does nothing to the target.
    -Creating in Windows: The unix command "mklink" in command prompt, followed by the target file's path location and then followed by the symlink's path location.
-Secretary problem: A famous problem in probability theory that posits the following scenario: There are n rankable objects in random order. You see one object at a time, and in that sighting learn of its rank relative to all the objects that have been viewed so far, but NOT relative to the entire set of objects total. The question asks for an algorithm to maximize the probability of selecting the best possible object, with the catch that when you view a particular object, you must either select it on the spot or reject it and move on, unable to recall the object. If we were permitted to make our decision at the end of viewing each object, the optimal solution would be the very simple algorithm of keeping a running maximum quality object and replacing it each time you came upon a better one, but the catch that the decision is made on the spot make the problem significantly harder. It is called the secretary problem because of the more intuitive description of the problem as interviewing n secretaries and 
            trying to hire the best one (obviously, you can rank a particular secretary against those already interviewed, but the quality of the remaining secretaries not yet interviewed is unknown), with the catch that a secretary that has been rejected from his interview cannot be recalled for the position.
    -The optimal strategy is a stopping rule (automatically reject the first r objects, and then choose the first object that is better than the best object of the initially rejected r objects). The probability of the best object is selected is equal to the intersection of the probability that a particular object i is selected and the probability that object i is the best one.
        1. (***INCOMPLETE***)
            -http://rs.io/the-secretary-problem-explained-dating/
-Principle of stationary action: (***INCOMPLETE***)
-Derivation of kinetic energy formula: We will derive kinetic energy from work. This is intuitively pleasing because kinetic energy is energy that is stored due to motion, and work is energetic output due to movement along some distance due to some force, which is of course a change in potential energy. 
    1. Work is defined as, using infinitesimals to denote an infinitesimal change in distance, dE = F * dx, where F is a (possibly variable) force and dx is an infinitesimal change in distance.
    2. Therefore, dE = (m*a) * dx, from Newton's second law, where m is the collective mass of the bodies and as the acceleration of the center of mass.        
    3. Since dx = v*dt, by definition, where v is the velocity of the center of mass and dt is an infinitesimal change in time, we have dE = (m*a)*(v*dt)
    4. Since a*dt = dv by definition, we have dE = m*v*dv. Integrating, we arrive at E = integral of m*v*dv = (1/2)*m*v^2.
-Magnetic resonance imaging: A methodology that aids physicians in the diagnosis and treatment of medical conditions; it allows for a direct view into the patient's body, similar to x-ray. It allows for the evaluation of the organs in the torso (heart, lungs, liver, stomach, pancreas, etc.), pelvic organs (bladder, uterus/prostate, etc.), blood vessels, lymph nodes. The procedure differs from computed tomography (CT) scans in that it does not use potentially harmful ionizing radiation. 
    -Process: MRI is based on the fact that the human body is composed mainly of water, which is two parts hydrogen. A powerful magnetic field aligns the magnetic domains of the protons in the water (so that their spins are aligned). A radio pulse briefly alters this alignment, and after the pulse's duration the spins re-align. The transition from alignment to altered alignment to re-alignment releases energy which is detected by a computer and Fourier transformed to produce an image. The released energy comes from the changing magnetic flux. The main magnetic produces a very powerful magnetic field between 0.5 and 2 Tesla, and additional gradient magnets, much weaker than the main at around 0.02 Tesla more specific images can be taken.  
-Monte Carlo methods: A Monte Carlo simulation is a method used to numerically approximate a solution to a problem; the problem in question is usually extremely difficult or impossible to analytically solve and is computationally infeasible for computer analysis. A Monte Carlo methodology will approximate a solution by using some kind of relationship between input variables and an output solution; our computational and analytical shortcomings are represented by treating our input variables not as unknown values to be solved for but rather as probability distributions (ie random variables). Each input variable will follow a probability distribution (which we must specify), and we will use computation to accurately "input" each probability distribution into the relationship to obtain an approximate distribution for our output solution. Many random numbers (between 0 and 1) are generated for each input variable. Each random number for a particular input variable's distribution represents a probability on that 
              distribution, and is the likelihood of the corresponding outcome occurring. For example, if the number 0.8819 is generated for input variable X = amount of sand used in a construction project, then this corresponds to the hypothetical situation in which the amount of sand corresponding to an 88.19% chance of being used happens to be used. Each random number generated for all possible input variables forms a set of random numbers, which corresponds to a set of possibilities for input variables, which is then plugged into the equation reLating inputs to outputs to obtain one possible output scenario with a given probability. Hundreds, thousands, and sometimes millions of such sets (unique, ie each different from the rest) are generated to obtain many output possibilities, each with a corresponding input probability that depends on the probabilities inputted (ie the random numbers generated); using this information, an approximate (technically discrete and not continuous) probability distribution for the 
              output variable can be obtained. As the number of sets of random numbers generated increases, the approximation gets better and better. The resulting distribution allows a user to deduce the probability of a certain outcome occurring in his solution set. 
    -Input variables' probability distributions: Common distributions used are:
        -Normal: The Gaussian distribution (very common).   
        -Log-normal: Used for variables whose logarithm is Normally distributed. Variables whose growth is independent of size, is positively bounded, etc. tend to have log-Normal distributions.
        -Triangular: A distribution specified only by three parameters: the minimum possible value, the most likely value, and the maximum possible value. The resulting graph is a triangle (since the minimum and maximum values must have probability 0 and so are always points on the horizontal axis). This is a very common distribution used in simplistic situations.
        -Uniform: A completely random distribution in which all possibilities are equally likely.
        -Discrete: Used when a finite and closed number of options exist (eg the outcome of a trial in law, which can only take on a few discrete values (ie guilty, not guilty, mistrial, settlement, etc.)).
        -Often personalized or custom distributions are employed as well.
    -Generalized process: The process for a Monte Carlo simulation depends heavily on the specifics of the problem, but a very general overview of the procedure is as follows:
        1. Identify what the solution you are searching for is, and then identify what parameters this solution depends on.
        2. Create a (explicit or implicit) relationship between the outcome/solution and the input parameters on which it depends.
        3. For each of the input variables outlined above, determine which probability distribution best applies to it.
        4. For each input variable, generate a random number between 0 and 1 to represent a certain probability of a situation occurring, and note the corresponding value of that input variable. This will produce a set of possible input values, one for each input variable. Generate N such sets. Make sure that each set is unique.
        5. For each of the above N sets of possible input values, use the equation between inputs and output to obtain N possible output values. 
        6. Plot the N output values on a histogram, with the horizontal axis representing the output value and the vertical axis representing the frequency (ie number of times it occurred) with which that output value occurred in the simulation. This will be an approximate probability distribution for the solution.
    -Etymology: The name of the technique comes from the procedure's similarity to how some people record results in casinos in the city of Monte Carlo to understand the probability of certain outcomes in poker.
-Public key encryption: This methodology of encryption is one solution to a major problem in modern cryptography. Throughout the history of cryptography, symmetric key encryptions (encryptions in which the same key is used to encrypt and decrypt a message) have been used. However, these have the disadvantage that the key must be agreed upon in advance by both parties doing the communicating. In modern cryptography, the two parties may never even have met before and thus have no way of securely exchanging s key in advance. This is very common when two computers need to securely communicate but have never made contact prior. This begets the advent of asymmetric key encryption, in which two different keys are used to encrypt or decrypt: a public key or a private key. 
    -Three conditions: The three main properties we need from our encryption function are: (1) the function is easy to compute, (2) the function's inverse is difficult to compute in general, and (3) the function's inverse is easy to compute given some special information.
    -Hybrid encryption: Because symmetric encryption tends to be far more efficient (in terms of both time and space complexity), it is common to first use public key encryption to communicate a symmetric key between two parties, and then simply use symmetric key encryption there on out.
    -Explanation: Suppose two parties, A and B, wish to communicate a message. However, A and B have not agreed upon a pre-defined symmetric key, so they cannot use traditional encryption methods (eg ciphers). Instead, A and B publicly agree on a public key. Note that since this public key was publicly chosen, anyone in the public has access to this key. However, A and B also each choose an individual private key, known only to themselves. Let's denote the public key A, the private key of A with K_A, and the private key of B with K_B. Both A and B mathematically combine their private keys with the public key using some function that we denote f. Thus, A constructs a new key using f(K, K_A) and f(K, K_B). A and B then exchange these new keys publicly, so now both A, B, and the public have access to not only the public key K but also both f(K, K_A) and f(K, K_B). A and B then decrypt the combination using a decrypting function g. Notice that both A and B have access to three key ingredients: K, K_A, and K_B; it is just that A does not have access to K_B explicitly but rather through the function f(K, K_B), and B does not have access to K_A explicitly, but rather through the function f(K, K_A). Using either K, K_A, and f(K, K_B) or, equivalently, K, K_B, and f(K, K_A), the three ingredients can combined to produce the SAME final message. The function f is designed such that the two collections of ingredients (K, K_A, f(K, K_B)) and (K, K_B, f(K, K_A)) are "decrypted" (perhaps with a third simplifying function g) to the exact same value. However, the public only has access to K, f(K, K_A), and f(K, K_B), but without a single explicit key, these ingredients cannot be combined into the final message.
        -Color analogy: A useful analogy to explain the concept uses color. Say two parties A and B wish to communicate some secret color, brown. To do this, they first publicly agree on a "public color": yellow. The public color is known not only to A and B, but also to the general public. A then chooses a "private color", say red, and B chooses its own "private color", say blue. A mixes the public color with its private color; mixing yellow and red produces orange. B likewise mixes the public color with its private color; yellow and blue makes green. A then sends its mixed color (orange) to B, and B sends its mixed color (green) to A. A now has three colors: yellow (the public color), red (its private color), and green (B's mixed color); B has access to three colors as well: yellow (the public color), blue (its private color), and orange (A's mixed color). If A mixes its private color with B's mixed color (ie mixing red with green), it will produce the same color as that produced when B mixes its private color with A's mixed color (ie mixing blue with orange). Thus, A mixes red with green to make brown and B mixes blue and orange to make brown as well; this is the secret color. However, since the public only has access to yellow, orange, and green, it is unable to obtain the secret color of brown.
        -A key point in public key encryption is that given the private key and some function, it is easy to verify that the private key is a solution. However, it is difficult to extract the private key given the combination of the private key and the function; this is analogous to how it is easy to mix two colors to produce a third, but it is difficult to extract the two colors that were mixed given a third. 
        -Discrete logarithm problem: A discrete logarithm is the same as the logarithmic function, but the domain and ranges are restricted to discrete sets (eg integers) instead of the usual continuous ones (usually the real numbers). Formally, a discrete logarithm is an integer n that provides a solution to the equation b^n = c, for some elements of some finite group b and c. The fact that b and c are elements of a finite group implies that modular arithmetic is involved, which can make the extraction of a solution, n, difficult. Notice that an ordinary logarithm is defined the same way, as a number n that solves b^n = c, with the caveat that b, c, and n can be any real numbers instead of being restricted to a particular group. There is no efficient method to compute a discrete logarithm, especially since the condition that b and c belong to a finite field imply that the exponentiation b^n is done modulo a prime number. 
            -There are an infinite number of possible discrete logarithms (since a^x = a^(x + kn) mod n), so we define, for convenience, the discrete logarithm to be the smallest such value.
            -Diffie-Hellman key exchange: The first form of public key encryption; the problem used (to satisfy the above three conditions) in this algorithm is the discrete logarithm problem. The algorithm allows two parties to publicly exchange two pieces of information, process the received information with a private piece of information, and arrive at the same result. Since both parties end up with the same number after processing each others' messages (which are computationally infeasible to decrypt without the private information), that number, although impossible to determine beforehand without knowing each others' private information, can be used as a key for future communication. If two parties A and B wish to communicate sensitive information over an insecure channel, the algorithm is:
                1. A and B both agree upon some finite field F of order q, and also agree upon some generator of F, say a. This is the public key.
                    -Recall that a finite field is a field with a finite number of elements, and its size q must be a prime power (ie q = p^n for prime p and natural number q); for more information see above. A generator of a field is an element in the field that is some natural number root of every element in the field under the field's multiplication. Formally, a is a generator of F if it is true that F = {a^x | x is an element of {0, 1, 2, 3, ...}}; intuitively, a generator is an element that "reaches" every possible element in the field by simply applying the multiplicative operator to it repeatedly. Thus, the entire field F can be generated from the single element a.
                2. A and B each select their private key, a random number in F. Specifically, A randomly chooses a number x and B randomly chooses a number y such that both x and y are in the set {2, 3, ..., q - 2}. Note that 1 and (q - 1) are omitted because when elements of F are exponentiated to 1 or (q - 1), they are unchanged.
                3. A and B exchange a^x and a^y over the insecure channel.
                4. A now possesses the x and a^y, while B now possesses y and a^x. They can both compute (a^x)^y = (a^y)^x = a^(xy) to attain the secret message.
                    -Note that since the only information exchanged over the insecure channel was a, a^x, and a^y. In order to combine this information to produce a^(xy) without knowing either x or y, a third eavesdropping party would need to compute log_a (a^x) or log_a (a^y). This is analogous to solving the discrete logarithm problem, and since this problem is currently computationally infeasible to solve, the above algorithm is secure.
    -RSA encryption: (***INCOMPLETE***) 
        -Proof that encryption and decryption functions are inverses: Let x (an integer between 1 and (N - 1)) be the message to be sent, and let E(x) = x^e mod N and D(x) = x^d mod N be the encryption and decryption functions, respectively. Additionally, N = pq for primes p and q, gcd(e, (p - 1)(q - 1)) = 1 (ie e is an integer that's relatively prime to (p - 1)(q - 1)), and d is the multiplicative inverse of e mod (p - 1)(q - 1) (ie ed = 1 (mod (p - 1)(q - 1)).
            1. We want to show that D(E(x)) = x mod N. D(E(x)) = (x^e mod N)^d mod N = x^(ed) mod N. If indeed D(E(x)) = x mod N, then x^(ed) = x (mod N). This implies that (x^(ed) - x) = 0 (mod N). 
            2. Consider the quantity (x^(ed) - x): ed = 1 (mod (p - 1)(q - 1)) implies that ed = 1 + k*(p - 1)(q - 1) for some integer k. Our expression becomes: (x^(1 + k*(p - 1)(q - 1)) - x) = (x)(x^(k*(p - 1)(q - 1))).
            3. We will now show that p and q both divide the quantity from step 2: First, consider the proof that p divides (x)(x^(k*(p - 1)(q - 1))). If p divides x, then p divides the entire expression. Otherwise, p does not divide x, and therefore x != 0 (mod p). By Fermat's Little Theorem, x^(p - 1) = 1 (mod p). Therefore,(x^(p - 1))^(k*(q - 1)) = 1^(k*(q - 1)) (mod p), which implies that x^(k*(p - 1)(q - 1)) = 1 (mod p), and so (x^(k*(p - 1)(q - 1)) - 1) = 0 (mod p), which of course implies that p divides the expression on the left. A similar proof shows that q also divides the expression.
            4. Since both p and q divide (x)(x^(k*(p - 1)(q - 1)) - 1), (x)(x^(k*(p - 1)(q - 1)) - 1) = 0 (mod (pq)), which simplifies to: (x^(1 + k*(p - 1)(q - 1)) - x) = 0 (mod N), since N = pq. Recall from step 1 that ed = x^(1 + k*(p - 1)(q - 1), and therefore: (x^(ed) - x) = 0 (mod N).
            5. This implies that x^(ed) = x (mod N), which of course proves the statement that D(E(x)) = x (mod N).
            -Fermat's Little Theorem: If integer a is not divisible by prime p, then a^(p - 1) = 1 mod p.
    -Three pass protocol: This system of secure information transfer is best explained with the following analogy: If person A wants to send a box of information to person B, but cannot communicate securely beforehand, then A can lock the box with her lock (to which she has the key) and send the locked box securely to B. B, who cannot open the box (since only A has the key to his lock) locks the box again with his lock (to which he has the key) and sends it back to A. A then unlocks her lock and sends the box, now locked only with B's lock, back to B, who unlocks it with his key; the entire transfer has been secure. A common implementation is to agree publicly on some prime number p. The message that A wants to send to B is converted to the number m, and A selects a private key, a, and B selects a private key, b. A then sends the number m^a mod p to B, and B then raises that number to the power b, sending back to A the number m^(ab) mod p. A then takes the a-th root of that number (modulo p) and sends the result `back to B, who takes the b-th root of that number to obtain the original message, m.
-Gaussian quadrature: (***INCOMPLETE***)
-Deliberate practice: A psychological theory about the success of exceptional people, ie people whose performance in their field far exceeds that of the general population, that has recently begun to gain traction in the scientific community and has also begun to push out the old theory that extraordinary talent is almost completely determined by genetic and other immutable factors. Instead, exceptional talent and growth is attained through consistent and mindful practice of the subject; not only that, but a specific kind of high quality practice. The quality of practice and its effectiveness matters even more than the quantity. Although predisposed factors (eg height, baseline intelligence) do have a significant effect on both maximum potential as well as growth rates, for the majority of people, gaining experience holds more weight, so long as the methodology of gaining experience follows certain guidelines of quality.
    -Components of deliberate practice: For practice to be high-quality and produce rapid results, it must be consistently done and dedicated to as well as mindfully designed to maximize performance. 
        1. The subject must have a vast reservoir of durable motivation to exert effort, not necessarily enjoyably so, towards improving performance. He must be ambitious enough to see the entire practice regimen through in the long-term even with obstacles.
        2. The method of practice should account for pre-existing skills and knowledge relevant to the task. 
        3. Immediate feedback with each session of practice to highlight areas of strength and areas of weakness, as well as recommendations for solidifying strong areas and repairing weak ones. This feedback must accompany each session of practice soon after it is performed, and the subject must consciously and actively implement the feedback into the next session of practice.
        4. Repeatedly perform similar tasks over and over rather than practicing with an unstructured and unorganized methodology. Experts tend to effectively break down the task into sub-areas of improvement, and work on each area individually instead of all at once. For example, a chess grandmaster might, instead of practicing simply by playing many games of chess (though this is important as well), by breaking down the task of learning chess into sub-groups, such as defensive ability, tactical prowess, positional prowess, calcuLating ability, etc. and work on each subset individually. One should also actively practice areas that one suffers in and is sub-par at, rather than over-emphasizing areas that one is already good at.
            -This can be paradoxical, because by consistently working on areas on which one is performing poorly, and by strenuously analyzing each failure and its causes and remedies, it will often feel as if one is constantly failing, leading to frustration and struggle. However, this sense of consistent failing is an illusion and in reality is the fastest way to build talent. 
    -Examples:
        -Aubrey Daniels, on the subject of basketball, said "Consider the activity of two basketball players practicing free throws for one hour. Player A shoots 200 practice shots, Player B shoots 50. The Player B retrieves his own shots, dribbles leisurely and takes several breaks to talk to friends. Player A has a colleague who retrieves the ball after each attempt. The colleague keeps a record of shots made. If the shot is missed the colleague records whether the miss was short, long, left or right and the shooter reviews the results after every 10 minutes of practice. To characterize their hour of practice as equal would hardly be accurate. Assuming this is typical of their practice routine and they are equally skilled at the start, which would you predict would be the better shooter after only 100 hours of practice?"
-Intuition behind the definition of the exponential function: We can define the constant 2.718281828459... as e using the limit e = limit as n approaches infinity of (1 + 1 / n)^n. This limit has a deep connection to the idea behind the function f(x) = e^x. The constant e is the most fundamental rate of growth for ALL continuously growing processes. To explain the base, we first begin with a very natural form of growth: discrete growth by doubling, ie consider the function f(n) = 2^n for integer n. As per exponential growth, the rate of growth is proportional to the value. Thus, at n = 0 we have the initial value f(0) = 1, and 1 "unit" later, at n = 1, we have f(1) = 2. The function has exhibited 100% growth in the discrete transition from n = 0 to n = 1, and so we can write f(1) = (1 + 100%), where the 1 represents the initial value and the 100% represents the 100% growth. We began with the function f(n) = 2^n because doubling is natural, but any function will suffice, for we will now use the intuition we have so far to examine continuous growth. In the transition from n = 0 to n = 1, the value at n = 0 doubled once we reached  n = 1. Instead, split the interval into two sub-intervals over which growth can occur: n = 0 to n = 1/2 and n = 1/2 to n = 1. Since the function exhibits 100% growth from "1 unit" of our domain (ie n = 0 to n = 1), then over half the interval it will exhibit half the growth, ie 50% growth. However, after the initial value grows to a larger value, f(1/2), during the transition from n = 0 to n = 1/2, the transition from n = 1/2 to n = 1 will cause growth in the value f(1/2), not f(0), making the total value larger than it was before. This can be expressed as f(1) = (1 + 50%)^2 = (1 + 1/2)^2. Splitting the interval into three sub-intervals, each sub-interval exhibits 33% growth but takes each previous growth factor into account during its own growth, yielding f(1) = (1 + 1/3)^3. As we continue with more and more sub-intervals, our return, f(1) improves, but it does not do so infinitely. If we partition the interval into an infinite number of sub-intervals, we have now reached completely continuous growth, and we find that f(1) = limit as n approaches infinity of (1 + 1/n)^n = e. The 1 comes from the initial value, the 1/n comes from the fact that over n sub-intervals, a growth of 100/n % is exhibited, and the exponent n comes from the fact that at each of the n sub-intervals, the growth is exhibited on the previously grown value and not on the initial value once more. 
    -Alternatively, the function f(x) = e^x has intuition by defining it as the solution to the differential equation df/dx = f. This implies that when the rate at which a quantity grows is proportional to the quantity itself, the result is exponential growth, which makes sense because if the quantity has an initial positive value, then it will grow at a rate equivalent to that amount, causing its quantity to rise and thus its rate of growth to rise, which again causes the quantity to rise, and so on. The constant e can then be viewed as the base of the "center" of the family of such exponential functions because it describes exactly direct exponential growth (whereas using other bases such as f(x) = 2^x results in a mere proportionality but not equality between rate of growth and value).
    -Intuition behind the periodicity of the complex exponential function: 
        -Quick proof of Euler's identity: Consider the differential equation df/dz = iz, f(0) = 1 where f is a complex-valued function. 
            1. z = cos(x) + i*sin(x) is a solution since dz/dx = -sin(x) + i*cos(x) = i^2*sin(x) + i*cos(x) = i*(cos(x) + i*sin(x)) = iz. Moreover, z(0) = cos(0) + i*sin(0) = 1.
            2. z = e^(ix) is also a solution since dz/dx = i*e^(ix) = iz, and z(0) = e^(i*0) = 1.
            3. Since both functions are solutions, they must be equal.
-SternBrocot tree: The Stern-Brocot sequence is a sequence similar to the Fibonacci sequence in that it begins with two numbers, both 1's, and subsequent elements are formed by adding together two previous ones. However, it differs from the Fibonacci sequence in that two numbers are added on to the end of the sequence every iteration, not one: the sum of the two digits that you are currently iterating through, as well as the second of those two digits. Thus, as one iterates through the sequence, the sequence grows ahead of the iteration point.
    -Sequence: The sequence begins with 1, 1. Since the sum of these two numbers is 2 and since the second of the two numbers at the iteration point is a 1, we extend the sequence to: 1, 1, 2, 1. Then we move our iteration point to the subsequence 1, 2. The sum of these two numbers is 3 and the second of the two numbers is 2, so we extend the sequence to: 1, 1, 2, 1, 3, 2. We then move the iteration point to the subsequence 2, 1. Following this process, we arrive at: 1, 1, 2, 1, 3, 2, 3, 1. As we continue moving our iteration point forward to the next number in the sequence and adding numbers to the end, we will eventually have the full sequence: 1, 1, 2, 1, 3, 2, 3, 1, 4, 3, 5, 2, 5, 3, ...
    -Significance: If we form fractions by using as our numerator and denominator the first and second digits, respectively, of each subsequence of two consecutive numbers in the Stern-Brocot sequence, we will arrive at every possible rational number in its simplest form. This is more elegant than the "brute force" method of forming a rectangular array of numbers with rows having increasing numerators and columns having increasing denominators (increasing as in iterating through the natural numbers) because every rational number only appears once: in its most simplified form.
        -The list begin 1/1, 1/2, 2/1, 1/3, 3/2, 2/3, 3/1, ...
    -Stern-Brocot tree: The sequence can be arranged in an infinitely long and complete binary tree (ie the node of the subtree on the left of a node is smaller than the parent node, while the node of the subtree on the right is bigger than the parent node) by beginning the tree with a single node, 1/1, and defining the left subtree's node to be the numerator of the parent node divided by the sum of the numerator and denominator of the parent node; the right subtree's node is the sum of the numerator and denominator of the parent node divided by the denominator. Then using the two newly defined sub-nodes, we recursively apply the algorithm described to obtain an ordered binary tree of every rational number.
-Pump and dump: An investment scam in which an investor promotes a stock, usually one with little to no independent information available on the public domain (as this will aid the scammer's ability to manipulate his target), to convince them to invest. As more people invest in the stock, its price rises. The rising price is often aided by natural investors that simply watch the price go up and independently choose to invest. Right before the stock price plummets due to market forces (the stock is, after all, highly overvalued), the scammer sells his shares at a large profit while all the victims lose money. The scam works best with micro-cap stocks that are highly illiquid. 
-Tueller drill: A training exercise in a knife-on-gun situation that illustrates the 21-foot rule, that an attacker with a knife can reach an officer with a holstered gun before the officer has enough time to draw and fire his weapon effectively so long as the attacker is within 21 feet of the officer. The drill is designed to prepare officers for such a situation. Shooting too early can be considered murder, but shooting to late risks injury or death. The drill attempts to locate the optimal point.
-Complexifying the integral: A technique for integration in which an expression involving sinusoidal terms, and often exponential terms, is simplified using Euler's formula that e^(ix) = cos(x) + i*sin(x). The sinusoidal terms are converted into the real part of a complex exponential, the integration is subsequently performed, and the real part of the anti-derivative is taken. The technique hinges on the generalization that integrating exponentials with linear powers is extremely easy. 
    -Example: integral of e^(-x)*cos(x) = integral of e^(-x) * Re(e^(ix)) = Re(integral of e^((-1 + i)*x)) = Re(1/(-1 + i) * e^((-1 + i)x)) = Re(-1/2 * e^(-x) * e^(ix) - 1/2 * i * e^(-x) * e^(ix)) = -1/2 * e^(-x) * Re(cos(x) + i*sin(x) + i*cos(x) + i^2 * sin(x)) = -1/2 * e^(-x) * (cos(x) - sin(x)).
-Surface tension: Cohesion, or various intermolecular forces ranging from Van Der Waal forces to Coulomb attractions due to displaced electrons that collectively cause the molecules of a liquid to attract each other, is responsible for surface tension by causing the liquid to behave as if it had an elastic membrane stretched over it. The molecules at the surface of a liquid have much stronger cohesive forces because they are not surrounded by liquid molecules on all sides; rather, there is a gas or vacuum above them, which produces a net force. Surface tension has units of energy per area or force per length.
-Prosecutor's fallacy: A common misunderstanding of the theory of conditional probability made by the general population; it is thusly named due to the high frequency with which it occurs in courtrooms, both by prosecutors trying to prove guilt and by defense attorneys trying to prove innocence. The core of the fallacy is the assumption that P(A|B) = P(B|A), which is not true. Specifically, when a suspect is put on trial and evidence is collected, a good prosecutor will argue that the small probability of collecting evidence that matches the suspect logically implies that there is a small probability of the suspect being innocent. This is incorrect; the probability that the given evidence was collected given innocence may be small, but it does not imply that the probability of innocence given the evidence (which is what the jury actually cares about) is similarly small.
    -Example: Suppose a murder is committed and a person is put on trial. If evidence comes to light that the murderer had a specific blood type that matches the suspect's blood type, and the probability of having that blood type was 1 in 1000 people, then a prosecutor might argue that because the suspect's blood type matches the murderer's blood type and the probability of having that blood type is 1 in 1000, the probability that the suspect is innocent is 0.1%. This is not true. Consider a city of 100,000 people; 100 people will still have the same blood type as the murderer, of which the suspect could be one. Thus, although the probability of having the same blood type given innocence was 0.1%, the probability of being innocent given the same blood type is 99%.
-Simpson's paradox: A counterintuitive result in statistics in which a large group of data exhibits a certain trend or correlation, but upon partitioning (ie un-pooling the pooled data) the data into sub-groups, the trend within each sub-group disappears and sometimes even reverses. It is counterintuitive that a correlation observed over several groups of data would then disappear or reverse when the groups of data are combined together. The paradox arises due to lurking variables. If two variables X and Y exhibit a certain relationship, this relationship may be misleading if a third variable Z has not been accounted for; often, if the data of X and Y are partitioned according to various values of Z the trend in each sub-group of data in X and Y will disappear or reverse. X and Y may exhibit a positive relationship over all data, but when viewed at fixed, specific values of Z, they may actually have a negative relationship at each value of Z that only appears positive when all the data is re-combined, and the effect of Z is once again hidden. 
    -Example: A textbook example of this paradox is when the University of California, Berkeley was sued for sexual discrimination by their admissions committee in 1973. In total, 46% of men who applies were accepted into the university but only 30% of women were accepted. Gender seems to be the explanatory variable and acceptance rate the observed variable, the relationship between the two being positive for males and negative for females. However, in this case, the department that was applied to was a hidden lurking variable that was unaccounted for in the initial analysis. Each individual department (eg engineering, film, art, biology, etc.) had  acceptance rates that showed no bias at all towards men; in fact, in some departments, more women were accepted than were men. None of the departments showed any gender bias; it was only when all the departments' data was combined did the trend appear. This was because of the fact that women, in general, tended to apply to more competitive departments with lower 
          acceptance rates than men did. Women tended to apply to lower-level, less specialized, and more abundant departments such as the humanities or liberal arts, resulting in most women getting rejected due to the competitive nature of the departments themselves. Men tended to apply to less competitive and demanding fields such as engineering or physics, and thus were accepted on average more than women were. This was the lurking variable that, when ignored, gave the appearance of gender bias.
-Coriolis effect: The apparent deflection of a projectile's trajectory due to its motion in a rotating reference frame. Essentially, if a projectile is thrown in a coordinate system that is rotating, it will be "deflected" from its intended path due to the rotation of the coordinate system; the projectile is not really deflected at all, but rather the observer in the rotating coordinate system moves out of the path of the projectile, giving the illusion of deflection. Consider this simple scenario: two children are on a merry-go-round, one at the center and one at the age. If the merry-go-round is spinning, then the two children are in a rotating coordinate system. However, the child at the center is simply rotating and has no linear velocity, whereas the child at the edge has a positive linear velocity. So if the observer at the middle tries to throw a ball at the child at the edge, since the former is moving faster than the child at the center, the ball will miss the child at the edge because he has moved out of the intended path by the time the ball reaches its destination. 
    -Meteorological effects: This effect also applies to the Earth, which rotates. Since the circumference of the Earth varies depending on one's latitude (the equator has the largest circumference, 60 degrees latitude is about half, and the north pole has 0), the tangential velocities also vary (since the rotation rate is a constant 24 hours). Thus, if a projectile is fired from one latitude to the other, it will miss the intended path due to moving too fast (if thrown from a lower latitude to a higher one) or will move to slow (if thrown from a higher latitude to a lower). This is the reason the hurricanes spin counter-clockwise in the northern hemisphere and clockwise in the southern.
-Azidoazide Azide: (A.K.A. AA) A substance with a chemical formula of C_2N_14; one of the most explosive compounds known to man. It is a high nitrogen energetic material (which is a chemical classification for substances with a high density and number of nitrogen atoms. These atoms tend to triple bond to each other in a very powerful covalent bond; a triple bond is the most stable energy state. When nitrogen atoms go from a higher energy state to the lower triple bonded state, energy is released). In AA, none of the 14 nitrogen atoms is in a triple bond, but all of them have a propensity to attain a lower energy, triple bonded state. This makes it extremely reactive and very violent, since even a slight disturbance can trigger the restructure of the molecule into a triple bonded state that releases energy. This process can be triggered by even the most cautious of actions, such as moving or touching the substance, and in some cases, it can occur spontaneously. This makes the substance extremely hard to study, 
           and so little is known about it.
-Langlands Program: A collection of mathematical conjectures, first formulated by Robert Langlands, covering Galois theory (the theory of Galois groups, ie groups associated with a particular field extension in the sense that it is the group of automorphisms (a homomorphism from a field to itself) from the field extension to itself that fixes the field extension pointwise (ie a(x) = x for all a in Aut(E/F) for fields E and F); the group is taken under the binary operation of function composition) and its relation to representation theory.
-Run length encoding: Compressing a list of numbers by, instead of listing all the numbers out, noting the number of times a particular element of the list occurs in a consecutive sequence and storing that number in the memory instead of the actual sequence of numbers. 
    -Example: To encode the list [1, 1, 1, 3, 3, 6, 6, 6, 2, 2, 2, 2, 3, 3, 1, 4, 4], we would create the list of pairs [[1, 3], [3, 2], [6, 3], [2, 4], [3, 2], [1, 1], [4, 2]], where each element in this new encoded list is a pair whose first element is the number and the second the number of times the first number appears.
-Loss aversion: The psychological phenomenon that people tend to view losses as much more harmful in magnitude than they view gains as positive in magnitude. On average, people tend to "feel" potential losses twice as hard as a gain of the same magnitude (ie a person would not be willing to risk losing 10 dollars for the chance of winning 10 dollars, but would take the bet if they won a minimum of 20 dollars). People thus prefer avoiding losses to making gains. Another consequence is that someone who loses a sum of money will be sadder than someone who gains the same sum of money will be happy.
-Peter principle: An observation of inefficiency in current management structures (though the principle applies to most general structures that involve meritocratic advancement anyways). Because people are promoted based on their skill level in their current position, they will consistently be promoted every time they are skillful on their current level, and so will be promoted upwards throughout the ranks until they fail at some level, where they remain. Thus, many people who work at their current level are incompetent for that level, leading to inefficiency. The logical advancement mechanism would be to promote people based on their ability to perform skillfully in their promoted role rather than the current role. 
-Rule of two and four: A quick approximation technique in poker to estimate the probability of having a winning hand. First, one must be able to count the outs (ie counting the number of cards that can be played on the turn or river that will lead to a winning hand (when combined with the pocket cards and the flop)). Multiplying the number of outs (from the flop) by 2 yields the percentage probability of getting a winning hand on the turn (or getting a winning hand on the river if the turn has already been dealt), and multiplying by 4 gives the probability of getting a winning hand on either the turn or the river. In the majority of cases, players will need to multiply by 2, since multiplying by 4 if only done if only the flop has been dealt and the opponents are all-in. This is because if one's opponents are all-in, one does not expect to face any extra bets (ie raises) that would decrease your pot odds.
    -Usage: If the resulting probability obtained through the rule is greater than one's pot odds, it is a good idea to continue; if the probability of winning is less than one's pot odds, one should fold, since it's not worth it to chase a smaller probability of winning for larger pot odds.
        -Pot odds: The ratio of the total amount one could win to the amount one must wager to win it. Thus, if a pot contains x units of currency and someone raises the bet by y units (and so the pot now contains (x + y) units), the pot odds are (x + 2y) : y, since one can win a total of ((x + y) + y) (the current size of the pot plus one's own bet, should he choose not to fold) but will need to wager y to get it. Expressed as a fractional probability (ie a probability between 0 and 1 as opposed to odds), it is y/(x + 2y).
-Mens rea: A common requirement for an act to be considered a crime; it describes the condition that the perpetrator must have had a guilty or malicious intent to commit the crime, and did not commit it purely accidentally and without knowing. It comes from the Latin phrase "actus reus non facit reum nisi mens sit rea", which translates to "the act is not culpable unless the mind is guilty".
-Phi coefficient: A measure of correlation between two binary or boolean variables. The traditional measure of correlation is used for numerical data, but often it is helpful to measure how much two binary variables correlate. A binary variable does not take on numerical values, but rather only two distinct values (not unlike a boolean variable) referred to as 0 and 1, or true and false, etc. Consider two binary variables x and y, each of which can be either 0 or 1. To calculate the coefficient, first create a contingency table, ie a 2 by 2 matrix whose two columns represent the 0 and 1 for the x variable and whose rows represent the 0 and 1 of the y variable. 
          Contingency table:
                    x = 0     x = 1      Total
            y = 0     A         B       (A + B)
            y = 1     C         D       (C + D)
            Total  (A + C)   (B + D)       N
           Where A = the number of observations that have both x = 0 and y = 0, B = the number of observations where both x = 1 and y = 0, and so on, out of a total of N observations. The phi coefficient essentially compares how much data lies on the main diagonal of the matrix (since the main diagonal consists of entries in which both variables match) against how much data falls off the main diagonal (where the variables differ). This can be done by comparing products: AD - BC. The true formula is phi = (AD - BC)/sqrt( (A + B)(C + D)(A + C)(B + D) ), where the denominator, the square root of the product of all the totals, is included to ensure that the coefficient is always between -1 and 1.
-Bush dome: Formally known as the National Helium Reserve, the Bush Dome is a stockpile of helium by the US government. It contains over 1 billion cubic meters of helium (H_2) gas, located in Amarillo, Texas. In 1996, the Helium Privatization Act of 1996 was passed, which called for the government to begin liquidating the reserve by slowly selling it into the private market by 2005. However, even by 2011 the majority of the reserve was still owned by the government, and in 2013, however, the House of Representatives voted to extend the reserve's lifespan.
-Parkinson's Law: "Work expands to fill the time allotted"; the statement is an observation on procrastination among people in that the amount of time it takes to finish a task depends more on the deadline for completion than on the difficulty of the task itself, especially with tasks requiring self-discipline. If one allots a week to complete a task, it will probably take the week (due to various breaks and other inefficiencies), but if one only gives oneself three days, it will only take the three days because one is forced to simply work at a level that is closer to one's maximum potential for efficiency.
-Archimedean property: A field F has the Archimedean property if the following statement holds: For all (x > 0) and y in F, there exists a natural number n such that nx > y. This theorem effectively states that F has no infinitely large or infinitely small elements.
-United States v. Scheinberg: A federal criminal court case between the federal government and the three largest online poker companies: PokerStars, Full Tilt poker, and Cereus. The three companies were convicted of vioLating the Unlawful Internet Gambling Enforcement Act of 2006 (which "prohibits gambling businesses from knowingly accepting payments in connection with the participation of another person in a bet or wager that involves the use of the Internet and that is unlawful under any federal or state law"). The date April 15, 2011 is now known in the poker community as Black Friday, the date when the companies were publicly indicted and online gambling made illegal.
-Geneva drive: A type of gear in which one of the gears is a continuously rotating wheel, disc, or circle, and the other is a regular gear. Essentially, the drive's purpose is to provide a mechanism that transforms continuous circular rotation into discrete, intermittent rotary motion, such as a gear that turns a certain fraction of its circumference every unit of time. 
-Endgame tablebase: A large collection, usually computerized, of various chess endgame positions along with detailed analyses of each position. These databases are stored in the memory of a chess engine and used during play. For a given position, the datase contains the win, loss, or draw value for every possible move in that position, and the number of moves to achieve that value. These tables are generated using retrograde analysis, ie by starting with a checkmated position and working backwards to generate positions. The computational power needed as one moves backwards from the checkmated position increases more than exponentially, and so only positions with fewer than seven pieces total (including both kings) have been completely solved, excluding positions with only two kings or positions with one king and the other king with five other pieces; these positions are considered trivial, with the former being a draw and the latter being a very easy win. The most popular and well known tablebase are the
            Nalimov tablebases, which require over a terabyte of storage. 
    -Solving games: Theoretically, any game in which the player has perfect information (all information relevant to the game at that state of the game) and the game does not depend in any way on probability can be solved completely with enough computing power. Note that solving a game does not guarantee a win but rather guarantees that one will not lose (ie at least a draw), such as the Tic Tac Toe solution algorithm, which guarantees at least a draw with perfect play. 
-Operation Popeye: During the US war in Vietnam, the US military attempted to weaponize the weather system above Vietnam through an operation known as Operation Popeye. The operation included cloud seeding (injecting into cloud systems) silver iodide and lead iodide into clouds to increase the amount of precipitation in Vietnam and extend the monsoon period by 4 to 6 weeks, with the intention of causing landslides, slowing down enemy traffic, disrupting logistics, etc. The operation is now considered to have been relatively successful. At the time, it was highly classified. 
-Alcohol withdrawal: Once a heavy alcohol drinker is addicted, sudden discontinuation or decrease in consumption can cause potentially fatal withdrawal effects. Alcohol is the only recreational drug capable of causing death simply from withdrawal, as opposed to fatal complications resulting from non-lethal withdrawal symptoms. Alcohol works by enhancing the effect of the neurotransmitter GABA (gamma-Aminobutyric acid), which relaxes and calms the body. As tolerance builds up from constant drinking, baseline GABA levels are actually suppressed as the body reacts to the artificial effects of the alcohol (this is why more and more alcohol is needed to achieve the same effects). The constant consumption of alcohol continuously suppresses GABA activity in this way, and sudden discontinuation causes a rebound of pent-up GABA levels in the brain, producing a phenomenon known as brain hyperexcitability, which has symptoms of anxiety, irritability, agitation, tremors, seizures, and delirium tremors, and can be fatal.
-Wagon wheel effect: A phenomenon in which objects rotating at very high speeds appear to rotate in a different way than reality. The effect is an optical illusion and is most commonly observed in rotating wheels that have spokes or on rotating propeller fans found in planes. The effect depends on a stroboscopic environment, that is, a medium characterized by discrete rather than continuous time. As such, it is most commonly seen in recorded media (which are subject to frame rates as opposed to true continuous motion) or in wheels upon which a flickering light is shone. The effect occurs because although a given spoke of a wheel might begin at position 1, after a frame, it has rotated to a position between halfway around the wheel and its original position; the net motion appears to be slightly backward instead of forward. In the next frame, the same thing happens again, with the spoke ending up slightly before its initial position. This collective effect gives the appearance of the wheel rotating backwards. 
             The effect has been observed to distort rotation into various different forms, not just backwards.
-2010 Flash Crash: A crash in the stock market that occurred on May 6, 2010. The market (specifically, the Dow Jones Industrial Average) plunged extremely rapidly by 9% (about 1000 points) in minutes, and quickly recovered a few minutes later. 
    -Cause: The algorithms that HFTs (see below) run on are based on certain mathematical models of the stock market, many of which assume that the market, more or less, roughly adheres to some form of a random distribution. When high-quality professional traders enter a narrow market, the trading becomes much more "rational", which also makes it much more difficult for HFTs to make money trading. Eventually, the algorithm responded by simply discontinuing trading altogether. The ramifications of this computerized action led to the flash crash.
        -Market makers: (Formerly known as "specialists) People or even companies that work on the stock market exchange by quoting the other side of incoming trades that did not immediately have offers (ie buying trades on the market that no one else was buying or selling trades that no one else wanted to sell). They hoped to make a profit by exploiting the bid-ask spread (the difference between the price bid on an asset and the actual price the asset is sold at) on the securities; such a discrepancy exists because almost all assets are at least slightly illiquid (assets with bid-ask spreads of zero are known as frictionless assets, and are completely liquid). Many argued that market makers provided a public service to the stock market by keeping it orderly and providing stability, and the government at one point even had designated market makers, known as specialists (in NYSE and AMEX (American Stock Exchange)). Lately, however, much of the market-maker style strategy is performed by high-frequency traders.
            -High frequency trading computers: (A.K.A. HFTs) Computers equipped with complex algorithms that buy and sell securities extremely quickly (on the order to seconds or even milliseconds), often scraping a razor thin profit margin but trading so much volume that a significant profit begins to turn. These computers are largely replacing human market makers.
-Tianhe-2: A computer rated at 33.86 petaflops (10^15 FLOPS) developed by over 1300 computer scientists in China. It is widely known as the world's fastest supercomputer to date. The whole project cost $390 million. 
    -FLOP: (FLoating-point Operations Per Second) A measure of computer performance that quantifies the number of base operations the computer is capable of executing per second.
-Artificial neural network: (A.K.A. ANN) A type of computing architecture that tries to solve problems in a way that emulates the human brain. ANNs are used both for neural simulations and computational mechanisms themselves, and have shown promise in machine learning (the idea of a computer learning through experience the way animals do). ANNs often exhibit the basics of pattern matching, heuristic analysis, predictive extrapolation, and other traits commonly associated with human intelligence.
    -Structure: The structure defined here is sometimes known as a back-propagation ANN. As with the central nervous systems of many animals, an ANN works by first utilizing a large amount of basic unit processor elements (either physical processors or software ones), simuLating the neurons in a brain, and draws connections (simuLating synapses) between them in a very complex network. These connections are structured so that there exists a loose layered structure in the ANN, with a bottom basic input layer where massive amounts of data are inputted (and subsequently divided and entered, several at a time, into individual neurons). The neurons, having had data inputted, use certain weighting functions to streamline and modify the data, favoring certain parts and rejecting others, drawing patterns among the data, extrapolating and interpolating the data, etc. The neurons in the input layer use these weighting functions to synthesize some output, and each neuron sends its output along all the connections to wit's attached, reaching the neurons in the upper layers. There are typically several more hidden layers of neurons until finally, an output layer of neurons receives the data and synthesizes is into a final output. The weighting functions used by the neurons and the connections between the neurons typically start off as being completely random, and a very large number of such trials with different weighting functions and neural connections (with the same data of course) are run. Each time a particular output is produced and the quality of each output is evaluated. The weighting functions and neural connections that produced favorable outputs are reinforced, while those that produced unfavorable outputs are diminished. Then the process is repeated recursively, now using the slightly better ANN to analyze data again and subsequently improve. Thus, in this way, a machine can learn.
    -Feed-forward network: A very common type of neural network in which the pattern in which neurons are connected is stratified into separate layers, with information flowing forward in one direction through the layers until it reaches a final output layer. Each layer consists of a number of neurons in parallel to each other, none of them connected to each other, receiving inputs from the previous layer and sending outputs to the next layer. Each neuron in a given layer is connected to every single neuron in the next layer, though some networks modify this structure. Thus, there are never any cycles or loops in this structure. The first layer is where the problem to be solved is inputted as initial inputs, and is known as the input layer, and the last layer that outputs an answer is known as the output layer; all other layers in between are known as hidden layers since they are hidden from the user.
    -Theoretical basis: 
        -Biological inspiration: Neural networks are based on the structure of the human brain, in which neurons connect to other neurons with long connections called axons, through which they are able to send signals (in the form of a propagating wave of electrochemical ions known as an action potential), and are connected to other neurons with dendrites, through which other neurons' axons connect and dump the above mentioned ions, allowing the neuron to receive a signal from another neuron. The connection between individual neurons are not all equally strong; some connections have more "weight" than others, in that more stimulus is passed on. If enough stimulus is received (ie the inputted stimulus surpasses some threshold), the neuron "fires" and sends another signal outward to any neurons that it's axon is connected to.
        -Computational model: A neural network is simply a collection of base processing units connected unidirectionally to one another, structured in a certain way that allows them to relay information and synthesize it into an output. The structure of a single processing unit, or "neuron", is described below; the entire network consists of many neurons organized and connected to each other in a pre-determined way. There exist a set of certain input neurons that receive user-inputted inputs (these inputs correspond to the problem being solved) and there are also certain output neurons which do not pass information along to any other neurons, but rather only receive information and instead output a final answer.
            -Neurons: An individual base processing unit is the computational analog of the biological neuron; this unit, ie the computational neuron as it will be hereafter called, will receive some arbitrary number, say n, of inputs which are either a 0 or a 1 (signifying whether or not the corresponding neuron that sent that input to the current neuron fired or not); each input is relayed from a previous neuron to the neuron in question through a connection or mapping, and the input is multiplied by a weight unique to that connection (the weight can be positive or negative). Thus, the neuron ends up with n input-weight products that it received from n previous neurons that are connected to it. The neuron will process all of these input-weight products with an activation function, which maps n-tuples of real numbers to another real number, to produce the an output known as the neuron's activation. The activation function is most commonly a simple sum of all the (positive and negative, due to the weights) n input-weight products. If this activation exceeds some threshold, the neuron will output a 1, signifying that the neuron fired, and otherwise outputs a 0. Note that other output functions can be used, rather than simply testing if the activation is larger or smaller than some threshold value, in order to map the activation to some other proportional output that better suits the needs of the problem the neural network is solving; however, the "all-or-nothing" output function above mentioned is the most common. 
        -Learning: A basic neural network with random weights, as described above, is not much use; rather, the potential and success of neural networks comes from their ability to evolve with respect to a certain problem and solve it better. If a neural network is meant to solve a certain problem, it can use training samples, which are simply sample problems along with their solutions, to learn how to solve the problem in the general case. Thus, a training sample is abstractly a sample of the problem being solved along with the solution to that particular case, and consists of some inputs to the neural network and some expected or desired output. We can define such a "desire function" that maps certain inputs to certain desired outputs. We can also define an error function which quantifies how well or poorly the neural network performed; such an error function will compare the neural network's output to a particular input set to the desire function's output on the same input set, and thus the objective of the neural network is to gradually minimize the error function as more and more training samples are entered. There are many different training samples in use, such as a simple difference between the actual output and expected output, though more complex functions are also common in practice. This process of a neural network using successive training samples to locate the global minimum of the error function is, in a nutshell, the entire purpose of a neural network and its ability to learn.
            -There arises a concern when the output function used is the simple threshold function. Notice that we described the learning process as "fine-tuning" all the individual weights in order to maximize performance, but this approach neglects an additional variable - the threshold. We can also vary the threshold value a neuron uses (each neuron has its own unique threshold value), and these variations can also increase or decrease performance. If our approach consists only of modifying weight vectors, we neglect to ever modify the threshold of a neuron and thus the approach will never be completely optimal. However, if our activation function is the above mentioned simple summation of all input-weight products, we can solve this problem with a clever modification: change the threshold of every neuron to be exactly 0, and add another "input" with value -1 and weight equal to the threshold to every neuron. Then when all the input-weight products, including the product of -1 and the threshold, are added up, the same effect is achieved; if the original activation (without the -1 input) was greater than the activation, than the activation that includes the -1 input will be positive, and otherwise it will be negative. The only difference is now the threshold is part of the weight vector, and thus will be taken into account when we locate the optimal weight vector. Note that this modification works only when the output function is the simple threshold function and when the activation function is the simply summation function; in other cases, unique modifications must be tailor-made to also locate the optimal output function along with the optimal weights. 
            -The threshold output function described above is discontinuous and so can be difficult to work with, mathematically. For this reason, sigmoid functions are commonly used instead. A sigmoid function is a function that emulates that behavior of the threshold function almost everywhere (ie horizontal lines at 1 and 0 after and before the threshold point, respectively) but continuously transitions around the threshold value rather than discontinuously jumping. The function is known as a sigmoid function because it has an "S" shape. The most common sigmoid function and the one used in neural networks is: f(x) = 1 / (1 + e^(-x)).
        -Back-propagation: The neural network can be viewed as a function that maps an input vector, which consists of the n inputs to the system, and a weight vector, which consists of all the weights used in a specific network, to a specific output. Clearly, the input vector is constant when analyzing a given training sample, and so to maximize performance (ie minimize the error function), we must find the optimal weight vector which minimizes the error function (since the inputs are constant for a given training sample, the error function as well as the desire function are both functions only of weight vectors). Thus we have reduced the neural network problem to an optimization problem, and we can solve such an optimization problem using common optimization techniques, such as with a hill climbing algorithm (an algorithm which optimizes a (continuous) function by starting at a random input of the function, moving a small amount in every direction of the input space and evaluating the function at each tiny perturbation, and then moving to the most optimal perturbation and repeating the process until every such perturbation in every direction is sub-optimal to the current position, which is then a maximum (though not necessarily a global maximum) by definition). Note that a hill climbing algorithm might be very inefficient if there are a large number of weights, since the input space will be huge and the "number of directions" to evaluate at each step of the hill climbing algorithm is huge. In this specific case, we wish to find the combination of weights that produces the optimal output (ie the output closest to the desired output). Because, as mentioned above, hill climbing algorithms can turn out to be inefficient, the more sophisticated method of gradient descent is more commonly used (which utilizes the partial derivative of the error function with respect to the weight vector). 
            -Algorithm: The algorithm is most commonly used in feed-forward networks, and is a supervised learning algorithm, meaning that the algorithm requires training samples that provide examples of the expected output for a certain input. The algorithm uses many such training samples to improve its pattern recognition until it is good enough to solve the problem in the general case. The algorithm, as described above, gradually and successively minimizes the error in the actual and expected results. To begin, the neural network is initialized with random weights. These weights will be continuously adjusted until the error is minimal. The activation function used in this algorithm is simply the common weighted sum, ie the sum of all input-weight products. The output function will be the basic sigmoid function described above: 1/(1 + e^(-a)), where a is the activation (which is obtained by plugging all the input-weight products into the activation function). For a fixed input, if we denote the desired (ie expected) output for the j-th neuron in the output layer as d_j, then we can define the error function for the j-th neuron in the output layer as (O_j - d_j)^2, where O_j is the actual output of the j-th neuron in the output layer (we square to ensure that it is always positive), and so the actual error function is simply the sum of all the individual layers, ie E = sum over j of (O_j - d_j)^2, where j runs over all neurons in the output layer. Now, the next steps in the algorithm are to first figure out how the error depends on each weight, and then to adjust each weight with the method of gradient descent. The gradient descent method is well understood, and indeed the bulk of this algorithm is computing the partial derivative of the error function with respect to an arbitrary weight. This computation can be done with repeated application of the chain rule, since the error function depends explicitly on the output function, which depends explicitly on the activation function, which depends explicitly on the weights. This approach will provide a description for how much the error function depends on each weight, but only for weights between the penultimate layer (ie the last hidden layer) and the output layer, since the output function referenced above is the output function for a neuron in the output layer. For neurons in some intermediate hidden layer, the above referenced output function (for a neuron in the output layer) depends on all the output functions and activation functions in between the hidden layer's neuron and the output layer's neurons. Thus, the computation for the partial derivative of the error with respect to a weight involves more and more applications of the chain rule the further away the weight is from the output layer, since there are more neurons and thus more functions (specifically, output and activation functions) in between. For the j-th neuron in the output layer, consider the weight corresponding to the connection between the i-th neuron in the last hidden layer and the j-th neuron in the output layer, denoted w_ij. dE_j / dO_j = 2 * (O_j - d_j), dO_j / dA_j = d/da (1 / (1 + e^(-a))) = e^(-x) / (1 + e^(-x))^2 = (O_j)(1 - O_j) (where A_j is the activation function), and dA_j / dw_ij = x_i (where x_i is the input from that i-th neuron from the last hidden layer, or equivalently, the output the the i-th neuron in the last hidden layer). Putting it all together, dE_j / dw_ij = (dE_j / dO_j) * (dO_j / dA_j) * (dA_j / dw_ij) = 2 * (O_j - d_j) * O_j * (1 - O_j) * x_i. Now we apply the method of gradient descent, in which we adjust the weight w_ij by an amount proportional to how much the error depends on it (meaning that if w_ij contributes a lot to the error, we adjust it a lot, and if it contributes a little to the error (meaning it's already quite close to its optimal value), we adjust it a little). This can be written as delta(w_ij) = - (eta) * dE / dw_ij, for constant of proportionality eta (the expression is multiplied by -1 since we are minimizing). Thus, we change the weight w_ij to: w_ij + (-2 * eta * (O_j - d_j) * O_j * (1 - O_j) * x_i). Once this is done for every neuron in the output layer, repeat the process for every neuron in the last hidden layer, this time computing the derivative with one more application of the chain rule to account for the the extra activation and output function in between each weight and the final output. Keep working backwards to the penultimate hidden layer, then the hidden layer before that, then the layer before that, and so on until the input layer is reached, at which point the algorithm terminates. Once this process is finished, the neural network has completed the learning process for the given training sample. Repeat the entire above process for many training samples, successively improving the weights each time, and eventually the network will be trained to a high degree of accuracy.
                -As can be guessed from the algorithm's description, the algorithm is not efficient, and increases exponentially in runtime with the number of neurons or the number of hidden layers.
                -The algorithm is named "back-propagation" because hidden layers are iteratively trained backwards from the output layer, allowing errors to "propagate backwards" throughout the network.
    -Drawbacks: The above described structure limits ANNs to learning in very specific and narrow tasks/fields. In addition, the process can be costly and inefficient. 
-Evolutionary algorithms: A generic optimization algorithm that works by mimicking mechanisms of biological evolution; rather than organisms evolving to better adapt to their environment, programs are evolved to better and more efficiently solve a problem. These algorithms are generally fairly successful.
    -Genetic algorithm: An algorithm with the goal of improving systems by mimicking natural selection (from evolutionary biology). Genetic algorithms are most effective in solving optimization problems. An initial class of possible solutions (usually randomly generated) to a problem are generated, and a pre-defined fitness function applies the solution to the problem and rates how effectively the solution solves the problem. Individual possible solutions to the problem are referred to as individuals, and each iteration of the algorithm is referred to as a generation (mimicking biological terms in evolution). Individuals with poor fitness-ratings are successively eliminated for the next generation, whereas individuals with high fitness are copied several times, with each copy having its source code (sometimes randomly) modified in a unique way. This is to simulate genetic mutation with each generation; beneficial mutations are copied further while harmful ones die out. The process of fitness assignment is then applied again iteratively, over and over, and the class of individuals improves and converges to a solution. The procedure can theoretically go on forever, generating an infinite number of generations, so in practice the iterations are continued until a sufficient level of fitness in the generation has been achieved or a certain number of iterations has passed.
-Escrow: A contract used to monitor large financial transactions in which a third party receives and holds onto the money from the transacting party, and then distributes it to the recipient(s).
-Truth trees: (***INCOMPLETE***)
-Necrosis: The unnatural death of cells due to external factors. Necrosis is a form of cell death, but differs from apoptosis (which is a naturally occurring and specifically engineered form of cell death that benefits the organism) in that it is not natural, and is almost always detrimental or even fatal. It can be caused by toxins, viruses, bacteria, or trauma. The cells die with the same mechanism that causes death in apoptosis: autolysis, or self-digestion of the cell by its own enzymes.
-Vibrational bonding: A recently (2015) discovered type of chemical bond, with the unique property that its existence causes certain chemical reactions to paradoxically slow down as the temperature is increased (going against a very general principle in chemistry that temperature and reaction rate are proportional). The bond is essentially a muonium atom (an exotic type of atom, very lightweight, consisting of an antimuon orbited by an electron) is attracted rapidly to two atoms at once, bouncing back and forth between them and causing them to vibrate (hence the name of the bond). As the two atoms share and "juggle" the muonium atom, they are loosely bound together in a bond. This type of bond only occurs when temperatures are increased, but by holding atoms together, the overall energy of the reaction is reduced, giving rise to the aforementioned paradoxical inverse relationship between reaction rate and temperature. 
-Paraneoplastic syndrome: A possible medical consequence of cancer. Essentially, the tumor secretes hormones or cytokines that trigger an immune response against the tumor, but when the immune system begins attacking the body's own spinal cord, brain, muscles, etc. the result is a diverse array of symptoms.
    -Symptoms: Ataxia (difficulty walking, nausea, loss of musculature, memory loss, vision problems, trouble sleeping, dementia, difficulty swallowing, etc.
-Maggot therapy: A medical treatment dating back thousands of years that uses maggots to clean open wounds. When an open wound is difficult to operate on and must be kept clean, live maggots (that have already been disinfected), may be applied onto the wound. The maggots eat away only the dead cells and skin, cleaning the wound. Maggots cannot reproduce in the wound as they are, at that point, too early in their life cycle to reproduce (they must first become adult flies). 
-Stuxnet virus: A computer worm designed to attack programmable logic controllers (A.K.A. PLCs; control the automation of mechanical processes (eg factory assembly lines, etc.)). The worm was incredibly sophisticated, and was released purposefully into the Iranian mainframe that handled the automation (specifically, the PLCs) of nuclear centrifuges (used to separate uranium and other radioactive material by isotope). The origin of the worm is unknown, though some suspect Israel or the US. The worm is estimated to have destroyed around 20% of Iranian nuclear centrifuges by causing them to rotate well above their mechanical limits, thereby destroying themselves (an early version of the worm was meant to manipulate the valves on the centrifuges in order to increase the pressure inside the centrifuges). The worm targets Windows operating systems and then targets Siemens Supervisory Control and Data Acquisition (A.K.A. SCADA; system encoded with signals transmitted over a communications channel to provide control of equipment from a distance. The worm then took control of a part of the software (called Step-7) that was vital to reprogramming the PLCs to which it was linked. The worm primarily spreads via USB drives; the worm was released by first infecting five external computers that were related to the Iranian nuclear mainframe, the target computers.
-Grey goo: A hypothetical apocalyptic scenario based on nanotechnology. The scenario begins with the creation of nanobots (microscopic (on the order of a nanometer, 10^(-9) meters; a virus is about 1000 nanometers), self-sustainable robots programmed with a purpose) that are programmed with the ability to self-replicate. The idea is that the nanobots would contain some internal structure capable of utilizing some input (eg light, carbon atoms, water molecules, etc.) and rearranging the input's molecular structure to create more nanobots. If such nanobots were created, even with positive intentions (eg cleaning up an oil spill), they would reproduce exponentially: If a single nanobot can reproduce itself every 1000 seconds, it will take 1000 seconds for 2 nanobots, 2000 seconds for 4, etc. until there are 68 billion nanobots after 10 hours; in under 24 hours, the nanobots collectively weigh a ton, and less than 24 hours later, they weigh as much as the earth. Another four hours after this and they weigh more than the sun and all the planets in the solar system combined. Such nanobots would rapidly consume all possible materials they could utilize for reproducing on earth, leading to the likely extinction of the human race. The nanobots keep reproducing until they are out of material to input.
-Chinese room argument: A thought experiment that seems to assert that it is impossible for a computer to gain consciousness. The idea is based on a metaphor: consider a computer that has been programmed with an extremely high degree of sophistication, and is able to generate believable and human-quality responses, in any language, to any inquiry. However, John Searle (the creator of the experiment) points out that the computer is merely algorithmically manufacturing the optimal response to a question and outputting it; it does not actually understand any of the language incident on it, but rather it merely selects the appropriate words (using sophisticated algorithms) from the dictionary stored on its hard drive and, using the grammatical structure stored on its hard drive, puts the words together to form a coherent sentence. None of this process involves any level of comprehension of the inputted language, and is akin to a Chinese room. The computer is analogous to a person in a room. The person does not speak Chinese, but has available a collection of Chinese symbols, as well as a rule book that associates specific combinations of symbols to other combinations of symbols. Thus, if the person receives a sequence of Chinese symbols and follows the rule book to present another sequence of symbols, it is possible in such a manner to carry out a full conversation in Chinese even without the person speaking a word of Chinese.
-Ventricular fibrillation: (A.K.A. v-fib) Sporadic and uncontrolled spasming of the cardiac muscles in the heart (specifically, the ventricles (two chambers in the heart that receive blood (by expanding) from the atrium (the chamber where blood directly enters the heart from the circulatory system; the blood is pushed into the ventricles, where it is expelled into the circulatory system again) and expel it to circulation throughout the bloodstream by contracting; the human body has four ventricles)) that causes them to shake and quiver instead of contract. If the condition lasts too long (on the order of seconds), the patient enters a state of asystole (colloquially, "flatline"; cessation of heart activity) and cardiogenic shock (an irreversible condition, usually fatal, consisting of massive cell death due to oxygen and nutrient starvation (hypoxia and hypoglycemia, respectively); although the lungs are intaking oxygen, the heart cannot replenish the blood with it).
-American leafletting of Japan: During the second world war, American forces fought the war in the Pacific as well as in Europe. The primary strategy in attacking Japan for the majority of the Pacific War was massive area bombing of Japanese cities; some cities had up to 97% of their infrastructure destroyed by the incredibly destructive fire bombing campaigns. Several months prior to fire bombing, US forces released tens of thousands of leaflets (small inexpensive pieces of paper with concise information on it) warning, in Japanese, of impending bombing and urging civilian residents of the cities to evacuate immediately. It's estimated that in total over 63 million leaflets were dropped. It should be noted that no leaflets were dropped in Hiroshima prior to its being struck with Little Boy; this was done to maximize the psychological trauma and shock from the world's first atomic weapon. 
    -Text: The leaflets stated (in Japanese), among other things:
        -"TO THE JAPANESE PEOPLE: America asks that you take immediate heed of what we say on this leaflet. We are in possession of the most destructive explosive ever devised by man. A single one of our newly developed atomic bombs is actually the equivalent in explosive power to what 2000 of our giant B-29s can carry on a single mission. This awful fact is one for you to ponder and we solemnly assure you it is grimly accurate. We have just begun to use this weapon against your homeland. If you still have any doubt, make inquiry as to what happened to Hiroshima when just one atomic bomb fell on that city. Before using this bomb to destroy every resource of the military by which they are prolonging this useless war, we ask that you now petition the Emperor to end the war. Our president has outlined for you the thirteen consequences of an honorable surrender. We urge that you accept these consequences and begin the work of building a new, better and peace-loving Japan. You should take steps now to cease military 
          resistance. Otherwise, we shall resolutely employ this bomb and all our other superior weapons to promptly and forcefully end the war."
        -"EVACUATE YOUR CITIES. ATTENTION JAPANESE PEOPLE. EVACUATE YOUR CITIES. Because your military leaders have rejected the thirteen part surrender declaration, two momentous events have occurred in the last few days. The Soviet Union, because of this rejection on the part of the military has notified your Ambassador Sato that it has declared war on your nation. Thus, all powerful countries of the world are now at war with you. Also, because of your leaders' refusal to accept the surrender declaration that would enable Japan to honorably end this useless war, we have employed our atomic bomb. A single one of our newly developed atomic bombs is actually the equivalent in explosive power to what 2000 of our giant B-29s could have carried on a single mission. Radio Tokyo has told you that with the first use of this weapon of total destruction, Hiroshima was virtually destroyed. Before we use this bomb again and again to destroy every resource of the military by which they are prolonging this useless war, petition the emperor now to end the war. Our president has outlined for you the thirteen consequences of an honorable surrender. We urge that you accept these consequences and begin the work of building a new, better, and peace-loving Japan. Act at once or we shall resolutely employ this bomb and all our other superior weapons to promptly and forcefully end the war. EVACUATE YOUR CITIES."
-Tuskegee experiment: (***INCOMPLETE***)
-Bengal famine of 1943: (***INCOMPLETE***)
-Amino acid: Biologically occurring organic molecules, made of amine (NH_2) and carboxylic acid (CO OH, ie a carbon monoxide bound to a hydroxide) functional groups (specific classifications of molecules that commonly bind together and produce many of the characteristics observed from a chemical reaction); each amino acid has a unique side chain as well. There are over 500 known amino acids.
    -Proteinogens: A group of 20 amino acids used in the human body to build proteins; they are essential to human life and the birth of cells. There are three main groups: essential, semi-essential, and non-essential (all with respect to human life).
        -Essential: Eight amino acids are essential. They are: isoleucine, leucine, lysine, methionine, phenylalanine, threonine, typtophan, and valine.
        -Semi-essential: Arginine and histidine. 
        -Non-essential: alanine, asparagine, aspartic acid, cysteine, glutamine, glutamic acid, glycine, proline, serin and tyrosine.
-Structure of human skin: The skin is the largest organ in the human body and accounts for approximately 15% of a human's weight. It exists to protect the internal organs of the body from exposure to the elements, to prevent excessive water loss by evaporation, and as one of the frontline barriers to pathogens and hence infection, among other things.
    -Surface: The surface of the skin contains many oil and sweat glands, which secrete fluids onto the skin surface that keep it moderately acidic (pH of between 3 and 5); this hampers the growth of microbes on the skin. The skin surface is also covered with many harmless bacteria that prevent harmful microbial invaders from growing on the skin's surface simply by competing with them for space. The sweat secreted by sweat glands contains the enzyme lysozyme, which digests the cell walls of bacteria.
    -Epidermis: A thin layer, 10 to 30 cells thick (about the width of a piece of printer paper). The epidermis has two layers: an outer layer called the stratum corneum, and an inner layer called the stratum basale. The stratum corneum's cells are constantly injured and worn down by friction and stress during the day-to-day activities of the body, so cells from the stratum basale divide at a constant and extremely high rate; they then migrate upwards to the stratum corneum to replace their damaged counterparts. The cells making up the epidermis also synthesize a protein called keratin, which toughens the structure of the skin (reducing likelihood of cuts and microbial entry, as well as inducing hydrophobicity (making the skin more water-resistant)). The stratum corneum's cells are replaced about every month. 
    -Dermis: The layer of skin directly under the epidermis. It is between 15 to 40 times the thickness of the epidermis. The epidermis rests on top of the dermis, so the latter acts as structural support for the former. Blood vessels (arteries (pump oxygen/nutrient-rich blood away from the heart to organs), veins (pump oxygen/nutrient-poor blood back to the heart), and capillaries (areas of transition that transfer blood from arteries to organs and from organs to veins)), nerve endings (which allow for nocioception (sense of pain), thermoception (sense of temperature), sense of pressure, etc. by the skin), muscles, etc. are embedded in the skin in the dermis. 
    -Subcutaneous tissue: The layer of tissue under the dermis comprised chiefly of adipose cells, which provide insulation for the body and thus aid in the regulation of internal body temperature. They also act as shock absorbers when the body experiences pressure. Its thickness varies significantly throughout the body, but it is thinnest in the eyelids (non-existent, in fact) and thickest in the soles of the feet. 
-Bollinger bands: An analytical tool, invented by John Bollinger, used to measure the volatility of a financial asset. They consist of a pair of trading bands, which are graphical curves that follow the graph of an asset's price against time, with one band generally above and one below the asset's curve. The asset's graph, of course, is highly volatile and jumps around rapidly (corresponding, mathematically, to a continuous but highly un-differentiable function), and the bands follow the graph's general trend and provide a relative measure of upper and lower bounds for the trading range. Bollinger bands are calculated by first computing the moving average of the asset's price over time and then adding and subtracting (for upper and lower bands, respectively) some number of standard deviations from the moving average. It should be noted that a simple moving average (the arithmetic mean of the values at a certain number of time periods, measuring backwards from the present) is not always necessarily used; rather an exponential moving average (a moving average that places more weight on more recent data) is may be used. The bands expand and contract as the asset becomes more or less volatile, respectively. When the asset curve consistently (ie for a reasonable number of time periods) touches the upper band, the asset it thought to be overvalued and will soon decline; the converse is true when the asset consistently touches the lower band, and it's predicted that the asset is undervalued and will soon increase in price. 
    -Composition: The computation of Bollinger bands consists of  an N-period (exponential) moving average (ie MA), an upper band that's K standard deviations above the moving average, and a lower band that's K standard deviations below the MA. Typical values used are N = 20, K = 1 or 2. 
-Strangle: A financial strategy in manipuLating options by placing both a call and a put option, both with the sane maturity, on the same underlying asset. The strategy is only effective if the price of the asset moves outside the range (call prediction - put prediction), because whatever direction it moves, the call or put option in the opposite direction expires as worthless (causing a loss of money to the trader), but the put or call option that aligned with the actual direction of movement for the asset's price will have gained a much larger amount of money, enough to offset the cost of both the call and put contracts.
    -Option: A contract which gives the buyer the right, but not the obligation, to buy or sell a pre-determined asset at a pre-determined and immutable price by a pre-determined date. The seller is contractually obligated to respect the buyer's choice should the buyer choose to exercise the option, and the seller must prioritize the buyer above all other parties that are interested in the asset. For this right, the buyer pays a certain premium to the seller for this contract to be put in place. A call option is one that allows the buyer to buy an asset at a previously specified price; a put option s one that allows the buyer to sell an asset at a previously specified price. The aforementioned specified price at which the trader is guaranteed to be able to sell or buy at is called the strike price.
    -Example: Say a trader believes that a certain stock, currently priced at $50, will vacillate significantly in the near future, but it is uncertain in which direction the price will move (ie up or down). The trader can buy a call option that costs $300 (say, $3 per option for 100 shares) and with strike price of $55, as well as a put option that costs $285 (say, $2.85 per option for 100 shares) and with strike price $45. If the price ends up moving outside the range, say dropping to $35, the trader makes money. The call option expires as worthless and the put option makes the trader $1000 (since the trader sells 100 shares at $45 even though they are valued at $35, for a profit of (45 - 35)(100) = 1000). The trader's costs are the combined costs of the put and call options, which is $585 ($300 for the call and $285 for the put). Thus the trader makes a profit of $415 ($1000 - $585). 
-Zipf's law: (***INCOMPLETE***)
-Continuum hypothesis: The first problem on Hilbert's list of 23 problems for the century. The problem has been shown (by Paul Cohen in 1963) to be independent of standard (Zermelo-Fraenkel, with the axiom of choice) set theory, meaning that it cannot be proven as true or false. Some mathematicians have proposed adding the hypothesis as an axiom to the standard foundations of mathematics, but the issue is hotly debated. The problem states: There is no set whose cardinality is strictly between that of the natural numbers and that of the real numbers (ie there is no cardinal number between aleph-null and aleph-one).
-Dan Zanger: The current (as of 2015) world record holder for largest percentage increase in a personal stock portfolio in a year. Between June 1998 and December 1999, he turned $10,775 into $18 million, and in around two years (the next six months), he accrued $42 million, a 29,000% change. He has no college education or formal training and is entirely autodidactic. 
-Anatomy of the eye: The human eye consists of several parts that work together. The entire eyeball is covered externally by the sclera, a white collagen-filled opaque membrane that protects the eye. In the front is the cornea, a transparent bulge at the front of the eye that helps focus the light. Underneath it is the pupil, the "hole" in the eye through which light enters, and surrounding it is the iris, an annular structure that controls the diameter of the pupil; the color of the iris is controlled by the pigment melanin, the same pigment that controls skin color (people with lighter color irises tend to be more sensitive to light). Between the cornea and the pupil is a pocket called the anterior chamber, which is filled with a clear watery fluid called aqueous humor, which bathes and nourishes the eye. The front of the eye, by the cornea, is covered with a thin, transparent protective mucous membrane called the conjunctiva. Behind the pupil, inside the eye itself, is the crystalline lens, which further refracts the light in order to focus it onto a point; it is surrounded by the ciliary muscle, which causes the lens to distort, hence allowing the eye to focus on different objects at varying lengths. The inner surface of the eye is covered with light sensitive cells that make up the retina; between the inner retinal surface and the outer sclera is a middle layer of cells called the choroid. The interior of the eye is called the vitreous body, since it is filled with vitreous fluid, a clear gel. On the retina, towards the center of the back of the eye, is the macula, a region in which the cells are pigmented slightly brown and in which light that falls is seen with the largest amount of visual clarity and acuity. Under the eyebrows and on top of the eye is the lacrimal gland, which produces tears, and the reddish triangular regions on the extreme sides of the eye (nearest the nose) are called the lacrimal puncta, a small circular opening that allows tears to drain. 
    -Orbital bones: There are seven orbital bones that make up the eye socket, or orbit. The frontal bone is on top of the eye, under the eyebrow, and directly underneath it, making up the bottom of the orbit, is the maxillary bone. On the outer side of the socket (furthest from the nose) is the zygomatic bone, and nearest the nose is a smaller bone directly opposite the zygomatic called the lacrimal bone. The frontal, maxillary, and zygomatic bones are the largest. In the back of the eye socket, behind the eye, is a large circular bone called the sphenoid, and sandwiched between the sphenoid and lacrimal bone is the ethmoid bone, a medium sized bone, smaller than the sphenoid but bigger than the lacrimal bone. Right in between and underneath the ethmoid and sphenoid bones is a tiny bone called the paLatine bone.
    -Extraocular muscles: The six muscles around the eye that control movement of the eyeball within its socket. Weakened eye muscles can cause the eyeball to face away from an object instead of directly at it. Some examples of misalignment are exotropia (the eye turns outward) or esotropia (the eye turns inward).
        -Medial rectus: (ie MR) A muscle located on the side of the eyeball closest to the nose that moves the eye inward, toward the nose.
        -Lateral rectus: (ie LR) A muscle located on the side of the eyeball furthest from the nose that moves the eye outward, away from the nose.
        -Superior rectus: A muscle located directly on top of the eyeball that moves the eyeball upward.
        -Inferior rectus: A muscle located directly underneath the eyeball that moves the eyeball downward.
        -Superior oblique: A muscle on top of the eyeball attached to the eye perpendicular to iris and pupil instead of pointing in the same direction as the light that enters the pupil does, which all of the muscles above do. This muscle rotates the top of the eye towards the nose.
        -Inferior oblique: A muscle under the eyeball attached perpendicular to the pupil and iris; it rotates the top of the eye away from the nose.
____________________________________________________________________________________________________
                    HUMAN IMMUNOLOGY
-Written around March 21, 2015 (date finished).
1. Overview
-Foreword: This section on the human immune system is by no means a comprehensive synopsis; immunology is an incredibly complex and vast field of study still not completely understood. There are an excruciating amount of details and rules, every rule has an exception, and the system constantly evolves over time. This section will provide an extremely basic glance at the immune system that gives one a shallow yet decent understanding of the field.
    -The immune system is a highly interconnected system; it's a team effort. No player involved performs isolated duties; every component relies on and is relied on by other components of the system.
-Basic summary:
    -Physical barriers: The first line of defense against invading pathogens are the physical bulwarks that prevent microbes from ever reaching the inside of the body. The skin and mucous membranes (inside digestive, respiratory, and reproductive tracts) work to prevent pathogens from entering vulnerable parts of the body.
    -Innate immune system: If an invader breaches the physical barriers, the second line of defense is activated. The innate immune system is named thusly because it is common to all animals, and is believed to have evolved about 500 million years ago with the first jaw-boned fish. It's a very general defense mechanism that fights pathogens without identifying them.
        -A common sign of the innate immune system's activity is inflammation, which is characteristic of the rush of white blood cells rushing to the point of microbial entry. A very important and perhaps the most famous white blood cell involved in the innate immune system is the macrophage, which destroys bacteria. Macrophages devour bacteria, and they don't wait blindly
         in hopes that they'll come into contact with bacteria; they are covered with tiny antennae on their surface that recognize certain molecules given off by common microbial invaders. It then moves towards the source of the danger molecules. 
            -Macrophage process (Phagocytosis): When a macrophage picks up foreign molecules with its antennae, it crawls towards the microbe. It then engulfs the microbes in a phagosome (a pouch or vesicle used to transport the microbes), which then fuses with another vesicle called a lysosome that contains power enzymes and chemicals that rapidly destroy the microbes (the contents of the lysosome are so destructive that they would kill the macrophage themselves if they weren't contained in the vesicle). 
                -Phagocytosis is actually an advanced version of the technique used by the amoebas (that first evolved 2.5 billion years ago) to ingest their sustenance. 
                -Etymology: The prefix "macro-" is used in reference to the macrophage's size, since these are huge cells (25 micrometers across), and the suffix "-phage" comes from Greek and means "to eat".
                -Relation to inflammation: When macrophages consume bacteria, they also release chemicals that increase blood flow to the wound, which we perceive as redness. Blood vessels also contract so that fluid from the capillaries can flow into the wound, which causes the swelling. These responses also stimulate pain signals to be sent to the brain. All this is done to create an optimal environment for the innate immune system to destroy the invaders. The signals that are sent belong to a class of signaling molecules called cytokines.
                -Additional functions: Macrophages also function as garbage collectors, since they can eat basically anything. They are used to clean up cell debris and dead cells and microbes from the bloodstream.
        -Stem cells: All cells that inhabit the blood (termed blood cells) are born in the bone marrow. The bone marrow produces stem cells, which are immature undifferentiated cells with the potential to mature into any kind of blood cell. Stem cells divide into two more cells; one of the cells becomes a particular blood cell, while the other remains a stem cell that repeats the process.
            -Stem cells must churn out 2 million new red blood cells (ie Erythocyte) per second to replenish those lost in our body due to normal wear and tear. 
            -Otherwise, stem cells can differentiate into macrophages, neutrophils, or other white blood cells. Macrophages are actually in a more immature form called monocytes when they're patrolling the bloodstream; it's only when they migrate to a particular tissue or organ where they're needed that they mature into macrophages. At any given time, there are approximately 2 billion monocytes circuLating throughout the blood. Monocytes patrol the circulatory system for an average of 3 days, after which they travel to capillaries and enter tissue, when they mature into macrophages. They remain there indefinitely.
    -Adaptive immune system: A highly specialized third line of defense that only vertebrates (which make up less than 1% of all species) have. This system adapts to protect the body from any invader. The adaptive immune system commonly fights viruses, which the innate immune system handles poorly. 
        -Discovery: In 1796, a smallpox outbreak was killing and disfiguring hundreds of thousands of people. A biologist named Edward Jenner observed that milkmaids (who collected milk from cows) that had contracted cowpox from cows never seem to get infected with smallpox. They had sores on their hands similar to those caused by smallpox, and yet were unaffected by the smallpox outbreak. Cowpox and smallpox are closely related viruses, but cowpox is far less dangerous. Jenner collected pus from the sores in the milkmaids' hands (which contained the cowpox virus) and injected it into a young boy named James Phipps. Phipps was then injected with smallpox pus, but he didn't get sick from the smallpox at all. The experiment showed that if the immune system is given time to prepare for an infection by reacting to but surviving from exposure to the pathogen, it would be ready the next time it was infected and quickly exterminate the infection. This was the basis for vaccination (the word "vaccine" comes from the Latin word for cow - "vacca"). 
        -Antibodies: Immunity to diseases is conferred by small proteins that circulate throughout the bloodstream called antibodies. Antibodies are produced in reaction to some external stimulus that warns the adaptive immune system of an invader, called an antigen. 
            -Etymology of B cells: The "B" in B cells from from the fact that the cells were first discovered in the  bursa, a body part in chickens.
            -Structure: Antibodies are "Y"-shaped. The bottom part of the "Y" is composed of two "heavy chains", and the arms of the "Y" are each composed of two "light chains"; the chains are held together with disulfide bridges (S-S bonds). The arms are known as the "Fab region", and is where the antigen actually binds. The chains are proteins; polypeptides, specifically. The arms are known as the Fc region, and they bind to Fc receptors on cell surfaces (usually macrophages). 
            -Classification: There are 5 types of antibodies, denoted with an "Ig" and a letter - A, D, E, G, or M. The "Ig" is short for "immunoglobulin", which is a general class of proteins that antibodies fall under. IgG is the most common and makes up 75% of all antibodies in the blood. All antibodies are produced by B cells, a type of white blood cell that matures into plasma cells that pump out antibodies. The tail of the antibody determines what class it falls under, and the composition of the arms determines what pathogens the antibody will bind to (antibodies in the same class have identical tail structure).
            -Antibody diversity: It is estimated that about 100 million antibodies total are required to reliably protect against any possible invader, which means that the heavy chain and the light chain structures should each be encoded by 10,000 genes each (to give a total of 100 million combinations), but human cells only have 25,000 genes in total. Thus, if a B cell were to allocate 20,000 genes for an antibody, most of its genetic information would be  used up simply to construct antibodies, leaving not enough for itself. To make up for this, B cells use four different kinds of DNA molecules to construct DNA sequences. They are named V, D, J, and C, and each molecule comes in a different varieties - V has about 40 different versions, D has about 25, J has 6, and C about 10. The four molecules are mixed and matched over and over. In addition, each time gene segments are spliced together, DNA bases are added or deleted in between. The combination of regular genetic diversity, gene segment diversity, and junctional diversity allow for well over 100 million possibilities using a relatively small amount of genetic material. Each possible B cell codes for a different antibody. 
        -Clonal selection: There are about 3 billion B cells in the blood stream at any given time, and since there are about 100 million possible pathogens that the body needs to have antibodies to detect for, there are about 30 B cells for a particular virus. This is a rather small amount, but each B cell manufactures antibodies and anchors them on their cell membrane pores in areas known as B cell receptors (ie BCRs); there are about 100,000 such pores on any B cell. The point of these B cells is to constantly "fish" for invaders. When a pathogen enters the bloodstream, eventually one of the appropriate B cells will pick it up. When this happens, the B cell begins to rapidly divide. B cell mitosis takes about 12 hours and the entire division process lasts about a week, after which there are about 20,000 B cells circuLating the body, which is enough to mount a defense against the specific pathogen in question. The group of 20,000 identical B cells is called a "clone", and the rapid division process is called "proliferation". Each of the B cells in the clone begin producing antibodies. The average B cell pumps out 2,000 antibodies per second, and these antibodies circulate the bloodstream. B cells are only produced "on demand" when they are needed.
            -Antibody function: The antibodies that are released into the bloodstream usually don't actually kill pathogen cells (though they do in some situations). Antibodies instead simply identify invaders and tag them for destruction, and other immune system cells destroy the pathogen. Antibodies do this by binding to molecules on the surfaces of the pathogen cells, a process called "opsonization". When antibodies opsonize an invader, they bind the invader in their Fab regions, and allow macrophages and other immune system cells to bind to the Fc regions on their tails. When the invaders happen to be viruses, the antibodies bind to the viruses in such a way that the virus is unable to enter any cells and infect them. However, only some antibodies are able to do this (called neutralizing antibodies). 
        -T cells: Although many of the players in the immune system discussed above destroy pathogens that have entered the body, the problem of destroying infected cells and preventing the infection from spreading still exists. Killer T cells evolved to destroy cells that have been infected with bacteria or viruses. There are about 300 billion T cells in the human body at any given time, and structurally T cells are very similar to B cells (they can't even be told apart under a microscope). Like B cells, T cells have T Cell Receptors (ie TCRs) that anchor antibody molecules on the surface (which are, as in B cells, produced using several mechanisms to guarantee genetic diversity), and, also like B cells, when a T cell picks up a pathogen that matches the antibodies on its TCRs, it rapidly proliferates by dividing for about a week. Unlike B cells, T cells cannot recognize any freely floating antigen; they require that the antigens be specially presented in a specific way (discussed below) to recognize them. 
            -T Cells etymology: T cells, though created from stem cells in bone marrow like all blood cells, mature in the thymus (an small organ located behind the breastbone (is sternum)), hence the "T" in T cells.
            -Types of T cells: There are three kinds of T cells: killer T cells (A.K.A. cytotoxic T lymphocytes (ie CTLs)), helper T cells (also called "Th cells"), and regulator T cells. Killer T cells destroy infected cells by communicating with them and triggering them to commit suicide (apoptosis), which causes the pathogens inside to die as well. Helper T cells (ie Th cells) secrete cytokines (signaling molecules) that direct the actions of other immune system cells. Regulatory T cells shut the immune response down after the invasion is over, and keep the system from overreacting.
            -Antigen presentation: T cells can only recognize antigens if they were presented a certain way. T cells can only recognize antigens if they are bound to a certain class of proteins produced in the body, known as major histocompatibility proteins (ie MHCs) 
                -Types of MHC molecules: There are two classes of MHC molecules: MHC I and MHC II. MHC I molecules are found on the surfaces of most cells, and when they pick up antigens, T cells are able to recognize them. This is why whenever a cell is infected with anything, it breaks down some of the infected pathogens and displays the parts of the pathogen on external MHC I molecules so that T cells can see them and respond by killing the infected cell. MHC II molecules also function as "billboards" in the same way MHC I molecules, but they are intended for helper T cells (A.K.A. Th cells) instead of killer T cells. Only certain cells of the body make MHC II molecules, and this class of cells is called antigen presenting cells (ie APCs). When helper T cells see the MHC II proteins bound to antigens, they react by activating the adaptive immune system. 
                -MHC Structure: MHC I molecules are made up of a long chain (called the heavy chain) and a short chain (called beta-2-microglobulin), while MHC II molecules are made up of two long chains (the alpha and beta chains). 
        -Activation procedures: The adaptive immune system is extremely potent and so must be activated before it strikes. There are a few different avenues of activation. The first is through helper T cells; when a helper T cell recognizes its particular cognate antigen displayed on an MHC II molecule (on an antigen presenting cell), the T cell binds to the MHC-antigen complex, linking it to the activated APC that's presenting the antigen. However, in order for the helper T cell to be activated, it must also bind to a certain protein on the surface of the APC in conjunction. This protein can be any in a class of proteins (and is commonly B7). The MHC-antigen bond is the specific key to the immune response since it depends specifically on the particular pathogen that has invaded, and the protein that binds to the helper T cell is non-specific (ie it binds to all helper T cells); this two key system prevents false positives. Upon the two-key binding taking place, the helper T cell rapidly proliferates into a large clone of identical helper T cells that mature and begin producing cytokines that direct the activities of the rest of the adaptive immune system.      
    -Secondary lymphoid organs: The blood carries with it a fluid known as lymph, which is similar to blood plasma and interstitial fluid, and contains white blood cells; lymph is also the fluid that circulates throughout lymphatic vessels. When lymph leaks out of blood vessels through capillaries into tissues (so the white blood cells can reach all parts of the body), the lymphatic system drains the lymph into the lymphatic vessels, where the lymph is transported through a series of one-way valves into the upper torso. The lymph from the left side of the body and the left side of the upper torso is collected in the thoracic duct and emptied into the left subclavian vein where it rejoins the blood to go through the cycle again; likewise, the lymph from the right side of the body empties into the right lymphatic duct and is then emptied into the right subclavian vein so it too can rejoin the blood. 
        -Lymph nodes: As the lymph makes its journey through the lymphatic vessels, it must pass through a series of lymph nodes. There are thousands of lymph nodes in various sizes. Lymph nodes solve the problem of probability: if there are only about 10,000 helper T cells with TCRs that match a given pathogen, it is unlikely that one of these helper T cells will bump into the pathogen by accident in the body. Instead, pathogens are swept into the lymph and through thousands of lymph nodes, where B cells and helper T cells are circuLating and can recognize the pathogen. It is thus nearly impossible for a pathogen to escape detection; B cells and T cells are constantly circuLating throughout the lymph nodes, and it is inevitable that if the pathogen is in the bloodstream, it will eventually be in the lymph and so eventually will reach a lymph node, where a B cell of helper T cell will detect it. In addition, whenever APCs pick up antigens, they travel directly to a lymph node to present their cargo and raise the alarm for infection. 
-Immunological memory: After an infection has been fought off, most of the B cells and T cells in the clone die off to prevent unnecessary use of resources, but a few of them become memory cells, which survive for long periods of time (decades) in case the body is invaded by the same pathogen once again; in case of subsequent infections, the pathogen can now be eradicated extremely efficiently since the body already knows what to do. This gives the individual immunity, meaning even in subsequent invasions by the same pathogen, the infection will be destroyed before any symptoms show up.
-Self tolerance: Cells with antibodies should not be able to pick up any molecules that are native to the body, as this would cause an unnecessary alarm to be raised and may even lead to the immune system attacking the body. During the creation process for T cells and B cells, any cells with the capabilities to recognize self molecules are killed before they can mature, guaranteeing that the immune system will not attack itself. 

2. Innate Immune System
-The innate immune system is a non-specific second line of defense against pathogens that acts extremely quickly. Bacteria multiply in number very quickly (a single bacterium dividing every 30 minutes would reach 100 trillion bacteria in a day, or 100 liters in volume, whereas the human body only contains 5 liters of blood; the result would be catastrophic), so the innate immune system acts quickly to stamp out any infection. The system contains several different parts, outlined below.
-Complement system: A system of about 20 different proteins that work together to destroy invaders and signal the rest of the innate immune system that an infection is present. The system is believed to have evolved over 700 million years ago. The complement system is activated in three different ways, after which it begins to function.
    -Modes of activation: 
        -Classical pathway: This pathway depends on antibodies, and is discussed in another section.
        -Alternate pathway: The alternate pathway is more commonly used than and evolved before the classical pathway; the two are so named due to the order of discovery. The complement proteins are produced in very high quantity by the liver, and circulate the bloodstream at high concentrations; the most prevalent protein is the C3 molecule, whose purpose is to break into smaller sub-molecules that have important functions (specifically, it decomposes into C3a and C3b). This pathway acts very quickly, due to the high concentration of complement proteins in the blood. Additionally, the complement system does not discriminate; it attacks all cells except those that are protected (ie the body's own cells), so any external cells at all that aren't natively protected, including transplant tissue cells (this presents a major problem with transplants), are attacked by the complement system.
            -C3b molecule: One sub-molecule of C3 is C3b, which is very reactive (it can bind to amino (a functional group that contains a basic nitrogen atom (with a lone pair of electrons) bound to other groups) and hydroxyl (OH) groups, which are very commonly found on the surfaces of invaders since their outer membranes tend to be made up of proteins and carbohydrates). Upon breaking off C3, if the C3b molecule does not bind to a hydroxyl or amine group (ie on pathogen surface) within 60 microseconds, it binds to a water molecule and is neutralized. Thus, a C3b molecule must be right up against the surface of a cell to bind to it. Once C3b does bind to the surface of a cell, another complement protein, the B molecule, binds to C3b to form C3bB. Complement protein D then binds to C3bB, but decomposes immediately, taking with it a portion of the B molecule that's bound to C3b, producing C3bBb (also called convertase). The C3bBb molecule is now in its final state, and, being still bound to the surface of a bacterium cell, can begin to wreak havoc on the cell. This C3bBb molecule initiates a positive feedback mechanism by breaking even more C3 molecule into C3b; the C3bBb molecule does this extremely efficiently, and so the first initial C3bBb molecule acts as a spark and sets of a round of exponential growth in the number of C3bBb molecules that all bind to the surface of the target bacterium. When enough C3bBb molecules are bound to the cell surface, they begin picking up passing C5 molecules in large numbers, which bind to C3bBb and are decomposed chemically, producing C5a and C5b. The C5b molecule combines with other complement proteins (C6, C7, C8, C9) to produce a molecule called a "membrane attack complex" (ie MAC), which destroys the bacterium. 
                -Membrane attack complex: The C5b, C6, C7, and C8 molecules bind in such a way to form a rigid and hollow stock-like structure, and C9 proteins then join in inside the stalk to rip open a hole in the bacterium cell wall; the combined structure forms a hole in the bacterium that's connected to a channel, which induces hydrolysis (the process in which water from around the cell rushes into the cell and causes the cell to burst).
                -It should be noted that this pathway affects not only bacteria but also most invaders, including parasites and some viruses.
                -Self tolerance: The complement system described above does not affect human cells because other complement proteins function to prevent this from happening. When certain proteins in the blood bind to C3b, they render it ineffective, and human cell membranes contain the enzyme MCP, which accelerates this neutralization process. In addition, human cell membranes contain a protein called Delay Acceleration Factor (ie DAF), which accelerates the destruction of C3bBb by other proteins in the blood. In addition, if an MAC does attach to a human cell, another blood protein called CD59 (A.K.A. protectin) destroys the bond between MAC and the human cell membrane, kicking it off.
        -Lectin activation pathway: Like the alternate pathway, this pathway is antibody-independent. It relies mainly on a protein manufactured in the liver called Mannose-binding Lectin (ie MBL), which belongs to a class of proteins known a a lectin (a protein able to bind to a carbohydrate molecule); MBL binds to mannose, a carbohydrate molecule commonly found on the surface of many pathogens. MBL binds to many pathogens, including yeasts (eg Candida albicans), some viruses (eg HIV-1, influenza A), many bacteria (eg Salmonella, Streptococcus), and some parasites (eg Leishmania); however, MBL does not bind to carbohydrates found on healthy human cells. The pathway works when MBL in the bloodstream bonds to a protein called MASP, so when the MBL finds and binds to a carbohydrate (usually mannose) on the surface of a pathogen, the MASP protein functions just as a convertase does, clipping C3 proteins in the blood to make C3b molecules; the sheer abundance of C3 in the bloodstream means this process happens very efficiently, and when the C3b proteins bind to the target pathogen's surface, the alternate pathway is initiated. Thus, the lectin activated pathway is a more controlled, directed, and aimed firing of the alternate pathway, whereas the spontaneous activation of the alternate pathway itself is random and volatile.
    -Additional functions: 
        -Antibody mimicry: Some C3b molecules, upon binding to a pathogen's surface, are clipped by a serum protein found in the blood; this produces a decomposition reaction that breaks the C3b protein down slightly, producing iC3b (the "i" stands for "inactive", since iC3b is unable to make MACs). The iC3b opsonizes the target pathogen, labeling it for destruction by phagocytosis when a macrophage or other phagocyte or granulocyte picks it up; all phagocytes have receptors that bind to iC3b and induce phagocytosis. In this way, C3b can act as a pseudo-antibody for opsonization. 
        -Chemoattraction: Fragments of complement proteins serve as effective chemoattractants (chemicals that attract other substances and induce reactions) by recruiting other immune system players to the site of infection. When C3 and C5 are clipped as described above, the C3b and C5b proteins function as described above, whereas the C3a and C5a molecules circulate throughout the tissue and bloodstream, attracting macrophages and neutrophils, as well as activating the cells to make them more enhanced killers that fight for longer periods of time before dying. 
            -Anaphylaxis: C3a and C5a fall under a class of substances called anaphylatoxins, since they can contribute to anaphylactic shock (discussed below). 
        -Teamwork with macrophages: The complement system and macrophages together form a positive feedback loop. Many complement protein fragments, such as C3 fragments, can bind to receptors on the surface of a macrophage and provide a signal that induces hyperactivation. Hyperactivated macrophages respond by producing several additional complement proteins, such as C3, B, and D (all of which are involved in the C3 pathway discussed above), which is helpful since although LPS can hyperactivate macrophages easily, bacteria without the LPS protein do exist. Macrophages also release histamine-like chemicals that expand blood vessels as well as making them more permeable, which allows more complement proteins to be released into the infected tissue. 
-Phagocytes: Phagocytes are cells that consume and digest pathogens as a means of combating them. The two most important classes of phagocytes are macrophages and neutrophils, though eosinophils and basophils (two kinds of granulocytes (cells which contain within them granules (ie vesicles or containers ) full of toxic chemicals and enzymes); eosinophils are a type of white blood cell that mainly combat multicellular parasites and certain infections; they are found in vertebrates; basophils are the least common white blood cell that contain histamine (a vasodilator that expands blood vessels, thereby improving blood flow to tissues) and heparin (an anticoagulant that prevents blood from clotting too quickly) and are involved in the inflammatory response, among other things) are also important.
    -Macrophages: Macrophages are huge cells (25 micrometers across) that are found mainly under the skin, where they provide immediate protection against pathogens that enter through breaches of the skin. They also exist in the lungs, where they combat inhaled pathogens. Some also circulate throughout the intestines, searching for pathogens. Macrophages act as sentinels for the innate immune system in all areas that are exposed directly to the outside world. 
        -Stages: Macrophages exist in one of three main states, from a simple garbage collecting organism, to an antigen presenting cell to a vicious killing cell.
            -Resting state: In this state, macrophages lie in wait of an infection and slowly proliferate. Their main function during this state is to act as garbage collectors and clean the body of cell debris due to dying cells (about 1 million cells die per second throughout the human body); dying cells give off messenger molecules that macrophages pick up and respond to by moving towards and consuming the dead cell. 
            -Primed state: When a macrophage is activated by signaling molecules that indicate a breach of the body's defenses or imminent intruders,  it begins to consume much more material with each "gulp" and up-regulates the expression of its MHC II molecules on its surface. Macrophages rapidly search for invaders to consume, and after their internal enzymes break the consumed pathogens down, the macrophage displays the pathogenic fragments on its surface as antigens to prime additional cells. 
                -Priming molecules: Many different signals and molecules can prime a resting macrophage, but the most common is a cytokine (a common intercellular communication molecule) called interferon gamma (as opposed to interferon alpha or interferon beta, three molecules belonging to the interferon class of molecules). 
            -Hyperactivated state: Macrophages are pushed into a state of hyperactivation if they receive a direct signal from an invader, which is communicated only be certain molecules that are given off directly from pathogens (eg lipopolysaccharide (ie LPS), which is a molecule that is found in the outer cell walls of Gram-negative (Gram-positive bacteria are those that test positive on the Gram stain test (a test for categorizing bacteria into two classes based on the presence of peptidoglycan (A.K.A. murein; a polymer made up of sugars and amino acids that forms a mesh-like layer outside the plasma membrane of more bacteria) in their outer cell walls); bacteria that test positive glow crystal violet under a microscope, due to the peptidoglycan absorbing the stain; in general, Gram-positive bacteria are more resilient to death and physical disruption than their Gram-negative counterparts, and so are relatively unaffected by broad spectrum antibiotics and require a more specialized approach) bacteria, such as Escherichia coli). When such molecules bind to the surface receptors of a macrophage, the macrophage knows for certain that an infection is present, and so a state of high alert is triggered. The macrophage stop proliferating and focuses on locating and destroying pathogens. They also grow larger and increase the rate of phagocytosis. In this state, macrophages grow so large that they can even ingest pathogens as large as unicellular parasites. Hyperactivated macrophages also secrete Tumor Necrosis Factor (ie TNF), a cytokine that kills tumor cells and virus-infected cells as well as helps to activate the rest of the immune system. The number of lysosomes inside a hyperactivated macrophage is also elevated, as is the amount of destructive molecules such as hydrogen peroxide. Even when a hyperactivated macrophage encounters an invader that's too large to consume (such as multiceullar parasites), the macrophage can dump the contents of its lysosomes directly onto the pathogen, effectively destroying the invader. 
    -Neutrophils: Considered by some to be the most important member of the phagocytes, the neutrophils reinforce the macrophages when an invasion is so strong that it threatens to overpower the initial response. There are about 20 billion neutrophils circuLating throughout the bloodstream. Neutrophils are the "foot soldiers" of the innate immune system, and their function is simply to efficiently kill pathogens. Neutrophils do not present antigens like macrophages do, and focus only on destroying invaders. Neutrophils are highly energetic and aggressive, and are incredibly phagocytes; any engulfed organism is very unlikely to survive the battery of deadly chemicals inside. In addition, neutrophils dump pre-stored destructive chemicals into the surrounding blood and tissue, and although this kills blood and tissue cells in the process, the resulting "toxic soup" is extremely lethal to invaders. This is why neutrophils are programmed to die in about five days (after which they commit suicide (ie apoptosis)), since otherwise they would instigate unwanted destruction, and if additional neutrophils are required then they can always be retrieved from the blood anyways (there is no short supply; bone marrow churns out 5 to 10 billion neutrophils per day during infection. They also produce cytokines (eg TNF) that alert the rest of the immune system.
        -Exiting the bloodstream: The mechanism by which neutrophils are transported from their birthplace in the bone marrow to the correct region of infection is important, since if neutrophils were released in a location without pathogens, they would simply kill tissue cells. The neutrophils rush towards the infection site through the bloodstream at a very fast rate of 1000 microns per second. The surface of the endothelial cells that line blood vessels is lined with proteins called Intercellular Adhesion Molecules (ie ICAM) that do not bind with the selectin ligand (ie SLIG) molecules expressed on the surface of neutrophil cells, but when a region is infected macrophages give off interleukin-1 and TNF, which causes the corresponding region in the blood vessels closest to the infection site to express a new protein, selectin (ie SEL), on its surface that does bind with the SLIG molecules on the neutrophil surface, and the neutrophils interpret this signal as a sign to disembark from the bloodstream. From here, chemoattractants such as C5a and formyl methionine peptides (ie f-mets; fragments of bacterial proteins; they are typically expunged into the bloodstream by macrophages that consume bacteria) lead the way for the neutrophil to arrive at the infection site.
            -Complexity: The reason that the system for stopping speeding neutrophils and allowing them to enter tissue is so complicated is to provide a high degree of failsafes; since so many different molecules are required for the neutrophil to exit the bloodstream, it is nearly impossible for a neutrophil to enter healthy tissue and kill it (even if some endothelial tissues began expressing the wrong protein by mistake).
-Natural killer cells: (A.K.A. NK cell) Like all blood cells, NK cells are descended from stem cells from the bone marrow, and although they are lymphocytes (like B and T cells), they do not contain an extremely diverse array of surface receptors like the rest of the lymphocytes do. They are also short lived, with a half-life of about a week. NK cells are like neutrophils in the sense that their only purpose is to destroy pathogens. They circulate the bloodstream, with some existing in the spleen and liver, and zip through the bloodstream and exit at the point of infection in the same way (with different signaling molecules) that neutrophils do. Once at the infection site, NK cells rapidly proliferate until they reach large numbers, after which they perform two functions: (1) upon receiving signals from other immune system cells, NK cells produce additional cytokines to aid the battle, and (2) NK cells destroy tumor cells, cells infected with viruses, bacteria, parasites, and fungi. The mechanism by which NK cells kill is by inducing the target the commit suicide; they bind to the target and "inject" perforin proteins (eg granzyme B) that communicate to the cell to commit suicide, or in some cases a surface protein on the NK cell called the Fas ligand signals the target cell to self-destruct. NK cells have two types of receptors on the surface: activating receptors and inhibitory receptors. When the NK cell binds to another cell, if the target cell contains certain proteins in the MHC I class (and all healthy human cells do contain these proteins by default), the NK cell recognizes this as an inhibitory molecule, and does not attack. When a cell is infected or is cancerous, however, it produces unusual carbohydrates and proteins (non-MHC I proteins) on its surface, and the NK cell recognizes these as activating molecules, and begins to attack the target cell. This mechanism is very helpful; some viruses have evolved the defense mechanism of preventing the cells it infects from displaying MHC I molecules, making them invisible to killer T cells. In this situation, the NK cells will still identify and destroy the virus-infected cell. NK cells can be activated by many different signaling molecules, such as LPS found in bacterial cell walls or the interferon-alpha and interferon-beta proteins given off by cells that are under attack. NK cells can thus not only kill infected and cancerous cells, but also produce cytokines (usually large amounts of interferon-gamma, which hyperactivates macrophages) that attract other immune system cells to infection sites, making NK cells a loose combination of helper T cells and killer T cells. NK cells and macrophages create a kind of positive feedback loop; NK cells produce interferon-gamma, which hyperactivates macrophages, and the hyperactivated macrophages produce TNF and interleukin-12, both of which increase the amount of interferon-gamma that NK cells produce. In this manner, all macrophages in the area can respond quickly to a NK cell's call. Macrophages also stimulate NK cell proliferation; NK cells naturally produce large quantities of interleukin-2, which acts as a growth factor for NK cells. However, NK cells lack receptors for picking up the interleukin-2. The TNF produced by macrophages up-regulates the expression of interleukin-2 receptors on the surface of NK cells, allowing them to respond to the interleukin-2 they're naturally producing, which leads to rapid proliferation. 

3. B Cells and Antibodies
-Throughout the history of life, microbes have constantly evolved novel defenses and strategies to overpower animal bodies, and as a consequence, animalian immune systems have been evolving new defenses. About 200 million years ago, certain fish evolved an entirely new system that worked in conjunction to the existing immune system. The new system, which we now call the adaptive immune system, is, in theory, so adaptable that it can protect against any invader. B cells are an important component of the adaptive immune system. Like all blood cells, B cells are created in the bone marrow; about 1 billion B cells are produced each day. 
-B cell receptor: A B cell receptor (ie BCR) consists of a heavy chain (ie Hc) and a light chain (Lc), which are long and short peptides, respectively. Through a randomized mechanism, different combinations of BCRs are made; B cells that fail to make a valid combination that codes for a BCR commit suicide, since B cells without receptors are useless. Every mature B cell produces only one kind of BCR and antibody which are both characterized by a unique Hc and Lc. The BCRs on different B cells are so diverse that it is suspected that, collectively, animal B cells can recognize every organic molecule that could exist.
    -Signaling mechanism: The unique antigen that a B cell's BCRs respond to are known as that B cell's "cognate" antigen; the region of the antigen that the antigen physically binds to is called the antigen's "epitope".  The BCR's light chains are exposed externally so that antigens can bond to them, and the heavy chains are embedded a few amino acids in length deep in the cell membrane. The inside of the cell membrane (the side facing the inside of the cell) is also lined with accessory proteins called Immunoglobulin-alpha (ie Ig-alpha) and Immunoglobulin-beta (ie Ig-beta); these typically protrude from the cell membrane next to the heavy chains of a BCR. When a large antigen (which has multiple epitopes if the same epitope (typically a sequence of amino acids) is repeated are multiple times)) binds to several BCRs at once, those BCRs are said to be "crosslinked"; BCRs can also be crosslinked when several BCRs bind to several individual antigens (as opposed to the same one) that are close together on the surface of the invader, and one last way for crosslinking to happen is if multiple BCRs bind to antigens that are clumped together. The tails of the Ig-alpha and Ig-beta molecules interact with enzymes in the cell and has the potential to set off an enzymatic chain reaction, but this can only happen if enough Ig-alpha and Ig-beta interact with the internal enzymes together. When many BCRs are crosslinked, enough Ig-alpha and Ig-beta molecules are brought together to initiate the enzymatic chain reaction, which terminates in a "BCR engaged" signal at the nucleus of the cell. The nucleus then switches on certain genes that lead to the B cell switching to its activated state (in which it produces antibodies).
        -Complement receptor: In addition to BCRs, there are certain other receptors on B cells that bind not to antigens but to complement protein fragments that are bound to an invader (for opsonization). When the complement receptor binds with the opsonized antigen or complement protein fragment, it is brought together with a BCR (there are over 100,000 BCRs on the surface, so any complement receptor is always in close proximity to a BCR) in such a way that the signals sent by the BCR to the nucleus (through Ig-alpha and Ig-beta) are amplified. For this reason, complement receptors are also known as the BCR's "co-receptors". The combination of BCRs and their co-receptors makes B cells extremely sensitive to antigens that the innate immune system has determined dangerous.
    -Activation: "Naive" or "virgin" B cells are those that have never encountered their cognate antigen before, and the vast majority of B cells fall into this category; B cells that have seen their cognate antigen before are called "experienced". The activation mechanism differs for virgin B cells and experienced B cells. In virgin B cells, the crosslinking of B cell receptors is not enough to fully activate the B cell; a second signal, known as the "co-stimulatory" signal, is required in conjunction (this provides a failsafe that reduces the risk of autoimmune disease). There are two ways the co-stimulatory signal is acquired.
        -T cell-dependent activation: In this pathway, helper T cells provide the co-stimulatory signal. Activated helper T cells have a protein called CD40L on their surface, and B cells have a protein called Cs40 on their surface; when an activated helper T cell physically contacts a virgin B cell and the two proteins bind, the co-stimulatory signal is sent. If the B cell's receptors are crosslinked, the B cell is activated. 
        -T cell-independent activation: If a huge number of BCRs are crosslinked (perhaps if the antigen is a large carbohydrate molecule (which are characterized by many repeating units of carbon, hydrogen, and oxygen, which means many epitopes)), the B cell can become partially activated and begin proliferating, but in order to fully activate, it must receive the co-stimulatory signal. If the B cell picks up an unambiguous danger signal that provides an unequivocal indication that invasion has occurred, then this can also act as the co-stimulatory signal. These danger signals can be any molecular patterns that are highly characteristic of bacteria, parasites, etc. and are called "pattern recognition receptors" (discussed in detail in another section). The primary reason for the existence of the T cell-independent activation pathway is to allow the appropriate B cells to respond to an invasion as quickly as possible if there is one, but another reason is that helper T cells can only recognize protein antigens (specifically, peptides in the MHC II class), so if the T cell-dependent pathway were the only pathway, the entire adaptive immune system would depend on proteins, which would make it very easy for a pathogen to evolve a defense, and anyways many invaders have several different carbohydrates and fats completely alien to human cells, making them excellent targets for a co-stimulatory signal.
-Maturation: Once B cells are activated and have proliferated to a sufficient number, they mature into new B cells fully capable to joining the fight against the invasion. There are roughly three stages to maturation, though B cells may go through them in various orders and may skip stages entirely: class switching (in which B cells change the class of antibody they produce), somatic hypermutation (in which the B cell's genes that code for its BCRs undergo mutation as the B cell divides multiple times), and "Memory vs Plasma cell" phase (in which the cell either becomes a memory cell or a plasma cell).
    -Class switching: Upon being activated, the B cell produces IgM antibodies, since those are its default class of antibodies. A B cell has the opportunity to, at this maturation stage, make a different kind of antibody: either IgG, IgE, or IgA. The gene segments that code for IgM also contain constant regions of DNA that code for IgG, IgE, and IgA, so if a B cell were to splice out the DNA that codes for IgM and splice in another DNA segment that codes for a different class of antibody, that B cell will end up producing a different kind of antibody. There are specialized signals that induce the B cell to switch its antibody type, located between constant DNA regions on chromosome 14 (the chromosome that codes for B cells).
        -Types of antibodies: There are five types of antibodies, each of which is built to perform a particular duty very well. IgD antibodies are also produced but compose a tiny fraction of all circuLating antibodies; in addition, it is currently uncertain if they provide any significant function in the immune system. For this reason, they are excluded below.
            -IgM antibodies: These are the most primitive of the antibodies (they were the first to evolve), and this is why they are the default antibody produced. It was mentioned in section 1 that an IgG antibody is "Y" shaped, with its Fab region (where the antibody binds) located in the arms on the light chains and the Fc region located on the heavy chains. An IgM antibody is essentially five IgG antibodies arranged together pentagonally, with light chains meeting in the center point (so the 10 branches that face outward consist of 5 light branches and 5 heavy branches, alternating). IgM antibodies are very efficient at activating the complement cascade, and activate the above-mentioned classical pathway.
                -Classical pathway (for activating the complement system): In the bloodstream, several complement proteins bind together to form a large complex called C1. However, C1 cannot activate the complement cascade itself because it's bound to an inhibitor molecule preventing it from doing so. If two C1 molecules are brought together, however, the inhibitor molecules fall off in the resulting chemical reaction, which then allowed the two C1 molecules to initiate the complement cascade (which begins by producing a C3 convertase by splitting C3 into C3a and C3b). When the Fab regions of the IgM antibody bind to an invader, two C1 complexes can bind to the Fc regions of that antibody, and the IgM antibody's structure means that the two C1 complexes are now close enough to initiate the complement cascade. This system means that bacteria which have evolved cell walls that don't allow complement proteins to bind to them are still affected by the complement system, since IgM simply has to bind to it (no bacteria exists to which antibodies cannot bind). Notice that the classical pathway, unlike the alternate pathway, is highly specific to invaders. IgM antibodies are so large that once they bind to a pathogen, the complement cascade is initiated very quickly (since it's so likely that two C1 complexes will bump into an IgM molecule, given the IgM molecule's size); this makes IgM the ideal default antibody to be released first. IgM antibodies also bind to viruses, preventing them from infecting cells.
            -IgG antibodies: The IgG class of antibodies is further categorized into subclasses of IgG antibodies, each of which is characterized by a slightly (structurally) different Fc (heavy chain) region and also therefore different functions (eg IgG3 excels at fixing complement (a term synonymous with "initiating the complement cascade") whereas IgG1 is optimized for binding to invaders' cell membranes and opsonizing them). Macrophages and neutrophils bind most easily with IgG1 antibodies, but NK cells bind best to the Fc region of IgG3 antibodies; IgG3 antibodies form a bridge between a target cell (to which IgG3 has binded using its Fab region) and an NK cell (which binds its Fc receptor to the Fc region of IgG3). This bridge both brings the NK cell closer to the target cell and stimulates the NK cell to kill more effectively through a process known as "antibody-dependent cellular cytotoxicity" (ie ADCC). IgG antibodies are also skilled at combating viruses, just as IgM antibodies, but unlike IgM, IgG can be passed from a pregnant mother to a fetus through the placenta, which provides the newborn baby with a source of immunological protection until it can begin making its own (typically several months after birth). This is probably because IgG antibodies live the longest (half life of about 3 weeks). 
                -Etymology: The "G" in Immunoglobulin G stands for "gamma".
            -IgA antibodies: Although IgG is the most abundant antibody in the blood, IgA is the most abundant throughout the entire body, overall. This is because IgA antibodies' primary job is to secure the mucosal surfaces inside the body (eg esophagus, nose, etc. for a total of 400 square meters of mucus). About 80% of the B cells located underneath mucosal surfaces produce IgA antibodies. IgA's structure is similar to two IgG antibodies bound together in their heavy chain regions by a "molecular clip", resulting in a structure that looks like (horizontally) two "Y"s rotated in opposite directions and joined together at the stem. This unique structure makes IgA resistant to many acids and enzymes found in the digestive tract. The molecular clipping mentioned before is also what the immune system looks for when admitting IgA antibodies through the intestinal walls and out of the intestine (other antibodies and cells are not permitted access). Inside the intestine, IgA coats pathogens by binding to their surface, preventing the pathogens from binding to the intestinal walls and hence preventing infection of the intestine. The IgA antibody has 4 Fab regions to bind to, and so are optimal at clumping several pathogens together in large groups that can then be swept out of the body with waste (urine or feces (expelled bacteria compose about 30% of human defecation)) or mucus (through sneezing, spitting, swallowing into stomach acid, etc.). Although IgG is passed from mother to fetus, IgA is passed from mother to infant through breast milk. 
                -Relationship with complement system: Despite their prowess at defending mucosal surfaces, IgA antibodies are unable to bind with C1 complexes, meaning that they cannot fix complement (ie initiate the complement cascade) at all. The reason for this is that if IgA could fix complement, the mucosal surfaces in our body (located in the reproductive and digestive tracts, among other places) would be in a state of almost constant inflammation, since mucosal surfaces are highly exposed to the outside world and thus encounter pathogens with very high frequency, though these are quickly and easily expelled from the body. 
            -IgE antibodies: IgE is most famous for being responsible for anaphylactic shock, which occurs when mast cells de-granulate. As with macrophages, mast cells live very long and are positioned underneath the body's exposed surfaces (eg skin, etc.). Mast cells' most important function (secondary functions include phagocytosing opsonized bacteria and giving off cytokines to recruit neutrophils) is to protect against infection by parasites that breach the first line of defense. Mast cells, upon encountering a parasite, de-granulate their contents (dump their chemical cargo), which include a variety of dangerous chemicals but mainly histamine and heparin. In addition to killing the parasite, mast cells' de-granulation also causes an allergic reaction due to the destructiveness of the chemicals involved. In extreme cases, this leads to anaphylactic shock. Although allergic reactions are dangerous, IgE is extremely effective at combating parasites and is thus invaluable to the immune system.
                -History of anaphylactic shock: French doctor Charles Richet decided to study the lethality of the Portuguese man-of-war's (a jellyfish-like siphonophore with venomous tentacles) toxin (which is injected upon stinging), and so he injected dogs with the toxin. He successfully determined the lethal dose for dogs, but a few dogs survived the first time, so Richet injected these dogs once more. He expected the dogs to have gained immunity to the toxin, but instead they all died, even the dogs that had been administered doses well below the lethal dose. Richet coined this effect "anaphylaxis", as an antonym to "prophylaxis" (the prevention of disease). Richet eventually won the Nobel Prize in 1913 for his work. 
                -Connection with allergic reactions: Some people, upon first exposure to an allergen (any foreign substance capable of inducing an allergic reaction), make a large quantity of IgE antibodies against the allergen, which the IgE perceives as an invader (though in many cases it's harmless). Mast cells have recetors that bind to the Fc region of IgE antibodies, so when the allergen is encountered again, the high concentration of IgE antibodies cause many mast cells to de-granulate simultaneously, leading to an allergic reaction. The runny nose and watery eyes are due to the released histamine, which expands blood vessels and increases capillary permeability, which causes fluids to leak into tissue. In severe cases, the saturation of blood with extraneous fluid reduces blood volume to such an extent that the heart can no longer pump blood effectively, resulting in cardiac arrest. The histamine can also cause the windpipe to contract, making breathing difficult and, in severe cases, leading to suffocation. 
        -Triggers: Since each antibody class excels at a particular function, antibody class switching is controlled by cytokines that communicate to B cells in their maturation stage to switch from one class to another. This way, the correct antibodies are produced when they are most needed (eg IgG antibodies for an infection, IgA antibodies for a cold, and IgE antibodies for parasitic infection). For example, high concentrations of interleukin-4 and interleukin-5 cause B cells to switch their class from IgM to IgE, since IL-4 and IL-5 are produced when worms and other parasites are present; Interferon-gamma, on the other hand, causes B cells to switch to IgG antibodies (specifically, they produce a large amount of IgG3), which is effective against bacteria and viruses; if a large amount of a certain cytokine called TGF-beta were present, B cells would switch from IgM to IgA. The correct cytokines that are appropriate for the pathogen in question are produced by helper T cells. 
    -Somatic hypermutation: The baseline rate of mutation in the mitosis process is about 1 in 100 million bases, but after class switching occurs, some very specific and restricted areas of B cell chromosomes that code for the V, D, and J gene segments discussed above have their mutation rate boosted to incredibly high rates in the neighborhood of 1 in 1000 DNA bases per generation. After an antibody gene has been selected through the usual randomized diversification mechanisms, the somatic hypermutation mutates the part of the antibody genes that code for the antigen binding region of the antibody. There are three possible outcomes: the antibodies produced by the B cell have an increased ability to detect and bind to antigen, an unchanged ability, or decreased ability, and these mutations occur very frequently due to the somatic hypermutation. However, B cells are designed in such a way that during maturation, they must be continually re-stimulated by binding to the cognate antigen, otherwise they are judged useless by the body and die (by committing suicide, the same way that lymphocytes that recognize self molecules do). Thus, the B cells with higher affinity for antigens proliferate more, and those with lowered or unchanged affinity die out. This results in B cells with BCRs that are highly sensitive to the antigens present on the invaders; this process is known as affinity maturation. The end result is B cells that are very well adapted to the current invaders. 
        -Activation: Somatic hypermutation (as well as class switching) is activated and overseen by helper T cells, so B cells that are activated without the aid of helper T cells skip both these stages during their maturation.
    -Memory vs plasma cells: B cells mature into one of two types of B cells: plasma cells and memory cells. Plasma cells are far more prevalent, and function as antibody factories. They produce extremely large quantities of antibodies and secrete them into the bloodstream and at the infection site. If a B cell is going to mature into a plasma cell, it travels to the spleen or back to the bone marrow, where it begins secreting antibodies at rates as high as 2,000 antibodies per second. This high rate is what enables the immune system to keep up with the extremely rapid exponential growth of bacteria. Plasma cells only survive a few days. Memory cells instead "remember" the infection and which antibodies to produce in order to combat it, and live for an extremely long time (decades), and so if the same infection is encountered again, the individual is immune since the immune system can stamp the infection out so quickly that symptoms don't even show up. It is unknown how a B cell "chooses" to be a plasma cell or a memory cell, but it is known that the co-stimulatory molecule CD40L (present on the surfaces of helper T cells) and the molecule CD40 (located on B cell surfaces) are both involved in generating the memory of memory cells. Thus, memory cells can only be produced with the help of helper T cells, which must first activate the B cells. 
        
4. Antigen Presentation
-When a pathogen breaches the body's defenses, its antigens are what sound the alarm and rouse the immune system in the first place. APCs (antigen presenting cells) isolate tiny fragments unique to the pathogen's structure so that other immune system cells can recognize the antigen and spring into action.
-Logic of antigen presentation: There are several very good reasons for the antigen presenting system to work the way it does. The reason that T cells cannot recognize antigens themselves and instead must identify antigen-MHC complexes is that this prevents T cells from wasting time attacking free floating pathogens that can easily be dealt with through antibodies and phagocytes; T cells are needed to focus on infected cells that are unreachable to phagocytes and antibodies and which do present antigens on MHC proteins. In addition, if T cells were able to recognize antigens floating around, it is likely that they would attack any healthy cells with the misfortune of having pathogen debris stuck on their membrane by mistake. The large amount of variation in the types of MHC proteins exists because if it didn't then a single pathogen could evolve molecules undetectable to the one MHC class and thus wipe out the human population quickly; variation protects against extinction by creative pathogens. 
-Class I MHC molecules: MHC molecules are the molecules that bind to the antigen and allow T cells to recognize them (most lymphocytes cannot recognize free-floating antigens and require that the antigen be bound to an MHC molecule, which is produced by the body and confirms the infection). MHC I molecules have a structure somewhat akin to a hot dog; they have a binding "groove" where small protein fragments (peptides) can bind, enclosed at both ends. 
    -Encoding genes: There are three genes involved in the encoding of MHC I proteins: HLA-A, HLA-B, and HLA-C that are all located on chromosome 6. Therefore, humans have a total of six MHC I genes (since we have two chromosome 6's - one from the mother and one from the father). Each HLA protein pairs up with another protein called beta-2-microglobulin, and these pairs compose the complete MHC I molecule. However, MHC I proteins can vary greatly from person to person, since there are over 480 variants of HLA-A, over 800 different variants for HLA-B, and 260 different versions of HLA-C; each variant differs by only a few amino acids, and have roughly the same shape. The variability of MHC I molecules means that throughout the body different MHC I molecules "prefer" different kinds of peptides. 
    -Antigen presentation: MHC I molecules act as "billboards" that display antigens on the surface of a cell. They present fragments of proteins that are manufactured by the cell (such proteins are called "endogenous" proteins). 
        -Endogenous proteins: Endogenous proteins are meant to act as a "sampling" of everything that's going on inside a cell; ordinary cellular proteins such as enzymes and structural proteins, as well as proteins encoded by viruses that have infected the cell, are all endogenous proteins. Since everything that's going on inside the cell is sampled and advertised on MHC I molecules on the cell's surface, if a pathogen infects a cell then it too will be sampled and displayed by external MHC I molecules. Killer T cells investigate the MHC I molecules of every cell they see, and if they see anything out of the ordinary they raise the alarm and, if the cell is infected, kill the cell. MHC I molecules are also replaced regularly to update the "internal state of the cell".  
            -Selection: The proteins that are selected to be endogenous proteins are those that are defective. The process is as follows. When the mRNA encoding proteins is translated in the cell's cytoplasm, occasionally transcription or structural mistakes are made. Additionally, some proteins wear down over time normally due to wear and tear. Such defective proteins are picked up by a destructive "wood chipper" like enzyme called proteasomes, which cut the proteins up into their constituent peptides (in most cells this decomposition process is fairly random and haphazard (it doesn't matter how the proteins are destroyed), but in APCs that specialize in antigen presentation (such as macrophages), the decomposition process is specifically geared to produce a good proportion of display-viable peptides). Most of these broken down peptides are further decomposed into individual amino acids so they can be recycled, but some are carried away by transporter proteins (TAP1 and TAP2) across the endoplasmic reticulum's (ie ER) membrane to prepare for transport to the cell membrane, where they will be displayed by MHC I molecules as endogenous proteins. The MHC I molecules bind to the "chosen" peptides within the ER and are then taken to the cell membrane in small vacuoles. Note that only peptides with the correct length (about 9 amino acids) and structure can be chosen. 
-Class II MHC molecules: MHC II molecules are encoded by the HLA-D gene on chromosome 6 and, like MHC I molecules, are highly polymorphic (ie the genes that encode them come in a large and diverse array of variations). MHC II Molecules are structured differently from MHC I molecules in that they lack an enclosing structure; they're open at both ends. This allows them to bind to longer peptides in the range of 13 - 25 amino acids (whereas MHC I molecules bind to peptides in the range of 6 - 11 amino acids). 
    -Antigen presentation: MHC I molecules are designed to present antigens to killer T cells, but MHC II molecules are designed to do so to helper T cells. MHC II molecules are also, unlike MHC I molecules (which exist in almost every type of cell), exclusively present only on immune system cells. This is because while MHC I molecules display what's going on inside the cell, MHC II molecules display what's going on outside the cell. 
        -Process: MHC II molecules are made up of two proteins: alpha and beta chains. Both chains are produced in the cytoplasm of the cell and injected into the ER, where they bind to a third protein known as the invariant chain. The reason for the invariant chain's existence is to prevent the MHC II molecule from picking up any peptides in the ER (since the molecule is supposed to pick up peptides from outside the cell, not inside). Another function of the invariant chain is to guide MHC II molecules out of the ER, through the Golgi Stack, and into cytoplasm vesicles known as endosomes. What goes on inside endosomes is not well understood, but it is thought that proteins from outside the cell are trapped in phagosomes and brought into the cell, where the phagosomes merge with the endosomes so the MHC II molecules bind to the peptides. Enzymes in the endosome also destroy the invariant chain on the MHC II molecules, allowing the molecules to bind to peptides again, and enzymes in the phagosome break down the exogenous proteins into peptides for the MHC II molecules to bind to.
        -Cross presentation: When an MHC molecule accidentally displays peptides from outside its class (ie MHC I molecules displaying exogenous peptides or MHC II molecules displaying endogenous peptides).
-Antigen presenting cells: Killer T cells and Helper T cells must be activated before they carry out their functions, but picking up their cognate antigens from MHC molecules is not enough; they need a second co-stimulatory signal. Only professional APCs can provide MHC I molecules, MHC II molecules, and co-stimulatory signals (the three ingredients necessary to activate T cells). The primary purpose of APCs is to activate T cells, so it makes sense that they are the only cells that have the potential to single-handedly activate them. Other cells that have MHC molecules but are missing the co-stimulatory signal can raise the alarm for the immune system and alert T cells but not fully activate them. 
    -Co-stimulation: Most co-stimulatory pathways involve a protein called B7, which is on the surface of an APC and which binds with a protein called CS28 found on the surface of a T cell. Thus, the T cell must not only be binding with the antigen that's bound to an MHC molecule, but must also have its CD28 molecules binding with the APC's B7 protein. 
    -Types of APCs: There are three main types of APCs: activated dendritic cells, activated macrophages, and activated B cells. All three are white blood cells that are born in the bone marrow and then migrate to various parts of the body, and all three must be activated before they can begin presenting antigens. In the beginning of an invasion, the dendritic cells are the most important APCs since they rouse the immune system (specifically, virgin B and T cells); during the battle macrophages are most important because they "refuel" T cells as they kill pathogens, and B cells are primarily important if the infection is encountered ever again, when the experienced B cells and memory cells left over from the attack rapidly activate the immune system and eradicate the invader.
        -Activated dendritic cells: The function of dendritic cells was unknown for a long time, causing them to have a reputation as obscure, and they were considered a mere biological curiosity. However, today it is clear that dendritic cells are the most important APCs. They are normally located in the spaces between the epithelial cells that make up the skin, and act as sentinel cells capable of absorbing four times their volume of extracellular fluid (which is then spit out), so they are able to scan the fluid for pathogens. In their resting state, dendritic cells don't have a very large number of MHC molecules, making them poor cells for antigen presentation. In the even of an infection, however, the dendritic cell is activated. 
            -Activation: There are two signals that can activate a dendritic cell. The first type comes from other immune system cells (that are already engaged in battle) or from dying cells (that are dying due to pathogens); both types of cells give off various cytokines (eg macrophages and neutrophils give off TNF (tumor necrosis factor)). The second signal comes from external molecules. Dendritic cells contain receptors on their surface that recognize broad molecular patterns that are commonly found across large classes of invaders. These receptors are called "pattern recognition receptors" (the most famous pattern recognition receptors are the Toll-like receptors (ie TLRs; these were mentioned above)) (one such receptor, TLR4, is responsible for picking up LPS (lipopolysaccharide, the compound found in bacterial cell walls). 
                -Toll-like receptors: There are two major characteristics of TLRs. The first is that unlike most other receptors, any TLR receptor identifies a general class of invaders, rather than a single pathogen. Second, TLRs recognize molecules that represent structural features that are vital to the pathogen's survival and structure. Molecules vital to structure and survival will not be easily altered by mutation (since the majority of mutations of a characteristic that's vital to the organism will be fatal or detrimental), which means that it is very difficult for pathogens to evolve by natural selection in such a way as to avoid detection by TLRs. 
            -Traveling dendritic cells: When battle cytokines (or chemicals released by dying cells, or both) activate a dendritic cell the dendritic cells alter their behavior. It remains in the infection site at the tissue for about six hours, swallowing and collecting antigens. It then travels through the lymphatic vessels to the nearest lymph node. This trip takes about a day, and on the way the dendritic cell begins binding the collected antigens to a reserve stockpile of MHC II molecules and displaying the complexes on its surface. It also increases production of MHC I molecules, in case it's been infected (so then T cells and NK cells will recognize and kill it before it reaches the lymph node). It also increases production of B7 co-stimulatory proteins. The high levels of MHC I and MHC II molecules and the rapid production of B7 proteins are everything the dendritic cell needs to activate virgin T cells once it reaches the lymph node. 
                -Lifetime: Dendritic cells only live about a week; the short lifespan ensures that dendritic cells provide up-to-date information on the battle. After a dendritic cell has been activated it begins producing chemokines (a type of cytokine), which prompt monocytes (immature macrophages that circulate the bloodstream) to leave the blood, enter the tissues, and mature into dendritic cells; in this way, although dendritic cells have a short lifespan, they recruit their own replacements. 
        -Activated macrophages: Like dendritic cells, macrophages are poor antigen presenters until they are activated. In their resting state, they act mainly as garbage collectors. When battle cytokines such as Interferon-gamma (or when their pattern-recognition receptors (eg Toll-like receptors) are ligated by invaders) are picked up, they are activated  and begin collecting and presenting antigens very effectively. However, dendritic cells travel to lymph nodes and do not kill, whereas macrophages do not travel anywhere else in the body once they are activated, but do become ferocious killers. They also act as "refueling stations" for T cells; T cells must be continuously re-stimulated in order to keep on fighting (this is a defense mechanism that ensure the rapid death of T cells after the infection passes so as not to waste resources), and activated macrophages provide the continuous re-stimulation. 
        -Activated B cells: Like the other main classes of APCs, virgin B cells are poor at antigen presentation and must be activated to reach their potential. Virgin B cells have low levels of MHC II molecules and little to no B7; both of these levels increase drastically upon activation. The main advantage that B cells have over other APCs is their ability to concentrate antigen for presentation: once a B cell's BCRs bind to an antigen, the BCR-antigen complex is transported into the cell where it is loaded onto MHC II molecules and re-displayed on the cell's surface. Experienced B cells, however, have gone through somatic hypermutation and so have an extremely elevated affinity for antigens, giving them an estimated 100 to 10,000 fold advantage over other cells in T cell activation (since the T cells must have crosslinked receptors), making them very efficient at activating T cells even with very little antigen present (once a B cell's receptors have binded to an antigen, it takes less than 30 minutes to display it on MHC II molecules).
-Non-classical MHC Molecules: The MHC I and MHC II classes are the classical MHC molecules, but other types of MHC molecules do exist. The best studied non-classical MHC molecule is the CD1 protein class.  CD1 proteins are, unlike classical MHC molecules (which are adapted to binding to short peptides), adapted to binding to lipids. CD1 proteins give T cells a way of surveying the lipid composition of cells (the same way classical MHC proteins allow T cells to survey proteins). 

5. T Cell Activation
-This section will focus on how T cells are activated. Individual T and B cells have receptors specialized for recognizing a specific kind of pathogen, and due to the sheer number of pathogens, it is not economical to have large numbers of any given lymphocyte, since the overwhelming majority are unlikely to ever confront their target and exist just in case. Thus, the few T cells that do keep watch for their target pathogen must be activated upon discovering the intrusion so they can proliferate. This section will focus on traditional T cells (discussed below).
-T cell receptors: Molecules present on the surface of T cells that bind to passing molecules and respond to them depending on what molecule they are; they function as the "eyes" of the T cell and allow it to have a sense of its environment. There are two types of T cell receptors: alpha-beta and gamma-delta, with the former composed of an alpha and beta protein and the latter composed of a gamma and delta protein. Alpha, beta, gamma, and delta proteins are all assembled with genetic mechanisms designed to increase variation in the proteins and resulting in a larger number of types of each protein overall, similar to the way the heavy and light chains in B cell receptors are assembled (in fact, both Hc/Tc and Alpha/Beta/Gamma/Delta are assembled by mixing and matching gene segments, and the same proteins (RAG1 and RAG2) initiate the gene splicing process). 
    -Traditional T cells: These are T cells that have alpha-beta TCRs, and make up about 95% of all T cells. Traditional T cells express CD4 or CD8 co-receptors. Alpha-beta proteins are able to recognize complexes composed of a peptide (ie the antigen) and an MHC molecule (some receptors recognize MHC I molecules while others recognize MHC II molecules). Alpha-beta molecules are about as diverse as BCRs.
    -Non-traditional T cells: The T cells with gamma-delta TCRs are called non-traditional, since they are in a small minority of T cells. Unlike traditional T cells, non-traditional T cells do not have the CD4 or CD8 co-receptor molecules at all, and traditional T cells are therefore most commonly found in areas of the body such as the intestine, the uterus, the tongue, etc. (all areas with a high degree of exposure to the outside world) since non-traditional T cells do not recognize specific antigens and rather fight broad classes of pathogens due to their lack of co-receptors, and the areas where they are present require the rapid and general expulsion of all pathogens instead of a specialized response. In accordance with this principle, gamma-delta receptors are much less diverse than both alpha-beta receptors and BCRs. Not much is known about T cells with gamma-delta receptors, such as where they mature (traditional T cells do so in the thymus). Another type of non-traditional T cell is the NKT cell (natural killer T cell), a rare (about 1% of all T cells in the blood) type of T cell that is similar to NK (natural killer) cells. NKT cells do mature in the thymus and have alpha-beta receptors (unlike most other non-traditional T cells), but NKT cells lack diversity and recognize lipids that are presented by non-classical CD1 MHC molecules (in contrast to either MHC I or MHC II molecules). 
-Signaling process: After a TCR (ie T cell receptor) recognizes its cognate antigen, it must now send a signal from its location on the surface of the cell to the nucleus of the cell, where genetic information can be altered so as to induce proliferation and activation. However, although alpha-beta molecules are excellent ligands (molecules that bind to signaling molecules), they are presented on the external side of the cell membrane, and they only extend about 3 amino acids deep into the interior of the cell, which is far too short to signal. As such , TCRs must signal in a similar way the BCRs signal: by setting of a biochemical chain reaction that terminates in the nucleus being notified. This is why TCRs are connected with protein complexes called CD3, which are made up of four proteins (gamma, delta, epsilon, and zeta; the gamma and delta proteins are not the same as the gamma and delta proteins involved in non-traditional T cells) and are anchored in the cell membrane and have tails extending into the cytoplasm that are long enough to signal. As with BCRs, TCRs have two parts: one to expose to the outside world and gather intelligence about what's going on outside (in T cells, these are the alpha and beta proteins), and another part to signal the nucleus when something important is detected (in T cells, these are the gamma, delta, epsilon, and zeta proteins). Similar to BCRs, the signaling pathway is only initialized when many TCRs are clustered together by binding to the same antigen, after which the number of kinase enzymes (recruited by gamma, delta, epsilon, and zeta proteins) hits critical density and an activation signal is dispatched.
    -Variability: TCRs are not used just to activate cells; they can transmit several different kinds of signals to the nucleus that depend on factors such as triggering mechanism, duration, etc. They can be used to induce cell suicide (ie apoptosis), such as in the thymus when T cells are maturing, and those that happen to detect self-molecules are induced to commit suicide. T cells require co-stimulatory signals in addition to antigens to be activated, and T cells that detect antigens but no co-stimulatory signals are signaled not to function correctly (ie anergization) so as not to cause unnecessary harm, and the anergy signal is also sent through TCRs. 
    -CD4 and CD8 co-receptors: These receptors are there to allow CTLs to only recognize MHC I molecules and helper T cells to only recognize MHC II molecules (if CTLs recognized MHC II molecules, they would kill antigen-presenting cells, and if helper T cells recognized MHC I molecules, they would activate infected cells). CTLs only contain CD8 receptors while helper T cells contain CD4 receptors. Whenever a TCR binds to an antigen-MHC complex, CD4 or CD8 receptors bind to the entire complex (of TCR, antigen, and MHC molecule), strengthening the bond. Both CD4 and CD8 receptors have long tails extending into the cytoplasm that send signals to the nucleus upon binding (to the proper MHC molecule, depending on the type of T cell). CD4 signals send out very different types of signals from CD8 signals, owing to their structures (they are structured with different proteins). It should be noted that although CD3 molecules are "glued" to alpha-beta receptors, CD4 or CD8 proteins are only loosely bound to any of these molecules. 
        -Maturation: T cells are born with both CD4 and CD8 receptors, but during the maturation problem, one of the proteins is up-regulated while the other is down-regulated, causing the cell to mature into a killer or helper T cell. As with the process of B cells maturing into plasma or memory cells, it is unknown how this maturation process actually works.
-Co-stimulation: The signaling pathway between TCRs and the nucleus is intentionally inefficient, requiring an extremely large number of TCRs to bind to antigen for the signal to reliably get through to the nucleus. However, if co-stimulatory molecules are also present and binding to TCRs in conjunction with antigens, the signaling pathway is amplified up to 100 times in strength, prompting rapid activation. The reason for such mechanisms is so T cells are activated only after it is absolutely certain that an invader is present. So far, it seems that B7 proteins (specifically, B7-1 and B7-2) are the most effective co-stimulatory molecules; they are present on the surface of APCs and provide co-stimulation to T cells by binding to CD28 receptor molecules on the T cell surface. It should be noted that only virgin T cells require co-stimulation to be fully activated; experienced T cells have their signaling pathways strengthened so that they are activated immediately upon detecting antigen.
    -Reinforcement process: The process by which co-stimulation reinforces and amplifies the TCR signal is not completely understood, but theories do exist. The cell membrane is not a rigid boundary but rather a heterogeneous viscous plasma in which proteins, carbohydrates, and other molecules sluggishly float. Often, specific combinations of proteins and carbohydrates with propensities for binding bind together and float through the membrane as a large complex, and these complexes are called "rafts". These rafts store many signaling molecules associated with TCR signaling, but most are not on the cell surface and are instead stored in the membranes of other structures inside the cell. When a TCR binds to its cognate antigen (and other factors such as sufficient crosslinking are present), and appropriate co-stimulation occurs, the rafts are rushed to the surface and bind to TCRs, which puts them in close contact with many signaling molecules that initiate the chain reaction that terminates at the nucleus and results in a modification of genetic code that activates the T cell. So, the idea behind co-stimulation is that it recruits rafts from within the cell and pushes them to the cell surface. This is also why experienced cells require far less co-stimulation; they have many rafts already floating around in the cell membrane that were pushed there from the previous infection and didn't return to inside the cell.
-Helper T cell activation: Helper T cells can be activated directly by encountering antigens-MHC molecules or signals from other immune system cells, but another important way they're activated is in the lymph nodes, by dendritic cells. A single dendritic cell in a lymph node is checked around 1000 times per hour by helper T cells to see if it's displaying its cognate antigen. If a T cell encounters a dendritic cell in the lymph nodes that's presenting its cognate antigen, the helper T cell will begin the activation process (which takes between 4 and 10 hours). First, adhesion molecules on the surface of the dendritic cell bind to their partners on the surface of the helper T cell; this anchors the cells together for the rest of the process. Next, CD4 co-receptors on the helper T cell bind to the MHC II molecules on the surface of the dendritic cell; this strengthens the adhesion between the two cells (anchoring them together stronger) and also strengthens all interaction between the two cells. This adhesion is crucial, since the MHC-antigen-to-TCR bond is rather weak and so without proper adhesion it may not even happen. The clusters of adhesion molecules and TCRs on the surfaces of the dendritic cell (or any APC) and helper T cell in the same spot is called an "immunological synapse". The helper T cell's TCRs being engaged also up-regulates the expression of CD40L proteins on its surface; CD40L proteins bind with the larger CD40 proteins (found on the surface of the dendritic cell), initiating a sequence of events. The expression of MHC and co-stimulatory molecules (eg B7 proteins) is up-regulated. The life of the dendritic cell is prolonged. The up-regulation of CD40L in helper T cells is also important because CD40L is required to activate B cells. After the whole process, the two cells depart, and the dendritic cell (or, generally, most APCs) will go activate additional immune system cells (it is now even better at doing this than it was before, due to the first activation) while the helper T cell begins its job by rapidly proliferating. Growth factors such as interleukin-2, produced in large quantities by helper T cells, helps spark the proliferation (even native T cells produce interleukin-2, but they don't have receptors to pick it up whereas activated T cells do), so in this way helper T cells help other T cells to proliferate as well as stimuLating their own proliferation. 
-Killer T cell activation: The T cell activation process is not as well understood as the process of helper T cell activation, but it is believed that three cells are involved: the killer T cell itself (to clarify, the specific killer T cell with the correct receptors for the pathogen), an activated dendritic cell (which is presenting MHC I molecules), and an activated helper T cell. However, the activated T cell merely provides co-stimulation to the CTL, and in many cases the dendritic cell is able to provide not only the cognate antigen-MHC complex but also the proper co-stimulation to the naive CTL, activating it; in many cases, helper T cells are not needed. However, in these cases it has been observed that CTLs do not proliferate as much, do not kill as efficiently, and do not live as long as CTLs that have been activated by helper T cells. It appears that the mechanism for dendritic cells to completely activate CTLs on their own exists as an emergency alert system to activate CTLs quickly in the early stages of an infection, but for an efficient, sustainable, and long-term immune response, helper T cells are needed. Although it is unlikely that the correct CTL, an activated dendritic cell, and an activated helper T cell would all meet at once, this probability goes up significantly once the cells involved start proliferating. Additionally, several mechanisms exist to increase this probability; when activated helper T cells meet activated dendritic cells, they bind together for a few hours and also release chemokines that attract CTLs to them, increasing the probability of a meeting between all three cells required; it has also been theorized that helper T cells affect dendritic cells in such a way as to "authorize" them to activate CTLs the same way that helper T cells do, eliminating the need for a meeting between all three cells. Helper T cells also secrete large amounts of interleukin-2 as mentioned above, which not only stimulates their own proliferation but also that of CTLs. 

6. Effector T Cell Mechanisms
-Activated helper and killer T cells are called "effector cells", and they are now ready to join the immunological battle against the disease. CTLs primary purpose is to locate and kill infected cells (which are presenting antigens on MHC I molecules on their surface) while helper T cells have two main purposes: they can circulate throughout the circulatory and lymphatic systems (moving about in the blood and lymph) as they travel from lymph node to node and activate and stimulate B and T cells, or they can exit the blood and lymphatic vessels are infection sites to provide help to the immune system cells fighting the pathogens in the area. Th cells are like the quarterbacks of the immune system, coordinating all the other players in a unified and multi-pronged attack. They produce many different kinds of cytokines to communicate with other cells, such as TNF (ie tumor necrosis factor), IFN-gamma (ie interferon gamma), IL-4, IL-5, IL-6, IL-10, IL-17, and IL-21 (ie interleukin 4, 5, 6, 10, 17, 21 respectively). However, no one Th cell secretes all these different cytokines; in fact, Th cells can be sub-categorized by which subsets of cytokines they produce: the three categories identified as of the present are Th1, Th2, and Th17, although these classes are by no means rigid, and many T cells exist that produce a combination of cytokines that spans more than one class. 
-Role of dendritic cells: Dendritic cells act as the "coach" of the immune system, and coordinate processes such as how Th cells know which cytokines are appropriate for production in a given situation. The two main pieces of information required for determining which cytokines to use (1) which type of infection it is (viral, bacterial, or fungal) and (2) where in the body the infection is located (eg in a finger, in the respiratory tract, etc.). Since virgin T cells are unable to gather this information (they are busy circuLating throughout the lymphatic and circulatory systems), dendritic antigen presenting cells act as observers throughout the body and especially the infection site, and gather this pertinent information. Thus, dendritic cells do more than simply travel from lymph node to node and activate virgin T and B cells; they act as the "brains" of the immune system by processing the relevant information and formuLating a plan of attack against the pathogen. There are two main inputs that contribute to the aforementioned plan of attack.
    -Inputs: The first mode of information integration comes from the dendritic cell's pattern recognition receptors (eg Toll-like receptor 4 (ie TLR4), which detects LPS (lipopolysaccharide), which is found in the cell walls of Gram-negative bacteria; TLR2 primarily detects "signature" molecules found in common structures of Gram-positive bacteria; TLR3 recognizes double-stranded RNA that's produced by viruses during viral infections; TLR9 identifies the un-methylated (methylation is the process of adding a methyl group (CH_3; methyl is an alkyl (a saturated hydrocarbon) derived from methane (CH_4))) DNA di-nucleotide CpG, which is commonly found in bacterial DNA). Different types of APCs in different parts of the body display different sets of pattern recognition receptors that are specifically attuned to identify different common structural features in common invaders. All the information from various APCs is consolidated by dendritic cells and used to create a plan of attack. The second input is received not through pattern recognition receptors, but through cytokine receptors on the surface of dendritic cells. Different pathogens both produce and induce (in immune system cells) different kinds of cytokines that are then picked up by dendritic cells, and provide a lot of information about the infection. These cytokines also vary based on the location at which they were elicited, giving dendritic cells information about the infection's location as well. In general, the first input provides information that dendritic cells analyze to understand which immune system weapons to dispatch, and the second input tells dendritic cells where to dispatch them. 
    -Messaging mechanisms: Once the information about the pathogens has concentrated in the dendritic cells, directions for T cells' actions are dispatched to them in two main ways. First, co-stimulatory molecules, specific to the invader in question, are displayed on the surface of dendritic cells, and these molecules bind to receptor molecules on the surfaces of helper T cells, passing along the attack plan; there are many co-stimulatory molecules, the most effective and common of which is B7. Activated dendritic cells also produce specific cytokines based on the invader, and those cytokines also relay information to Th cells. 
-Types of helper T cells: When a particular infection invades the body, resident dendritic cells are alerted through their own pattern recognition receptors and also due to cytokines produced by the invaders and by other immune system cells (eg macrophages). These signals activate the dendritic cell and imprint it with the specific characteristics of a reporter cell that has observed this specific kind of infection (how this imprinting process happens is not well understood); the dendritic cell then performs a number of specific actions; the most common such action is traveling from lymph node to node, activating B and T cells in specific ways, in accordance with the "attack plan". Different types of T cells follow what are referred to as different "cytokine profiles".
    -Th1 cells: When a puncture wound takes place, inflammation cells give off cytokines that cause dendritic cells to travel from lymph node to node, producing interleukin-12 as well as presenting battle antigens to virgin T cells. IL-12 and those specific antigens, which respond to bacterial infection as well as to viral infection of tissue cells, instruct Th cells to produce classical Th1 cytokines, making it a Th1 cell. The classical Th1 cytokines are: TNF, IFN-gamma (interferon-gamma), and IL-2 (interleukin-2).
        -Classical Th1 cytokines: The three cytokines produced by Th1 cells, TNF, IFN-gamma, and IL-2, each have specific purposes. The TNF secreted by Th1 cells helps to activate macrophages. The IFN-gamma constantly re-activates macrophages (which exit the activation stage and back into the resting state rather quickly) as well as influencing B cells to class switch (so they produce IgG3 antibodies ideal for fighting bacteria and tissue-infecting viruses as well as for fixing complement (ie activating the complement protein system)). The IL-2 "recharges" NK cells, which tire out quickly and revert to their deactivated state; IL-2 also acts as a growth factor, stimuLating the proliferation of CTLs, NK cells, and more Th1 cells. 
    -Th2 cells: These cells are ideal for fighting against parasites and bacteria that have infected the digestive tracts. When news of an infection reaches a dendritic cell, it travels to nearby lymph nodes and activates T cells with receptors capable of recognizing parasitic or digestive bacterial antigens. This leads to Th2 cells that secrete the classical Th2 cytokines, which are more diverse than the Th1 cytokines but include most prominently IL-4, IL-5, and IL-13.
        -Classical Th2 cytokines: The most common Th2 cytokines produced to fight parasites or bacterial invaders in the digestive tracts are: IL-4, IL-5, and 
        IL-13. In most cytokine secretions, some kind of positive feedback loop is present to induce rapid growth of the immune system's fighting resources, and 
        IL-4 provides this in this instance; IL-4 stimulates the proliferation of more Th2 cells. It also acts as a growth factor for B cells and can influence them to class switch to IgE antibodies (which are specialized against parasites). It should be noted that IL-4 is not secreted by dendritic cells; the initial source of IL-4 is unknown as of now (though of course once Th2 cells have been activated they will produce more IL-4 themselves; only the initial round of IL-4 that begins the activation of Th2 cells is unknown in source). IL-5 induces B cells to class switch to IgA, which is very useful fighting invaders of any type that are located in the digestive tracts. IL-13 stimulates production of mucus in the intestines; this helps prevent more intestinal parasites or pathogenic digestive bacteria. Note that it would be wasteful and probably harmful if B cells were induced to class switch to both IgA and IgE, since one fights bacteria optimally and the other fights parasites, and it's unlikely that both will infect the digestive tract at the same time. Thus, Th2 cells must identify the invader and make the decision of which class to induce B cells to switch to; how this decision is made is unknown as of yet.
    -Th17 cells: These cells are the least understood of all helper T cells, but we do know that they play a critical role in battling fungus infections. When dendritic cells in a part of the body detect fungal infections or certain types of extracellular bacteria (bacteria which do not enter cells), they travel to the nearest lymph node to activate T cells that are capable of recognizing the antigens being presented. The dendritic cells do this by, among other things, producing TGF-beta (ie transforming growth factor beta) and IL-6, which, together with co-stimulatory molecules, cause Th cells to produce the classical Th17 cytokines, which include most prominently IL-17 (hence the name) and IL-21.
        -Classical Th17 cytokines: The main classical Th17 cytokines are include IL-17 and IL-21; IL-21 is a growth factor for Th17 cells and is thusly used to stimulate more Th17 cells in a positive feedback loop. IL-17 is the signature cytokine for Th17 cells, and its presence recruits massive amounts of neutrophils to the infection site. Neutrophils are particularly effective at killing pathogens that Th1 and Th2 cells do not combat well (which include fungi and some extracellular bacteria). Both IL-17 and IL-21 also have the secondary function of influencing B cells to class switch to antibodies capable of opsonizing fungi and bacteria and activating the complement system. 
    -Th0 cells: These are the helper T cells that, when first activated, remain classless and unbiased and still retain the ability to produce a wide and diverse array of cytokines. Dendritic cells are able to direct Th0 cells to the infection site's location, but they do not influence its behavior with respect to actually combating pathogens. When Th0 cells reach the battle site, the cytokine environment present is what influences them to commit to a particular cytokine profile.
-T cell commitment negative feedback loop: Once a particular cytokine profile has been chosen, it is optimal for all helper T cells to commit to that cytokine profile. Thus, as certain cytokines produced by the T cells of the correct cytokine profile induce proliferation in more helper T cells of the same cytokine profile, forming a positive feedback loop, those same cytokines actually decrease proliferation of helper T cells in other cytokine profiles. For example, the IFN-gamma produced by Th1 cells stimulates the proliferation of more Th1 cells (creating a positive feedback loop) while simultaneously decreasing the rate of proliferation in Th2 cells (creating a negative feedback loop); the IL-10 produced by Th2 cells has the same negative effect on the proliferation of Th1 cells. 
-Cytokine range: It should be noted that cytokines have very short range, meaning that they, on average, only travel very short distances before being captured by cellular receptors or being degraded. Thus, the commitment of all helper T cells to a particular cytokine profile is a purely local phenomenon, and it's not as though every Th cell in the body now only produces a certain subset of cytokines; far from it. The cytokine profile commitment is localized to within the infection site.
-Delayed type hypersensitivity: Experienced memory Th cells of the correct cytokine profile will recognize a pathogen that infects the body if it has already infected the body before, and will activate resident macrophages and recruit more phagocytes and neutrophils to the battle site. However, the process of recognition, activation, proliferation, and recruitment takes a few days, but once it happens, the pathogen is quickly eradicated. This phenomenon is known as delayed type hypersensitivity (DTH). 
-Cytotoxic T cell combat process: The bulk of this section so far has focused on helper T cells, but killer T cells also play an important role. Once a CTL is activated, it rapidly proliferates, building up its numbers. They then leave their stations or the lymph node they're in and travel, via the bloodstream, to the area of the body that invaders are located. They then exit the blood vessel and begin killing pathogen cells. Most CTL killing mechanisms require direct physical contact between the CTL and the target pathogen. There are two main methods of killing discussed below, and both end in forcing the target cell to commit suicide through apoptosis. Apoptosis is not the only way for a cell to die, however; the other main form of cell death is necrosis, in which autolysis (self-digestion by one's own enzymes) is induced by external factors, or other forms of trauma (eg burns, shocks, blunt force, etc.) due to external factors kill the cell. However, when cells die through necrosis, destructive enzymes and chemicals, normally safely enclosed within the now-dying cell, are released uncontrollably as the cell dies, damaging nearby healthy cells. In apoptosis, all the deadly chemicals and enzymes remain packaged in vesicles and are safely digested by macrophages in their resting state (ie "garbage collection"). 
    -Perforin-using method: One important weapon is the protein perforin, a close relative of the complement protein C9. As discussed in the sections above, the C9 complement "drills" holes in the cell membranes of pathogens; perforin behaves the same way. CTLs first attach to their target using adhesion molecules, and as these molecules hold the two cells close, the CTL expunges a mixture of perforin and an enzyme known as granzyme B onto the surface of the target cell. Perforin and granzyme B then enter the target cell's cytoplasm (there are two theories for how the entrance process occurs: (1) Perforin drills a hole in the cell membrane through which both molecules enter, or (2) vesicles on the target cell's membrane package and enclose the two molecules for digestion, but once the vesicle is inside the cell, the perforin drills holes in the vesicle from within and allows both molecules to escape). Once inside, granzyme B initiates an enzymatic chain reaction that results in the cell's committing suicide through apoptosis (this involves the destruction of the target cell's DNA by the target cell's own enzymes); the entire process for a CTL to completely kill a target cell takes about half an hour.
    -FasL-using method: Another way CTLs can kill is by using a protein on its surface called the Fas ligand (ie FasL). This ligand binds to Fas proteins on the surface of the target cell, and the binding of this ligand to the Fas proteins induces an enzymatic chain reaction that results in the target cell's apoptosis (cell suicide). 

7. Secondary Lymphoid Organs and Lymphocyte Trafficking
-This section will cover the "geography" of the immune system, ie where every phase and activity involved in an immune response takes place. The primary lymphoid organs are the bone marrow, where B and T cells are born (as undifferentiated stem cells that mature in the thymus (for T cells) of lymph nodes (for B cells; specifically, in the paracortex of a lymph node)), and the thymus, where T cells are "trained" and mature. This section will focus on the secondary lymphoid organs, which are: the lymph nodes, the spleen, and the mucosal-associated lymphoid tissue (ie MALT); these are the locations where the recognition phase (when the body understands which invader it is dealing with) of the immune response takes place. A very important function of the secondary lymphoid organs to constantly screen the body for pathogens; they are strategically placed for easy and rapid detection should pathogens enter through a breach in the skin, through the digestive tract (perhaps due to eating contaminated food), the respiratory tract if airborne pathogens were breathed in, or through the blood by a blood-borne pathogen. Moreover, upon detection of a pathogen, secondary lymphoid organs are located in the perfect place to launch a rapid immune response and mobilize the adaptive immune system (it is unclear how the mobilization process works (for one thing, the secondary lymphoid organ would have to identify the pathogen and communicate that information to the adaptive immune system), but it is suspected that the secondary lymphoid organs gather and then communicate information through the detection and use of different cytokine environments). 
-Lymphoid follicles: All secondary lymphoid organs have lymphoid follicles. When these follicles are first created, they are called "primary" lymphoid follicles and are simply loose networks of follicular dendritic cells (ie FDCs) that are embedded in various regions of the secondary lymphoid organs that happen to contain large amounts of B cells; a primary lymphoid follicle is nothing more than many densely concentrated B cells interspersed with FCDs. 
    -FDCs: Follicular dendritic cells are different from the antigen-presenting dendritic cells discussed above in many different ways. Whereas antigen-presenting dendritic cells are a type of white blood cell and are thus produced in the bone marrow (after which they migrate to their sentinel post underneath a part of the skin or body that's in frequent contact with the outside world), FDCs behave in the same way as any other regular body cells (eg a skin cell, liver cell, etc), and unlike antigen-presenting dendritic cells, they remain in the final stations almost from the very creation of the immune system itself, during pregnancy as the embryo develops (they are in their final location by the third trimester of gestation). FDCs also differ from regular DCs in function; dendritic cells display antigens to T cells (using MHC molecules), whereas FDCs display antigen to B cells. 
        -Antigen presentation: FDCs have surface receptors capable of binding to complement proteins, so in the beginning stages of an infection when the complement system kicks in, many complement proteins opsonize antigens, and many complement-antigen fragments make it, through the bloodstream and lymphatic vessels, to the secondary lymphoid organs. FDCs bind to the complement, and they do this in a way that concentrates all the antigen in one area on the surface, so when the antigen is presented to a B cell, the B cell's receptors are crosslinked. FDCs can also do this with antibodies, if they are present. This is why the lymphoid follicle consists of an FDC surrounded by a high concentration of B cells: so when the FDC picks up antigen fragments (either bound to complement proteins or to antibodies), it can quickly present it to the surrounding B cells. 
        -Germinal center: When B cells within a lymphoid follicle pick up antigen from an FDC, they rapidly proliferate to build up their numbers, turning the lymphoid follicle into a B cell factory and center for B cell development and maturation. An activated lymphoid follicle such as this one is known as a "germinating center". Germinating centers are incredibly efficient at churning out B cells; the number of B cells doubles every 6 hours. This region, within the lymphoid follicle, of rapidly proliferating B cells, is known as the "dark zone", since the region is so saturated with B cells that it looks dark under a microscope; by contrast, the rest of the lymphoid follicular area, which contains the FDCs, is known as the "light zone". In order to prevent the waste of resources, these B cells, which reproduce at a highly accelerated rate, are programmed to commit suicide very quickly upon being born (if they weren't, they would reproduce so fast (a single B cell would copy itself 270 million times in a week, and there are thousands of B cells per germinating center and dozens of germinating centers in the body, leading to billions of B cells per week) that even after an infection was eradicated, there's a good chance there would be an excessive number of B cells left behind and others still being produced). So long as an infection is still raging, helper T cells that were activated in the "T cell areas" of the secondary lymphoid organs travel from lymph node to lymph node and communicate to these B cells to not commit suicide. During the proliferation process, B cells undergo the normal B cell maturation process: they "choose" to become plasma or memory cells, and some undergo somatic hypermutation. They can also switch their antibody class.
            -This is one reason why complement proteins are so important; people with defective complement systems are unable, during an infection, to progress past the primary stage of the immune response, and thus the adaptive immune system is never activated, leaving the person vulnerable to a host of pathogens. 
-High endothelial venules: Another common anatomical feature shared by all secondary lymphoid organs is the high endothelial venule (ie HEV). HEVs are the gateways through which B and T cells arriving from the bloodstream can enter the secondary lymphoid organs. An HEV is, technically, a special region in a blood vessel that's lined with high endothelial cells, which, unlike normal endothelial cells (which are thin "glued" together to form a thin lining on the interior of blood vessels), are much thicker and are column-shaped and cuboid as opposed to small and "shingle"-like. Whereas normal endothelial cells are "glued" together, high endothelial cells are much more strongly "welded" together, leaving just enough space for lymphocytes to enter; high endothelial cells also have many receptors and other features which express ligands that attract lymphocytes. 
-Tour of secondary lymphoid organs: 
    -Lymph nodes: Every lymph node has incoming and outgoing lymphatics that bring lymph in and allow lymph to exit, respectively. There are also small arteries, called arterioles, which bring blood into the lymph node (to provide nutrients and nourishment to lymph node cells) as well as small veins for blood to exit. Lymphocytes can enter the lymph node through an HEV, as mentioned above, but they can also enter directly through the lymph if they are in the lymphatic system instead of the circulatory. It should be noted that although lymphocytes can enter through the blood or through the lymph, they can only exit through the lymph; they cannot return to the blood, as HEVs are a "one-way" gate. Another important substance that needs to enter lymph nodes is antigens, since lymph nodes are the places where B and T cells recognize their cognate antigens. Dendritic cells carrying antigens go to lymph nodes through the lymphatic vessels, bringing antigens to each lymph node (other APCs can also bring antigens to the lymph nodes by traveling through the lymph), and antigen which has been opsonized (by either complement or antibodies) can be carried directly through the lymph to a lymph node. 
        -Structure: A lymph node is arranged a bit like a series of concentric circles (though each shape is far form a perfect circle, of course) and consists of a structure in the center called the medullary sinus, which is surrounded by the paracortex, where T cells are present (they are held there with adhesion molecules); this is because dendritic cells are also present in the paracortex and thus can easily access and activate T cells. The cortex surrounds the paracortex and is home to B cells. The cortex is also where lymphoid follicles are present, interspersed uniformly throughout the cortex. The marginal sinus is covers the outside of the lymph node, is covered with small cavities that allow lymph to percolate through and into the node (though lymph can also enter through lymph vessels that empty into a node). The walls of the marginal sinus cavities contain high concentrations of macrophages which destroy pathogens as they enter the lymph node, acting as filtration systems for the lymph node and the lymphatic system as a whole. 
        -Lymphatic circulation: When helper T cells enter the lymph node through lymphatic vessels, they move through the paracortex, where they are likely to encounter dendritic cells that will activate them. If they are activated, they exit the node through the lymph, circulate throughout the blood for a while, and eventually re-enter a lymph node through an HEV. The circulation process generally takes under a day. The circulation process is vital to the activation of the adaptive immune system, which, as discussed above, requires four things to kick into action: (1) APCs that present antigen to helper T cells, (2) Th cells with the appropriate receptors to recognize the antigen, (3) opsonized antigen that's displayed by follicular dendritic cells, and (4) B cells with receptors capable of recognizing the antigen. The probability of a Th cell encountering its cognate antigen as the cell circulates from node to node is rather small, so when it does happen it is important to activate the rest of the helper T cells quickly. Once a helper T cell is activated, it proliferates to build up its numbers. These Th cells then circulate throughout the blood to "spread out" and reach as many nodes as possible, so they can activate further Th cells as well as B cells (and program B cells in the dark zone of a lymphoid follicle to not commit suicide).  Killer T cells are also activated in the paracortex, should they come across their cognate antigen. Like Th cells, activated CTLs first proliferate then re-circulate the body. Some CTLs terminate the circulation by entering the lymph nodes once more to restart the proliferation-to-activation-to-circulation cycle, while others exit the blood at infection sites to fight the pathogen. Lastly, B cells also participate in this cycle of activation, proliferation, circulation, and re-stimulation. B cells, when re-stimulated, are also capable of undergoing a class switch or somatic hypermutation. 
        -Chemokine interaction: ("Chemokine" is short for "chemoattractive cytokine") APCs and lymphocytes know where and when to go to lymph nodes because they are controlled by special cytokines called chemokines. FDCs in lymph nodes produce a chemokine called CXCL13. Virgin B cells have surface receptors for CXCL13, so when virgin B cells enter the lymph node, the CXCL13 released by FDCs attracts them to lymphoid follicles. When the B cells find their cognate antigen, they down-regulate the expression of receptors for CXCL13 and instead up-regulate the expression of another receptor, for the chemokine CCR7. This is because once they have been activated and have proliferated in the lymphoid follicle, they must receive help from T cells (which program the B cells to not commit suicide), and activated T cells release CCR7, guiding B cells to them. The actual mechanism through which body cells differs from the one by which bacteria move (bacteria are equipped with rotating flagella that allow them to swim); B and T cells "crawl" to move by having the end of the cell at the front of the direction of motion "reach out", while the back end contracts. 
    -Peyer's patches: Peyer's patches (discovered by Swiss anatomist Johann Peyers in the late 17th century) are patches of smooth cells embedded in the villi-covered (small hair/finger-like projections that protrude from the epithelial lining of the intestinal wall) cells that line the small intestine, and are examples of mucosal-associated lymphoid tissues (ie MALTs). The adult human has approximately 200 Peyer's patches. 
        -Structure: Attached to the intestinal wall is a cell called an M cell, which is further attached to a lymph node-like structure, which has an HEV to allow lymphocytes to enter from the blood, lymphoid follicles, a B cell area (analogous to the cortex), a T cell area (analogous to the paracortex), and connected arteries and veins. However, unlike a lymph node, it does not have incoming lymph, but rather only outgoing lymph. Whereas lymph nodes "sample" lymph for antigens, Peyer's patches sample the interior of the small intestine (called lumen) for antigens. By design, a Peyer's patch is not covered with mucous, making it easy for microorganisms to enter. The microorganisms found in the small intestine specialize in transporting antigen from the surrounding lumen to Peyer's patches. The M cell collects antigens by encapsuLating the antigen-collecting microorganisms with vesicles called endosomes that transport them through the M cell and into the Peyer's patch, where the antigens can be recognized by FDCs, T cells, or B cells; outgoing lymph then allows activated lymphocytes to rouse the rest of the immune system. 
    -Spleen: The spleen is situated right between an artery and a vein (called the splenic artery and splenic vein, respectively), and functions as a blood filter. 5% of the blood pumped by the heart in a heartbeat goes through the spleen, where the blood is screened for pathogens (thus the spleen can check the entire blood volume in about half an hour). Unlike lymph nodes and Peyer's patches, the spleen has no HEVs or any kind of filtration system; everything in the blood, no matter what it is, passes through the spleen. Virgin B and T cells circuLating throughout the blood, therefore, also pass through the spleen. They are temporarily retained in different parts of the spleen; T cells are held in a region called the periarteriolar lymphocyte sheath (ie PAL), which surrounds the splenic artery, and B cells are held in a location between the PAL and the marginal sinuses. The marginal sinuses (through which blood first enters the spleen), contains resident dendritic cells. These dendritic cells search for antigens, and once activated, travel to the PAL where they can present the antigen to T cells, after which the remainder of the immune responsive process is the same as in lymph nodes. 
-Trafficking: Lymphocytes do not randomly circulate throughout the blood. Experienced lymphocytes visit locations where the pathogen they pick up is most likely to infect. This is engineered during the T cell maturation phase, in the thymus, when virgin T cells mature and end up expressing a wide array of various cellular adhesion molecules. These molecules act as "passports"; the HEVs that lead to various secondary lymphoid organs only allow lymphocytes displaying the correct adhesion molecules to enter (for example, the surface molecule L-selectin binds to a molecule called GlyCAM-1, which is found in the HEVs of lymph nodes, and the surface integrin molecule alpha-4-beta-7 binds to MadCAM-1, which is found in the HEVs around the Peyer's patches and around lymph nodes near the intestines). Virgin T cells can go anywhere in the lymphatic system and circulate the entire system about every 12 hours, searching each secondary lymphoid organ for their cognate antigen (if it doesn't find one even after 6 weeks of circuLating, it commits suicide). Experienced T cells, however, up-regulate the expression of the cellular adhesion molecules that allow them access to the area where they last encountered their cognate antigen (and hence the area last infected by the pathogen), and down-regulate the adhesion molecules to other locations. This makes sense since it increases the response time of the adaptive immune system if the same pathogen infects someone twice. Experienced T cells also produce certain surface molecules that virgin T cells do not, and these exclusive surface molecules act as "combat passports" that allow the experienced T cell to exit the blood directly at infection sites. In contrast, virgin T cells are unable to visit sites of tissue inflammation altogether. This is because virgin T cells in battle would be useless anyways; they have to be activated first. B cells have a trafficking system similar to that of T cells, but B cells tend to be far less mobile; they prefer to settle in a specific secondary lymphoid organ or the bone marrow and produce antibodies, which do the traveling through the body. 

8. Immunosuppression
-Although the immune system is a highly sophisticated, effective, and deadly bulwark against pathogens, the attacks it launches against pathogens must not be too strong while still being strong enough to repel the invader. It is important that the immune system does not attack an invader with a disproportionate amount of force, and that the immune system eases its state of activation once a pathogen has been vanquished; complications in these areas of immune system restraint can cause irreparable harm to the person's body. For example, in human intestines reside about 100 trillion bacteria (in over 1000 different types) that are beneficial to the body by helping break down food, producing healthy vitamins, and preventing ingested harmful intestinal bacteria from proliferating (the friendly bacteria out-compete harmful bacteria for space and resources, starving the harmful bacteria to death); however, the layer of endothelial cells separating the friendly intestinal bacteria from the intestinal wall is thin, especially compared to the vast number of intestinal bacteria, so it is quite possible that some of these friendly intestinal bacteria make it through the intestine and into the bloodstream, where they could trigger a systemic infection. The immune system cannot react to this threat with a full-blown attack, as this would kill many of the friendly bacteria in the intestines as well as cause intestinal inflammation, leading to diarrhea and other problems. It must react just strongly enough to manage the threat of infection while simultaneously maintaining the balance in the intestines. This is why certain macrophages exist in the intestines specifically to destroy the few "deserter" bacteria that enter the intestinal wall; these macrophages are special in the sense that they do not produce the cytokines that would trigger a full-blown immune response.
-Inducible regulatory T cell: A new type of helper T cell has recently been discovered called the inducible regulatory T cell (ie iTreg), which actually aids in turning the immune system "off" instead of activating it. Instead of traditional Th cell cytokines such as TNF or IFN-gamma, iTreg cells produce IL-10 and TGF-beta cytokines; TGF-beta binds to the surfaces of activated Th cells and CTLs and reduce their proliferation rate, as well as tempering the viciousness with which CTLs kill, whereas IL-10 binds to the surface of activated T cells and prevents co-stimulatory signals from binding to the T cell, hence hindering the cell from being activated. Such iTreg cells are called "inducible" because naive Th cells can be induced to become iTreg cells, if the need arises. 
    -Birth of iTreg cells: In non-battle conditions, epithelial cells produce TGF-beta, which encourages naive, unactivated Th cells to become iTregs. It was mentioned above that TGF-beta also encourages naive Th cells to become Th17 cells; dendritic cells control which of the two cells a native Th cell picking up TGF-beta becomes. If pathogenic bacteria have invaded, dendritic cells release IL-6, which causes Th cells to react to TGF-beta by becoming Th17 cells. Without IL-6, in normal situations, Th cells instead become iTreg cells. 
-Deactivation of the immune system: The innate immune system is a positive feedback loop that is continuously stimulated by the presence of foreign antigen, which is picked up by macrophages and other immune system cells. Likewise, the adaptive immune system is continuously stimulated when T cells recognize antigens (from invading pathogens) that have been transported by dendritic cells to secondary lymphoid organs. Thus, as pathogenic cells are killed, the presence of foreign antigen is eliminated, and the immune system automatically decreases its activity. This is the first and most important step in deactivating the immune system. Another way of restraining the immune system is with the B7 proteins. Normally, B7 is a co-stimulatory protein that binds to the CD28 receptor on virgin T cells and reduces the number of receptors which must be crosslinked for activation to take place in the T cell, making it easier to activate. However, it can also bind to another receptor, CTLA-4, and this binding makes it harder for a T cell to become activated or re-stimulated. This is simply because the CTLA-4 receptor has an affinity for B7 that's thousands of times greater than the affinity CD28 has for B7, and so CTLA-4 simply out-competes CD28 for B7. Normally CTLA-4 is stored in the cell membranes of T cells, where it is unable to interact with anything, but as the battle against a pathogen comes to a close, it is released onto the T cell's surface, gradually deactivating T cells and preventing them from being re-activated. Finally, most cells in the immune system are short-lived anyways, so the immune system simply stops producing them and the existing ones die out; examples of such cells are neutrophils and B cells, both of which live about five days. Other immune system cells deactivate automatically as a result of the death of the aforementioned short-lived cells, such as macrophages, which require the IFN-gamma produced by neutrophils to remain in their activated state. 
    -Activation-induced cell death: (ie AICD) During an infection, massive amounts of T cells (of every immune system cell, really) are produced through proliferation; during some major viral attacks, as many as 10% of all T cells in the body recognize just the invading virus. These vast numbers of T cells that only recognize a single pathogen must be cut down after an infection is over, since otherwise our body would quickly be filled up with T cells capable of recognizing a few past invaders. The way that this number is cut down is through the use of T cells themselves. As mentioned above, CTLs kill in many ways, one of which is by binding their FasL (Fas ligand) protein into its binding partner, Fas, on the surface of the target cell; this binding sets off a cascade that results in the target cell committing apoptosis. T cells have Fas receptors as well but are genetically insensitive to the receptor, preventing the apoptosis chain reaction from happening. Each time a T cell is restimulated, however, its internal wiring changes slightly to make it slightly more sensitive to FasL proteins. Eventually, an "exhausted" T cell that has been re-stimulated many times becomes susceptible to either their own FasL proteins or those found on CTLs, and will soon commit suicide as a result. THis process is called AICD.

9. Tolerance Induction and MHC Restriction
-This section discusses the mechanisms by which B and T cells are "trained" to recognize foreign antigen while simultaneously ignoring self molecules. Without such a mechanism, the human body would die quickly from autoimmune disease as the immune system began attacking the body's own cells and organ systems. T cells must be additionally trained to recognize only antigens that are bound to MHC complexes, rather than free-floating antigens. 
-Thymus: A small lymphatic organ located just below the neck, in front of the heart and behind the sternum. It is where T cells first learn self-tolerance through a process known as "central tolerance induction". Like the spleen, the thymus has no incoming lymph and allows cell entry purely through the blood, but unlike the spleen (which lets everything in the blood in, so as to effectively screen the blood), the thymus is highly selective about which cells are able to get in. Only immature T cells (straight from the bone marrow, where they are born from stem cells (as is every other immune system cell originating in the marrow)) and antibodies are able to get through, though how the selection process works is a mystery, since the thymus has no HEVs (high endothelial venules; see above). 
    -Central tolerance induction: The training process begins with the entry of newly born T cells that don't express either CD4, CD8, or TCRs. These cells also do not express the Fas receptor (which CTLs can bind to with FasL to trigger apoptosis), and do express the Bcl-2 protein (which protects a cell against apoptosis), which isn't expressed by mature cells. These cells go straight to the outer part of the thymus, the cortex. They then proliferate by dividing; during the division process, gene segments begin to be altered. Specifically, the segments encoding the alpha and beta chains of TCRs are altered to induce expression of TCRs and their associated accessory proteins, the CD3 protein complex. However, these cells now express CD4, CD8, and TCR instead of just one; as a result, they are called double positive cells (ie DP cells). DP cells also express large quantities of Fas protein and down-regulate their expression of Bcl-2, stripping them of their immunity to apoptosis. The DP cells are then tested for self-tolerance and MHC restriction, and if they fail either test, they are forced to commit suicide. The final result is T cells that recognize self MHC complexes but which do not recognize self antigens or peptides. Each day, the body tests about 60 million double positive cells, and after a two week testing process, about 2 million survive and leave the thymus (through lymph, blood, or some combination of the two).
        -MHC restriction: (ie positive selection) The process of testing a T cell for MHC restriction is known as "positive selection". The cells that actually do the testing are epithelial cells in the cortex of the thymus, called cortical thymic epithelial cells. If a T cell does not have TCRs that recognize self MHC (ie MHC complexes created in the person's body), it commits suicide. The epithelial cells present the MHC complexes, bound to their associated peptides, on their surface to the T cells, and include both MHC I and MHC II molecules.
        -Self-tolerance testing: (ie negative selection) Shortly after (and sometimes during) positive selection, T cells stop expressing either CD4 or CD8 and commit to their choice of helper or killer T cell; at this stage, the cells are called "single positive" cells (ie SP cells). How this selection process works is unknown. The T cells that survived the MHC restriction testing move from the cortex (located at the outer ends of the thymus) to the medulla, at the center of the thymus.Here, the self-tolerance testing takes place; the testing process is called "negative selection". Negative selection is administered by two types of cells: thymic dendritic cells and medullary thymic epithelial cells.
            -Thymic dendritic cells: These cells, like T cells, are born in the bone marrow and then travel to the thymus. These cells test if T cells have TCRs capable of recognizing self peptides (molecules (specifically, peptides) found naturally in the body), and those that do are forced to commit suicide. Without such a protective mechanism, CTLs would attack the body's own healthy cells, and Th cells would help B cells produce antibodies for natural body molecules (eg insulin) that would then be destroyed by other immune system cells (eg macrophages). However, the existence of thymic dendritic cells presents an ingenious opportunity for pathogens to gain the upper hand in an infection: if a pathogen's antigens found their way into the thymus, thymic dendritic cells might use them for testing and subsequently destroy any T cells that recognized the antigen, making the pathogen virtually invisible to the adaptive immune system. To safeguard against this possibility, thymic dendritic cells have a short lifetime of about a week, ensuring that only the "newest" antigens are presented. This way, even if some foreign antigens did make their way into the thymus, without their constant production, the thymic dendritic cells that presented them would soon die out, taking the antigens with them and returning the testing process to normal. 
            -Medullary thymic epithelial cells: The process of making sure that T cells do not attack the body's own organs is taken care of by medullary thymic epithelial cells, which force any T cells that recognize any organ's tissue-specific proteins to commit suicide. Medullary thymic epithelial cells are "cousin" cells of the cortical thymic epithelial cells that test for MHC restriction, but express, in addition to all the usual receptors and molecules, thousands of different types of tissue-specific proteins, sampled from every organ in the body. It is not clear if these cells provide these tissue specific proteins to thymic dendritic cells for testing or if the medullary thymic epithelial cells do the testing themselves. 
                -Tissue specific material: Every organ in the body has certain organ-specific proteins that exist to identify it as a certain organ, and so no other organs carry a particular organ's proteins (for example, the heart contains, on its surface, specific proteins that identify the organ as a heart and so are not present on any other organs; in the same way, the kidney produces some molecules on its surface that are completely unique to the kidney and are not found anywhere else in the body). T cells must also be educated to not recognize these molecules (which are not antigens and so are not taken care of in the traditional thymic dendritic cell presentation process), else they would attack the body's own organs. 
            -Safeguards: The important thing to realize about the entire testing process is that it is not rigid and completely comprehensive; it is unfeasible to test every single T cell for every single self antigen in the body; instead, the system in the thymus detects and eliminates T cells with a high affinity for self antigens that are abundant in the thymus. T cells with low affinity for self antigen or T cells that recognize self antigen that is rare in the thymus (since the thymus can't test every single antigen, it prioritizes them) may escape deletion and pass the tests. This is why virgin T cells are restricted to circuLating throughout the secondary lymphoid organs, but they are not allowed into the body's tissue. These strict travel restrictions and patterns imposed on virgin T cells make is extremely improbable that a self-antigen-recognizing T cell that escaped deletion would actually come across its self-antigen (and become activated as a result), since self antigens abundant in the thymus are also abundant in the travel pathways taken by virgin T cells, for precisely this purpose. 
                -Natural regulatory T cells: (ie nTreg) Even despite the safeguards imposed above, it cannot be guaranteed that virgin T cells will always be separated any potential self antigens they might react to if they escaped deletion during testing, since external influences, such as internal injuries, may cause the travel pathways of the virgin T cells to accidentally intersect with self antigen (eg if a blunt force trauma ruptured a blood vessel, causing self antigen to flow in from the surrounding tissue and subsequently leading to a few rare T cells that escaped deletion during testing to recognize the antigen). To account for this situation, before maturation in the thymus, some helper T cells become a specialized category of helper T cells instead of traditional helper T cells; these cells are called natural regulatory T cells (ie nTregs), and their purpose is to suppress the activation of potentially self-reactive T cells. About 5% of all helper T cells are nTregs. They differ from normal CD4+ cells (ie helper T cells, all of which carry the CD4 surface glycoprotein) because a gene called Foxp3 is expressed in nTregs and not in normal Th cells. This gene causes the nTreg to express certain adhesion molecules that allow it (normally restricted) access to lymph nodes and other secondary lymphoid organs, where they hunt down rogue T cells and suppress their activation mechanisms if they have recognized any self antigen; the exact mechanisms through which they do this are unknown. nTregs are antigen-specific, just like Th cells and CTLs, meaning that a particular nTreg recognizes exactly one type of self antigen, and it will suppress the activation of only the T cells that mistakenly recognize that self antigen. 
                    -People who are unable to express the Foxp3 gene tend to die early from autoimmune disease. 
                -Peripheral tolerance: This safeguard deals with T cells that stray from their mandated traffic pathways and hence encounter self antigen. However, this is one major reason that simply recognizing antigen is not enough to activate a T cell; it must also pick up the appropriate co-stimulatory molecules. Activated APCs exhibit both high levels of MHC proteins (that are, of course, bound to antigens during an infection) as well as high amounts of co-stimulatory molecules. In contrast, most of the body's cells do not exhibit both MHC proteins in high quantities and co-stimulatory molecules (though they may exhibit one), and so they will not activate a T cell even if it does mistakenly recognize a self antigen on that cell. Moreover, whenever a T cell binds to the MHC-antigen complex of a cell but does not receive co-stimulation in conjunction, instead of simply not being activated, it is anergized, meaning that it is now unable to be activated in the future no matter what, even if it does come across MHC-antigen complexes alongside co-stimulation; the cell is effectively "neutered", and many anergized cells end up dying soon anyways.
                -AICD tolerance: This final layer of tolerance exists to deal with the very unlikely situation that a T cell in the thymus accidentally passed the testing process (due to either a low yet non-zero affinity for self antigen or an affinity for self antigen that's very rare in the thymus (or both)), accidentally somehow made it out of the normal travel pathways and into the tissues (possibly due to external injury), and somehow came across a rare cell that happens to be presenting both the cognate self antigen and appropriate co-stimulation. In this case, the activated T cell is stimulated over and over by the same cognate self antigen so much that it triggers a particular pathway inside the cell that results in activation-induced cell death (ie AICD, discussed above), the same way that CTLs kill infected cells; this pathway exists specifically for such a situation as described, so the immune system can recognize that if a T cell is constantly re-stimulated extremely frequently (a situation known as "chronic stimulation"), something is probably wrong. 
    -Riddle of central tolerance induction: In the two phases of testing, T cells' TCRs are tested for recognition of self MHC and self peptide, but the question is: how does the same TCR, when presented with MHC-peptide complexes, signal three different outcomes: positive selection (during MHC restriction testing), negative selection (during self-tolerance testing), and activation (during an infection)? This questions is a major unanswered question in biology (worthy of a Nobel prize) and has not yet been answered. 
-B cell tolerance: B cells are unlikely to be able to produce antibodies against self antigen because they require helper T cells to activate them first, and any helper T cell that was capable of activating a B cell that targeted the body itself would have to recognize self antigen itself, and would therefore have been destroyed in the thymus. However, safety net mechanisms to ensure B cell tolerance are still in place. Most B cells are tolerized immediately after their birth in the bone marrow. The process mirrors that of T cells in the thymus: gene segments are continuously mixed and matched, and any B cells that recognize self antigen are destroyed through apoptosis (although the B cell is first given a chance to rearrange its gene segments once more to try and produce receptors that don't recognize self antigen; this process is called "receptor editing"; about 25% of B cells (in mice, studies show) take advantage of their "second chance" and survive). About 10% of B cells survive pass the tolerance testing process; the rest die in the bone marrow. The same traffic restriction safeguard and AICD tolerance mechanisms that regulate rogue T cells apply to B cells as well.
    -Tolerance during somatic hypermutation: T cells never change their receptors from the moment they have them, so if a T cell passes the tolerance tests, it has passed for its lifetime. However, in germinal centers B cells are able to undergo somatic hypermutation, when they proliferate rapidly and with an extremely high rate of mutation (in order to accelerate the process of evolution, in this case through artificial selection for B cells adept at fighting the invader), and so it is possible that a B cell that passed the tolerance tests at birth would, during somatic hypermutation, have descendants capable of, due to random mutation, recognizing self antigen. There are two main reasons that such B cells do not survive the process. B cells during somatic hypermutation are in a state of extreme vulnerability (so eliminating sub-par B cells is rapid and efficient), and so if a B cell does not receive the specific signals from follicular dendritic cells to continue on, it will commit suicide. B cells that recognize self antigen are very unlikely to come across a follicular dendritic cell that's displaying its cognate self antigen, since follicular dendritic cells only display antigens that have been opsonized, and self antigens are never opsonized. Thus, a B cell will die because it did not receive the continuation signals. Even if it does receive these signals, however, it is very unlikely that it will receive co-stimulation, and so will die this way. 

10. Immunological Memory
-The immune system remembers pathogens that have infected the body in the past and which the immune system fought off, so the next time the pathogen is encountered, it is very rapidly and efficiently destroyed (often so fast that the body doesn't even develop symptoms, and so the hosts don't even know they're sick).
-Innate memory: Millions of years of evolution have encoded an innate memory in every person's genes, allowing the immune system to instantly recognize and respond to thousands of everyday invaders. Pattern-recognition receptors (eg Toll-like receptors) immediately recognize such daily and mundane invaders, leading to their rapid destruction. 
-Adaptive memory: The adaptive immune system does not recognize pathogens that affected our ancestors but rather pathogens that have infected us in our own lifetimes, in the past. 
    -B cell memory: During the first encounter with a pathogen, activated B cells become either plasma cells or memory cells. Two different types of B cells are produced in germinal centers during an invasion: the long-lived plasma cell, and the central memory B cell. Long-lived plasma cells are akin to the short-lived plasma cells that live for a few days but produce overwhelming quantities of antibodies during the infection, but they reside in the bone marrow and produce less expansive amounts of antibodies. Specifically, they produce IgG, which is what confers long-term immunity. Central memory B cells reside (mainly) in the secondary lymphoid organs, and instead of producing antibodies, they slowly and continuously proliferate to maintain a large pool of central memory B cells, replenishing old central memory B cells as they die, and when an infection by the same pathogen occurs once again, the central memory B cells can quickly produce large amounts of short-lived plasma B cells, as well as maintain the supply of long-term plasma B cells over time. 
    -T cell memory: During an initial infection, activated T cells build up their numbers (up to 10,000-fold), and after the infection has died out, most of the activated T cells die through apoptosis. However, some of these T cells persist on as memory effector T cells, and survive for at least ten years. During a subsequent attack by the same pathogen, these memory effector T cells instantly proliferate rapidly and drive the infection away. Another type of memory T cells are the central memory T cells, which do the same thing as memory effector T cells (re-activate, proliferate, and attack during subsequent infections), but these T cells reside only in the secondary lymphoid organs and bone marrow.
    -Properties of memory cells: Memory cells in general have several advantages over regular immune system cells that allow them to battle off an invader much more effectively (so effectively that most people don't even notice that they're sick during subsequent infections). The first is that of pure numbers. The first time a pathogen attacks, the body is not ready for it and so the correct B and T cells to recognize the pathogen must be selected out of every possible B and T cell, approximately a 1 to 1 million proportion. After an attack, however, many B and T cells survive on as memory cells, and so the proportion has risen to 1 to 1 thousand immune system cells that will recognize the invader. Another important advantage is ease of activation; co-stimulation is no longer essential to memory cells (though it accelerates and enhances the process nonetheless), and the activation process is much faster than it is for virgin B and T cells.  Another reason that they're so much faster is that memory B cells have been through somatic hypermutation and so have greatly enhanced receptors that can detect very small amounts of foreign antigen debris (their antibodies also bind much tighter to invading cells). Moreover, memory B cells do not have to begin with IgM antibodies, identify the invader, then switch to more specialized antibodies; since they already know the identity of the invader based on the first encounter, they can immediately produce the specialized and targeted antibodies to subdue the attacker much more effectively. 

11. Vaccination
-Vaccination is a mechanism for inducing adaptive immunity in an individual without exposing that individual to the devastating effects of the pathogen in question. The immune system, upon encountering an invader, produces B and T cells against that invader that leave behind memory cells that will repel the infection very effectively if it is encountered again, but the first encounter can be very debilitating to the individual, and there is no guarantee he will even survive. Vaccination exposes the individual to an extremely weak or dead strain of the pathogen, to induce the creation of memory cells without any actual harm, so that if the full-fledged pathogen ever infects the individual, the memory cells produced during vaccination will quickly deal with the infection. 
-Generating memory cells: For memory cells to be created, dendritic cells must pick up pieces of foreign antigen debris and bring it to a lymph node, where they will break the antigen down into unique peptides and display them on MHC II molecules on the surface; if the appropriate B and T cells recognize the antigen, they will proliferate, and some of these cells will remain behind as memory cells. Thus, it is possible to provoke the body into producing memory cells without actually infecting the body with live pathogen; all that requires is that antigen debris from the pathogen is injected into the body. This is the basis for vaccination. 
    -Although memory helper T cells and memory B cells can be produced using only antigen debris, memory killer T cells need to recognize the appropriate MHC II-antigen complexes to activate, proliferate, and create some memory CTLs. Thus, for memory CTLs to be created, an antigen-presenting cell must be infected with the pathogen. 
        -Cross-presentation: In certain experimental conditions, APCs use MHC I molecules to pick up and display antigens taken up from outside the cell, in which case virus-specific memory CTLs can be generated without infecting any APCs. However, it is very unclear about this process works and not much is known about it; to date, not a single anti-viral vaccine is in use that employs cross-presentation.
-Vaccine development: There are many different strategies for producing effective vaccines, but the process is not easy, and varies from disease to disease.
    -Non-infectious vaccines: The majority of vaccines are designed not to infect the recipient. Many vaccines against viruses (eg polio (the Salk vaccine), common flu, etc.) and bacteria (eg typhoid, whooping cough (ie pertussis), etc.) are developed by "killing" the virus, so that it still looks to the immune system as a virus, but it is completely functionally disabled. For example, the polio vaccine is created by treating the polio virus with formaldehyde, which "glues" the proteins in the virus together and renders them disabled, disallowing the virus from infecting the host but nonetheless appearing to the immune system as a deadly invader. For bacteria, vaccines can be derived from proteins released by the bacteria upon infection, called toxins (these actually cause the symptoms); the vaccine is typically treated with something like aluminum salts to weaken the toxin (this weakened form is called a "toxoid"), which is injected into the patient and induces the production of antibodies and hence memory cells (for example, the vaccines for diphtheria and tetanus are made this way). Another technique is to break apart the pathogen so that the parts that actually cause harm to the body can be discarded and the parts that the immune system needs to recognize can be injected, so as to rouse the adaptive immune system without harming the body (obviously this technique only works on pathogens whose antigens that are recognized by the immune system are not part of the harmful effects of the pathogen). Many vaccines nowadays are created through the genetic engineering of viral proteins (eg the hepatitis B vaccine, human papilloma virus (ie HPV) vaccine, etc.); such vaccines are called "subunit" vaccines. 
        -In some cases, the procedure used to disable to virus is not completely 100% effective and may leave a few live viruses amongst the millions of dead. For very common, low risk vaccinations such as the flu, this is not the problem, as the vaccination is still an improvement over lack thereof. However, more rare and serious vaccines, such as that for HIV, would need to have a virtually 0% probability of giving the host the disease before it could be used on the general public. 
        -A major drawback of non-infectious vaccines is that in many or most cases, they will not provoke the creation of memory CTLs (though this is not a problem at all for extracellular bacteria (ie bacteria that do not infect cells at all and so render CTLs useless in the fight)). This is why "killed" virus vaccines are ineffective for the measles or mumps virus (though other vaccinations for these diseases to exist).
    -Attenuated vaccines: These vaccines do not use dead versions of the virus but rather extremely weak versions that are still capable of infecting cells, but too weak to cause any real harm. It has been shown in laboratories that in many situations, viruses grown in cells that aren't their normal host cells develop mutations that make them much weaker (the measles, mumps, and rubella vaccines are made this way, and the Sabin polio vaccine is made this way as well (specifically, by growing the polio vaccine (which normally resides in human nerve cells) in monkey kidney cells)). Attenuated vaccines do produce memory CTLs, but they do present some safety issues. First, since the virus isn't dead but rather is weak, someone injected with the vaccine can infect others with the same weak strain of the virus, which is actually a good thing if those infected are healthy (since it would then inadvertently immunize them as well), but if the person infected is immunosuppressed (eg due to another disease, chemotherapy, etc.), then even the weak virus could very well harm the person, since his weakened immune system cannot fight it off. Another concern is that the weakened virus, which is weakened usually due to accrued mutations, may mutate again once inside the body and restore its strength this way. Although this is an unlikely risk, it makes attenuated vaccines a poor choice for viruses with high mutation rates (eg HIV).
    -Carrier vaccines: This method involves taking a normally benign virus and using it as a carrier to insert the vaccine. A single gene from a harmful pathogen is taken and genetically engineered into a carrier virus' genetic material, and the modified carrier virus is then injected into a patient. The carrier virus will infect cells but will not harm them - it's a benign virus, after all. However, the cell will nonetheless start reading the carrier virus' genetic material, as with all viruses, and produce the proteins encoded in it, including the spliced in protein for antigens from the harmful pathogen. In this way, antigens from the harmful pathogen are displayed by MHC I molecules without ever infecting the patient with any strain, dead or weak, of the actual virus at all and without harming the patient at all (since only a benign strain was injected), producing memory cells. 

12. Autoimmune Diseases
-Although the immune system usually protects against disease, in certain situations, it goes wrong and begins to attack the body itself, causing debilitating effects. 
    -Examples: Tuberculosis (ie TB) is one example of a disease that results in large part from the immune system trying to do its job: When TB bacteria (Myobacterium tuberculosis) are inhaled, they enter the lungs, where they encounter lung macrophages designed to destroy invaders that enter via the airway. Normally, macrophages kill by first engulfing the invader in a piece of its cell membrane, which then breaks off inside the macrophage to form a structure called a phagosome, which contains the microbes; the phagosome fuses with a lysosome inside the macrophage, and the deadly chemicals inside the lysosome destroy the microbes. However, TB bacteria alter the surface of the phagosome in such a way that the phagosome is unable to fuse with a lysosome, and so the bacteria survive and grow inside the phagosome, eventually bursting out of the macrophage, killing it and going on to infect more macrophages. Each time a macrophage dies by necrosis, the contents of all its lysosomes are released, exposing lung tissue to a battery of harmful chemicals. The resulting inflammatory reaction as well cytokines given off by the macrophages recruit more macrophages and other immune system cells to the lungs, continuing the cycle.Another example is sepsis, a broad term that describes a series of symptoms and is also caused by the immune system simply doing its job. An important feature of the immune system is that its attacks are localized to the infection site, so as to preserve the rest of the body's normal rhythm. However, when certain pathogens infect the entire body, called a systemic infection, the immune response is launched throughout the entire body, often with harmful results. Gram-negative bacteria most commonly cause sepsis, and they contains LPS, which attracts NK cells and macrophages, which release cytokines that stimulate each other to grow in a positive feedback loop. When this positive feedback loop is happening throughout the entire body, massive amounts of cytokines are being released; specifically, excessive amounts of TNF are circuLating throughout the body, and that level of TNF can lead to small ruptures in the walls of blood vessels that cause them to "leak" blood. If too much blood is lost due to this internal bleeding, the blood pressure drops low enough for the patient to go into cardiac arrest, a phenomenon termed "septic shock". 
-Allergic reactions: Allergic reactions are excessive immune system reactions to harmless allergens. Not all individuals suffer from allergies to any particular allergen, and those that do are referred to as "atopic" individuals. Allergic reactions are caused, at the first level, by IgE antibodies. The first time an individual is exposed to an allergen, the body perceives it as an invader (specifically, a parasitic invader) and produces large amounts of IgE antibodies in response (with concentrations 1,000 to 10,000 times higher than normal). When the same allergen is encountered again, it is immediately, as a result of the adaptive immune response, swarmed with IgE antibodies. This causes mast cells to rush to the site, where the IgE antibodies cluster together and bind to the allergens (which typically have long, repeating structures that enable IgE antibodies to bind in this cluster format), so that when mast cells bind to the antigen-antibody complexes, their Fc receptors are immediately crosslinked, activating them. Activated mast cells are triggered to degranulate, meaning they rapidly expunge the granules of harmful, deadly chemicals onto the "invader" as well as other chemicals such as histamine and heparin, which ends up producing the adverse effects commonly seen with allergies. There are two phases of an allergic reaction: immediate and delayed. The immediate phase is caused by mast cells degranuLating as well as basophils degranuLating, which also have receptors for IgE antibodies and are recruited from the blood by activated mast cells. The second phase, the delayed phase, is caused by eosinophils. Eosinophils have the same effects as mast cells and basophils (all three are granulocytes), but the eosinophil concentration is normally very low in the blood, and eosinophils must be recruited from the bone marrow by Th cells that release cytokines, such as IL-5, that attract eosinophils during an allergic reaction. the travel time from the bone marrow to allergen site is what causes the delayed reaction. 
    -Hygiene hypothesis: The difference between atopic and non-atopic people likely likely resides in the germinal centers. If the B cells in the germinal centers are exposed to Th1 cells, they will mostly class switch to general IgG antibodies, but instead sometimes these germinal centers contain Th2 cells, which influence B cells to switch to IgE antibodies and hence cause allergic reactions. It is thought that individuals exposed to harmful microbes or allergens early in life have their immune system "rewired", since if an imbalance between Th1 and Th2 cells exists (in favor of Th2), then it will be corrected during the normal immune response. This may explain why children who grow up in developed countries, where exposure to harmful pathogens is much less likely, especially early in life, tend to be much more allergic and to more allergens than children who grow up on farms or in developing countries. This "rewiring" of the immune system may be caused by the production of long-lived iTregs (inducible T regulatory cells) during the immune response. The iTregs suppress the immune system in situations that provoke immune responses but when an attack is over or not present, such as exposure to an allergen. 
    -Heredity: It has been observed that children whose parents are allergic tend also to be allergic, and in identical twins, if one is allergic, there is a 50% chance that the other is as well. However, the genes that confer allergy are very numerous and vary from person to person, and so are difficult to identify. 
    -Treatment: Glucocorticoid steroids can help manage an allergic reaction, since the steroids block cytokine production in Th cells and thus prevent B cells from being activated and then producing IgE antibodies. However, the immunosuppressant nature of glucocorticoid steroids increases the patient's susceptibility to infection. A more recent development is the engineering of Omalizumab antibodies, which bind to the Fc region of IgE antibodies and prevent them from binding to allergens and so preventing the allergic reaction altogether. This treatment is now approved for use in the US and has been shown to be very effective, especially in severe allergic reactions. 
    -Cure: Treatments for allergies merely manage the symptoms, but do not eradicate the underlying cause. Only one known approach has been demonstrated to actually permanently cure allergies - specific immunotherapy. In specific immunotherapy, gradually increasing amounts (starting from very small) of crude allergen extract are injected into the person, and over the period a few years, the injections build up to a maintenance dose that is maintained for another few years, after which the person will usually develop a tolerance for the antigen and no longer be allergic. It is not well understood why this method works, though some think that the repeated exposure to the same allergen somehow cause B cells to class switch away from IgE antibodies (the ratio of IgG to IgE has been observed to increase with time during specific immunotherapy, to ratios as high as 10- or even 100-fold). It is also thought that the repeated exposure may somehow stimulate the creation of iTregs whose cytokines suppress IgE production. 
-Autoimmune disease: Autoimmune disease occurs when the mechanisms to preserve self-tolerance break down, and the immune system attacks the body. About 5% of Americans have some form of autoimmune disease. Although some autoimmune cases are genetic, the majority are caused by self-reactive B and T cells that slip through the cracks of the multi-layered self-tolerance mechanisms; in a way, autoimmune disease are the price we must pay to have B and T cell receptors so diverse that they can recognize virtually any pathogen in existence. There are three components that produce an autoimmune disease. The first is the expression of MHC molecules that efficiently present peptides derived from self antigen. The second is the production of B and T cells with recognize the self-antigen presented in the first component. The final component is some environmental factor that causes the breakdown of the mechanisms that are in place to eliminate self-reactive cells. It has been observed that when the first two components are present, the environmental factor that actually causes the autoimmune disease is often an infection.
    -Molecular mimicry: This is the most widely accepted theory for why infectious diseases can act as the "last straw" in provoking autoimmune diseases. B and T cell receptors do not concretely bind to exactly one cognate antigen and ignore the rest; rather, the binding strength is a spectrum, and BCRs and TCRs have very high affinities for their cognate antigen, relatively low affinities for several other antigens, and no affinity for the rest (in this case, we say that the B or T cell "cross-reacts" with the antigens for which it has low affinity). There might exist B or T cells who recognize foreign antigen as they are supposed to but also have low affinity for self antigen. This affinity is low enough that these B and T cells cannot be crosslinked by self-antigens, but if an infection invades the body that triggers these B and T cells and activates them, even the low affinity for self antigen will now cause the activated B or T cell (which was activated by the pathogen) to attack cells that have these self antigens, leading to autoimmune diseases. The invading pathogen "mimics" the self antigen in this way. 
    -Inflammatory necessity: For a self-reactive T cell to actually harm the body, it must be activated and also stay activated. It might become activated through molecular mimicry, but in order to stay activated, it requires constant re-stimulation so that it avoids anergization or apoptosis. APCs such as macrophages must be present to continuously re-stimulate these self-reactive T cells in order for an autoimmune disease to result. This is not a problem during an infection at infection sites, when macrophages are present in huge numbers anyways, but in areas of the body without infection, there must be inflammation for autoimmune disease to result, since the inflammatory response recruits macrophages and other APCs. 
-Examples of autoimmune disease:
    -Insulin-dependent diabetes mellitus: This is an organ-specific autoimmune disease in which self-reactive B cells produce antibodies that bind to the beta-cells in the pancreas that produce insulin, disabling the body's ability to create insulin. Note that although B cell antibodies drive the disease, it is initially sparked by self-reactive CTLs attacking the beta-cells. Destruction of these pancreas cells often occurs long before any symptoms appear; about 90% of beta-cells are already dead by the time symptoms appear. Since the disease has a genetic component, if one family member has insulin-dependent diabetes mellitus, his family members can be tested and even cured if the disease if caught early enough. 
    -Myasthenia gravis: This is a neural autoimmune disease that occurs when self-reactive antibodies bind to receptors meant for the neurotransmitter acetylcholine to bind to. The lack of acetylcholine stimulation in the brain can cause muscle weakness and even paralysis. This is similar to the way that the polio virus attacks. 
    -Multiple sclerosis: This is another neural disease, and is characterized by chronic inflammation in the brain. Self-reactive helper T cells initiate the inflammatory response in the brain, and the resulting inflammation destroys the myelin sheathes that coat the neurons, damaging or disabling communication between neurons. The self-reactive Th cells in the brain recruit macrophages using cytokines to cause the inflammation. Moreover, CTLs target the myelin basic protein, an essential component of myelin sheaths. It is thought that the Epstein-Barr and herpes simplex viruses can provide the molecular mimicry necessary to push the condition into an autoimmune disease. 
        -Although most immune system cells cannot cross the blood-brain barrier, activated T cells can. 
    -Rheumatoid arthritis: This is a systemic autoimmune disease characterized by chronic inflammation in the joints. Self-reactive T cells recognize a particular cartilage protein and attack it, inflaming the joints in which the cartilage is contained. In the joints, there are IgM antibodies that can bind with IgG antibodies, and the IgM-IgG complexes attract macrophages and other innate immune system cells to the joints, causing inflammation and swelling. It is thought, but not conclusively, that myobacterial infections can trigger rheumatoid arthritis through molecular mimicry. The most prominent factor in this inflammation is TNF, which recruits large amounts of cells associated with inflammation. 
    -Lupus erythematosus: A systemic autoimmune disease; it affects 250,000 people in the US, and mostly (90% of victims) women. Lupus is the widespread assault of the immune system on the entire body, and results from a breakdown of a large portion of the self-tolerance mechanisms in the body. When large amounts of self-reactive B and T cells are circuLating throughout the body, a diverse collection of IgG antibodies capable of binding to a wide variety of self-antigens (including DNA, DNA-protein complexes, and RNA-protein complexes) are produced. These autoantibodies also cause inflammation in the body's "filters" (eg kidneys, joints, brain, etc.). It is believed that the disease has a strong genetic connection. 

13. Immunodeficiency
-Immunodeficiency is a state in which part or all of the immune system is disabled in a person's body, leaving them extremely susceptible to many diseases, even normally very easy-to-kill ones. Some immune deficiencies are genetic (inherited or by mutation), others are medically induced (eg during chemotherapy or an organ transplant), and others are caused by disease (eg HIV AIDS). 
-Genetic immune disorders: When even single genes are mutated, the immune system as a whole may be compromised. For example, people with dysfunctional genes that result in non-functional CD40 or CD40L proteins cannot mount any T cell-dependent defenses, since T cells cannot activate B cells' co-stimulatory receptors, and so only IgM antibodies are produced (since somatic hypermutation and antibody class switching are never invoked), severely diluting the efficacy of the adaptive immune response. Another example is DiGeorge syndrome, in which the thymus does not develop at all (all of its tissue is just missing) and so the body cannot produce functional T cells, leaving the affected person vulnerable to many life-threatening diseases. Some people are born without functional complement systems (eg no C3 protein), while others do not have functional or even existent germinal centers. Nonetheless, inherited immunodeficiency is relatively rare, with about 1 in 10,000 newborns born to immunodeficient parents being affected genetically. 
    -Severe combined immunodeficiency syndrome: (ie SCIDS) A class of rare disorders in which both B and T cells cannot function properly (in most cases, only one is affected). 
-AIDS: (ie Acquired Immunodeficiency Syndrome) The AIDS virus currently affects about 30 million people worldwide, with about 3 million new infections per year. The virus that causes AIDS was termed the human immunodeficiency virus number one (ie HIV-1), and was discovered when physicians observed the high incidence of infections and cancers that typically affect only immunosuppressed people. 
    -Infective process: HIV-1 is typically passed from person to person when the virus penetrates the vaginal or rectal mucosa (though it can also be passed through an infected person's blood), where it begins attacking the helper T cells that reside in that area of the body. At this point, the innate immune system begins fighting the virus while the adaptive immune system is mobilized, and meanwhile the virus continues trying its best to replicate itself using infected cells' biosynthetic machinery. After about a week, the adaptive immune system sends B cells, Th cells, and CTLs. This is the acute stage of the infection, when the viral load (number of viruses in the body) drastically skyrockets as the viruses replicate quickly, though once the B and T cells arrive at the infection site, the viral load begins to decrease as the B and T cells do their job and repel the virus. In most viruses, this acute stage is followed by a sterilization stage in which the immune system destroys all the viruses, memory B and T cells survive on, and the body lives on. However, in some viruses, including the full-blown version of the HIV-1 virus, the acute phase is followed by a chronic phase, in which the virus fights back against the immune system and the two are locked in a ferocious battle that can last ten or more years. Although viral load is decreasing, in some cases they kill CTLs (a normal part of the virus' replication process involves the death of CTLs) faster than the viruses themselves are dying, and so after a certain point, when the number of CTLs gets too low, the virus gains the upper hand and the viral load again begins to rise. The final result is the immune system being completely overwhelmed, with extremely few CTLs left, leaving the body vulnerable to countless "opportunistic" infections that would normally be repelled very easily but which can be deadly to someone in such a immunosuppressed state. 
    -Reasons for success: The immune system is usually extremely effective at repelling pathogens, but the HIV-1 virus has been remarkably successful, and it has a few key properties that allow this. The first is a strategy that the virus uses to infect cells: once HIV-1 viruses infect a cell, the virus' RNA is copied onto a piece of copy DNA (ie cDNA) which is spliced into the host cell's DNA in the nucleus. However, the spliced-in viral cDNA has the unique property that it is able to enter a latent unactivated state and just sit in the host cell's DNA. In this latent state, the infected cell cannot be detected by CTLs, and so this way a large number of such "sanctuary" cells are amassed, each infected with this "stealth" virus. At some point, through an unknown mechanism, the latent virus is reactivated somehow and all the stealth viruses attack at once, bursting from the cell in higher numbers, killing it and infecting additional cells. Another major property is the virus' extremely high mutation rate. The enzyme used for copying RNA into cDNA is called reverse transcriptase, and on average one mutation (in one DNA base pair) occurs every time a piece of RNA is copied. Thus, even in the body the virus is mutating very quickly and thus evolving to evade the immune system; studies show that although most these mutations are harmful or have no effect at all, the rate is so high that about once every ten days a virus with completely new antigen peptides is created, meaning an entire new class of B and T cells must be activated once more just to deal with the new evolved virus. This entire time, the original virus continues replicating very quickly. Another reason that the virus is so successful is that it specifically targets immune system cells: helper T cells, macrophages, and dendritic cells. The virus kills or causes CTLs to kill Th cells, macrophages, and dendritic cells, all of which are activate CTLs themselves; thus the virus pinpoints a crucial part of the adaptive immune system and cripples it. The virus even uses the immune system against itself, such as its use of dendritic cells' travel pathways from lymph node to lymph node as a mechanism for using infected dendritic cells to transmit the virus throughout the entire body, or its use of activated B and T cells, which rapidly proliferate, so if a virus infected an activated B or T cell, the cell itself would rapidly proliferate and thus replicate the virus on its own.  

14. Relationship between the Immune System and Cancer
-Overview of cancer cells: There are many control systems (that affect and control various cellular processes) within a cell, and they can be broadly categorized into two main types: proliferation systems, which promote cell growth, and safeguard systems, which defend against irresponsible and inappropriate cell growth. There is an enormous amount of cell proliferation that goes on in the body every single day, and this proliferation must be carefully controlled. When the proliferation system of a cell malfunctions, it can lead to cancer; this malfunction is usually in the form of a gene mutation. Certain genes have the potential to mutate into specific versions of themselves that can cause the cell to proliferate irresponsible; such genes are called "proto-oncogenes", and the actual mutated versions of these genes that actually cause the cancer are called "oncogenes". In fact, the aforementioned safeguard systems exist to account for this very risk. For example, every day each cell goes through, on average, 25,000 mutations per day, and so there exist safeguard mechanisms that work non-stop to correct these errors. Even if the DNA repair system misses some mutations, a backup system that checks for un-repaired mutations will instruct the cell to stop proliferating so the repair system has more time to find and fix the mutation, and if the mutation is severe enough, the backup system may instruct the cell to commit suicide. Such repair systems often involve complex systems of proteins, such as P53 (which is integral to the backup mutation repair system), that are called tumor suppressors (the genes that encode them are called anti-oncogenes or tumor suppressor genes). P53 specifically is vital to anti-cancer mechanisms; in lab tests, mice with mutated P53 genes died of cancer in less than 7 months. For cancer to actually occur, the cell must suffer multiple mutations in which not only do the proto-oncogenes, but the tumor suppressor genes in that cell are mutated as well; on average, a cell must suffer 4 to 7 mutations spanning multiple cellular control systems for the cell to become cancerous. One of the hallmark characteristics of a cancerous cell is a genetically unstable condition of constant mutation, due to various malfunctions of control systems. The probability of this happening is very low, but still positive, and this is why cancer tends to strike late in a person's life; it takes time for the unlikely event of 4 to 7 mutations in multiple control systems at the same time in the same cell to occur. About 1 in 3 people in the US will develop cancer at some point in their life.
-Cancer cell classification: 
    -By location relative to the blood: There are two main types of cancers: blood-cell cancers and non-blood-cell cancers (called "solid tumors"). Carcinomas, the most common type of cancer in humans, are cancers of epithelial cells such as the lung, breast, colon, cervix, etc.; carcinomas generally are deadly because they metastasize to vital organs. Sarcomas, a much less common cancer type, are cancers of the connective and structural tissue. Both carcinomas and sarcomas are solid tumors; the most common blood-cell cancers are leukemias and lymphomas. Blood-cell cancers occur when descendants of blood cells do not mature out of the proliferation phase, and continue proliferating indefinitely. In leukemia, these proliferating cancerous cells fill up the bone marrow, preventing other cells from proliferating and maturing. This typically, though not always, causes the victim to die from anemia (healthy red blood cell deficit), if not enough blood cells are able to mature from the bone marrow, or from infection, if not enough immune system cells are able to mature in the bone marrow. In lymphomas, clusters of proliferating cancer cells form and grow in the lymph nodes, sometimes, the other secondary lymphoid organs, where they disrupt the immune system. Victims typically die from infection or organ malfunction. 
    -By cause: Cancer can also be classified into the two main categories of spontaneous and virus-associated. Spontaneous tumors arise through the process described above, due to random mutations striking at the same time; most human tumors fall into this category. In virus-associated tumors, the cause of a cancer is viral. Some viruses are able to produce proteins that interfere with the safety anti-mutation mechanisms in cells, and thus they increase the probability of cancer dramatically. For example, human cervical cancer almost always is caused by HPV, and hepatitis B can, in some cases, disrupt safeguard mechanisms in liver cells and thus increase the likelihood of liver cancer. Approximately 20% of all human cancers have a viral-accelerating factor. 
-Immune surveillance: It is not known with certainty whether the immune system plays a role in preventing cancer, but it has been documented that defective immune systems do correlate with higher incidences of lymphoma, leukemia, and virus-associated cancer (though blood-cell cancers are rare).
    -Tolerance and surveillance: When it comes to CTLs, there is a major conflict between self-tolerance and immune surveillance. For one thing, CTLs have restricted travel pathways, as this is one of the layers in the self-tolerance safety net. However, these restricted travel patterns mean that CTLs that are patrolling for cancer miss a large portion of the body entirely (eg lungs, legs, liver, etc.) and so would not detect many different kinds of cancer at all. Moreover, even if a CTL did detect a cancer cell and recognize its antigens as foreign, it would require co-stimulation that most cancer cells are not going to provide, and so the CTL would be anergized or killed anyways. When it comes to blood-cell cancers, the possibility of CTL surveillance is more promising; the travel pathways of CTLs do intersect with the blood and secondary lymphoid organs (where lymphoma and leukemia occur), and cancerous cells in the blood and secondary lymphoid organs do in fact carry co-stimulatory molecules such as B7. Indeed, immunosuppressed patients are more likely to get lymphomas or leukemias. However, the system must be an incomplete one, since even patients with healthy immune systems still get leukemias and lymphomas. 
    -Macrophages and NK cells: Macrophages and NK cells may play a productive role in cancer prevention. It has been shown that TNF can kill certain types of cancer cells; what happens is that the TNF cytokine attacks the blood vessels that feed the tumor. The blood supply is cut off and the tumor cells starve to death, dying through necrosis. This is why TNF is called "tumor necrosis factor", because it was first observed killing tumor cells through necrosis. There is some evidence that hyperactivated macrophages and NK cells can kill cancer cells; in fact, treatments for some cancers (eg bladder cancer) involves injecting the tumor with a weak cousin of tuberculosis, called BCG (ie bacille Calmette-Gurein), to cause the tumor to fill up with hyperactivated macrophages that kill the tumor cells. Whereas CTLs recognize specific peptide antigens and can therefore be easily evaded if those antigens are changed through mutation (and cancer cells have extremely high rates of mutation, which is essential to their evasion of the immune system), macrophages and NK cells recognize broad categories of foreign antigens and so are resistant to deception by single mutations.  
____________________________________________________________________________________________________
-Dollar cost averaging: A risk-averse investing strategy that alters the way an investor allocates money in the stock market in order to minimize the impact of volatility. It's main principle is dividing up the capital to be invested into uniform chunks and investing each chunk at regular time intervals (eg investing $10,000 by putting $1,000 into the market every week for 10 weeks). This is to account for the risk that if all the money is invested at once, right before the market falls (due to generally unpredictable volatility), the investor will realize large losses. Spreading the money out over time minimizes the risk, since only part of the total sum of money to be invested is exposed to any reasonable risk of market decline, and will probably be balanced out be market upswings (due to volatility) in the long run anyways. 
-Ordinal numbers: A well-ordered set is a set for which every non-empty subset (including itself) has a least (ie smallest) element; the ordinal numbers are the numbers that measure the "length" of well-ordered sets. The ordinals satisfy three defining properties: (1) There exists a "first" ordinal, (2) Each ordinal has a successor, ie another ordinal that comes "next", (3) For every (finite or infinite) set of ordinals, there exists an ordinal that succeeds every ordinal in the set.
    -The first ordinals are simply the natural numbers, ie 0, 1, 2, 3, ... and the ordinal that succeeds all of the naturals is denoted "omega". It should be noted that omega, and all ordinals that succeed omega, are not natural numbers. Thus the set of ordinal numbers (properly) contains the natural numbers (the natural numbers are all the finite ordinals). The ordinal after omega is denoted "omega + 1", but this is simply an abuse of notation and is not actually a sum in any sense. Omega is the length of the set {0, 1, 2, ...}, omega + 1 is the length of A, where A is the union of the natural numbers and another element a defined such that a is the successor of all naturals. Omega + 1 could also be thought of as the length of the set {0, 1, 2, ..., omega}. Similarly, omega + 2 is defined as the length of B, where B is the union of A and another element b defined such that b is the successor of all elements in A, and so on. The ordinal that succeeds omega + x for all natural numbers x is denoted "omega + omega", or "omega * 2" (note that with infinite ordinals, multiplication is not commutative (2 * omega = omega, whereas omega * 2 is defined as aforementioned)); after that comes (omega * 2 + 1), and so on until "omega * 3", and so on until "omega * omega" or "omega^2", and so on until "omega^omega", and so on until "omega^omega^omega^(...)". We denote the first ordinal number that succeeds any natural number tetration of omega (ie omega^omega^(...) a finite number of times) by "epsilon_0". 
-Cardinal numbers: In contrast to ordinal numbers, cardinal numbers measure the size of a set, rather than the length. Every ordinal between omega and epsilon_0 has the same size, namely, aleph_0 (ie the size of the natural numbers and the first infinite cardinal). Every natural number is also a cardinal number; indeed, the naturals are the only finite cardinals. The cardinals' notation is such that they are labeled by the ordinals, ie the cardinals can be written {aleph_0, aleph_1, aleph_2, ..., aleph_omega, aleph_(omega + 1), ..., aleph_(epsilon_0), ..., aleph_(aleph_0), aleph_(aleph_(aleph_0)), ...}. The first cardinal number to succeed all of the above written cardinals is denoted "kappa", and is equivalent to "aleph_kappa", ie the cardinal whose size is itself.
-Universal edibility test: A rudimentary test applied to plants to gauge whether or not the plants can be safely consumed for sustenance. The test is designed to be performed in situations of no training or equipment.
    -Test:
        1. Test only one part of the plant at a time.
        2. Separate the plant into its basic components - leaves, stems, buds, and flowers.
        3. Make sure the plant parts do not have an acidic odor.
        4. Perform the test on an empty stomach (ie not having eaten for 8 hours prior), and take nothing through the mouth other than (purified) water.
        5. During the 8 hour fasting period, test for contact poisoning by placing a small part of the plant component on the inside of the elbow or wrist. Wait 15 minutes for a possible reaction in the skin (eg redness, itchiness, etc).
        5. Select a small portion of the plant and prepare it for consumption in whatever way is appropriate.
        6. Touch a small portion of the prepared plant component to the outer surface of the lip and watch out for burning or itching. Wait 3 minutes for the reaction.
        7. If there is no reaction, hold the plant part on the tongue for 15 minutes. If there is no reaction after 15 minutes, thoroughly chew the plant part and hold it in the mouth for another 15 minutes, without swallowing. If no reaction (eg burning, itching, numbing, stringing, etc) occurs, swallow.
        8. If any ill effects are present in the following 8 hours, induce vomiting and drink lots of (purified) water.
        9. If no ill effects have occurred thus far, the plant is safe for eating.
-Dirichlet function: The Dirichlet function is the indicator function for the irrationals; it is a real function f such that f(x) = 1 if x is rational and 0 otherwise. It turns out that this useful function has a closed analytic form: f(x) = limit as m approaches infinity of (limit as n approaches infinity of (cos(m!*pi*x))^(2n)). To see why the function (cos(m!*pi*x))^(2n) would converge to an indicator function, consider (cos(pi*x))^2. If x is an integer, then the cosine function will be evaluated at an integer multiple of pi, meaning it will be 1 or -1, and squaring the function guarantees that the value is 1. If x is not an integer, then the function will have some value between 0 and 1, since cosine is bounded between -1 and 1. Thus, if we raise the function to another power, ie ((cos(pi*x))^2)^n, then if x is an integer the expression reduces to 1^n, and if x is not an integer, the function reduces to a^n, where a = (cos(pi*x))^2 and 0 < a < 1. Clearly, if we now take the limit as n approaches infinity, 1^n = 1 and a^n = 0, and so the function is now an indicator function for the integers. If we multiply the argument of the cosine by m!, then if x is a rational it can be written as p/q for integers p and q, and if m > q, m! = m * (m - 1) * (m - 1) * ... * (q + 1) * (q) * (q - 1) * ... * 2 * 1, and so m!/q will be an integer, which means p * (m!/q) = m! * x is also an integer, which means the cosine if m! * x * pi will be 1. Thus if we take the limit as m approaches infinity, thus guaranteeing that m > q for any q, we have transformed the function into an indicator function for the rationals.
-Microfluid electrospray propulsion: (ie MEP) A proposed form of propelling very small spacecraft. There are many advantages to using spacecraft that are only a few centimeters in size to using conventional and very large spacecrafts, since they would be extremely cheap to both produce and put into orbit, would be able to be deployed in swarms of thousands at a time, and would be individually expendable and thus be able to be sent into very hostile environments. Such spacecraft would be equipped with sophisticated sensors and gathering information as they explore. MEP is one theorized methodology currently being researched as a way to efficiently propel small spacecraft. The methodology takes advantage of the small size of the spacecraft and uses normally negligible forces that become relevant on small scales, such as capillary action and surface tension. MEP is an ion thruster that generates (so far, between 20 - 100 micronewtons) thrust by ejecting ions. The engine is made up of many tiny needles that are coated with a metal that has a low melting point, usually indium. A metallic grid is placed above the needles in order to generate an electric field between the needles and the grid. The grid is then heated, causing the indium to melt; capillary action and the electric field's forces draw the indium up towards the grid, while the indium's own surface tension try to keep it together and cohesive. This discrepancy in forces causes the indium to instead remain on the bottom part of the engine while slowly climbing up the needles, thus deforming the liquid indium into small needle like arrays. The tiny tips of the needles accumulate tiny amounts of liquid indium, and since the tips of the needles have such low surface area, the surface tension is low enough, only on the tips of the needles, for the electric field to overcome the force of surface tension and begin to tug indium ions off the needle and eject them, creating thrust. This process is very fuel efficient. 
-Coherent extrapolated volition: 
    -Premise: A major theoretical problem in the field of artificial intelligence arises under the hypothetical (yet very likely in the near future) scenario of an artificial super-intelligence (ie ASI) that outmatches the entire human race. Such a machine would be extremely complex, but would nonetheless "think" and reason computationally and in accordance with extremely logical problem solving, a thought process that is much different from most humans'. Most goals, when given to an ASI, could result in undesirable side effects that technically meet the goal criteria but which were completely unintended; the prevalence of these unintended consequences that nonetheless apply to the criteria of the goal is because when humans solve problems, they do so under an unconscious framework built with many assumptions, such as morality. For example, an ASI's solution to the goal "end world hunger" might be to kill all humans, effectively ending hunger and yet being drastically different from the inputted goal's true intent. Another example would be an ASI's solution to the problem "gather solar power efficiently", whereby the ASI begins dismantling large chunks of the earth in order to use them for raw materials to build solar panels. Clearly, a morality code is needed to be a part of the ASI's programming, which would take precedence over any reasoning or problem solving. The idea of coherent extrapolated volition is that instead of a machine asking humans for goals and carrying them out, which could lead to disastrous consequences as seen in the examples above, the machine would analyze the inputted goal and synthesize the analysis with its (extensive) knowledge of human psychology and neurobiology to, instead of carrying out the goal, figure out what the human's true intent was in phrasing the goal as they did and then carrying out that intent instead of the goal. Returning to the above example, instead of ending world hunger by killing any hunger humans, the ASI would analyze the goal "end world hunger" and estimate that the human's intent with this goal is to effect a global scenario, which was reached through means that abide by a complex moral code that humans subscribe to, in which there is an abundance of physical and mental sustenance for the entire human race that leaves them in an optimal state of control and utility.
    -Theory: First developed by Eliezer Yudkowsky, the argument is that it is destructive and insufficient to simply program our desires into an ASI; rather, it is necessary and optimal to program an ASI that acts in humanity's best interests; a machine that does what humans want it to do instead of what they tell it to do. The idea of "volition" is a mental concept separate from a decision. A decision or choice is an action taken by someone, but that person's volition is more akin to their intent - what they wanted to achieve with their decision, regardless of whether or not they did. For example, if someone is faced with two boxes, A and B, one of which containing one million dollars, and the person chooses box A only to see that it is empty, it was that person's decision to choose box  A but it was his volition to choose box B, which contained the money. The exact wording used to describe how an ASI would act in our best interests is to automatically predict our desires if we were idealized versions of ourselves - "if we knew more, thought faster, were more the people we wished we were, had grown up further together".
-De Beers Group: The world leader in diamond mining and retail. The company is vertically integrated and operates in every category of diamond mining and sales. De Beers sells approximately 35% of all diamond production in the world. The company has had a monopoly on the diamond industry since Ernest Oppenheimer, who was elected to the board of directors in 1926, consolidated the monopoly. De Beers is claimed to be responsible for instilling the idea that an engagement for marriage must be accompanied by an engagement ring in the population, as the result of a very successful advertising campaign in the early 1900s.
-Matrioshka brain: A hypothesized megastructure that is closely related to the concept of a Dyson sphere. The structure is essentially an immense computational device; it's structure would be a series of concentric spherical shells centered at the sun (ie Dyson spheres), using the sun's heat and radiation for power. Each shell would be built completely out of material that had been engineered at the atomic level for computation, so called "computronium". Each layer would absorb power, use it, and re-radiate it outwards for the next layer to use; thus, the innermost shells would be almost as hot as the core star itself, but if sufficiently large, the outermost shells would be almost as cold as the interstellar medium. Although theoretically possible under the currently understood laws of physics, such a structure has immense engineering challenges, most chiefly acquiring enough raw material for construction; the structure would require most or all of the mass in the solar system, excluding that of the sun. Uses for such a computational system are limitless, but a common proposed one is the run simulations of our universe or to upload the minds of humanity.
-Paradox of plenty: (A.K.A. resource curse) The apparent paradox that countries with large amounts of natural resources (eg sub-Saharan Africa), such as minerals, oil, etc., tend to undergo slower economic growth than countries with fewer natural resources. The idea that natural resources were a curse instead of a blessing to countries was put forward in the 1980s, and since then, many studies (eg Jeffrey Sachs, Andrew Warner) have established a negative correlative relationship between the abundance of natural resources and economic growth. 
    -Causes: (***INCOMPLETE***)
    -Dutch disease: (***INCOMPLETE***)
-Quantum immortality: Quantum suicide and quantum immortality are two names assumed by a particular thought experiment in quantum mechanics that attempts to determine whether the many-worlds interpretation or the Copenhagen interpretation of quantum mechanics is true, from the perspective of the experimenter. The experiment is as follows: suppose an experimenter sets up an experiment in which there are a large (for our purposes, infinite) number of protons available. Every second, the spin (which can either be up or down, with probability 1/2) of a new proton is measured; if the spin is up, nothing happens, but if the spin is down, the experimenter is instantly killed (perhaps by a nuclear explosion). If the many-world's interpretation is true, then no matter how many seconds pass and how many protons are measured, there exists a universe in which the experimenter is still alive. For example, the probability of the experimenter being alive after 100 seconds is (1/2)^100 = 7.886 * 10^(-31), but nonetheless there exists a universe in which this is true. If the experimenter is killed in a particular universe, he obviously cannot experience death (assuming that there is no afterlife), and so since there is always a universe in which the experimenter is still alive, the experimenter can never experience death; he is, from his perspective, immortal.
    -Copenhagen vs Many-Worlds interpretation: The Copenhagen interpretation of quantum mechanics, which models various state properties (eg position, momentum, etc.) of particles (eg electrons) with probability, is that before being measured, those state variables of a particle are all true at the same time, and it is only when the particle is measured that the wavefunction collapses and one precise value of those state variables is assumed, probabilistically. For example, before an electron is measured, its position is a essentially a random variable; the Copenhagen interpretation interprets this as the electron being in every possible position at the same time; it is only when the electron is measured that all the probabilities collapse to a single position. The many-worlds interpretation posits something different, and in fact makes statements about the nature of probability itself. It interprets the probabilistic nature of the state variables of a particle as each branching out to a separate "universe", eg for each possible position an electron could be in, there is a specific universe in which that position is realized. It is when we measure the electron's position that we know which universe we are in; the wavefunction collapse never actually happens.
-Impact bias: In affective forecasting (one's prediction of one's own emotional state in the future), impact bias is the tendency to overestimate the length or intensity of a particular emotional state they imagine themselves in in the future.
-Saccadic masking: A phenomenon in which the brain blocks certain portions of visual perception when the eye moves, so the movement of the eye and gap in visual perception is not noticed by the person. This is why even when someone moves his eyes, he can never observe them moving himself, even in front of a mirror. When the eye moves rapidly, the retina blurs during movement, and since blurry vision is not useful, the brain blocks it out, effectively blinding the person.
-EM Drive: (A.K.A. RD resonant cavity thruster) (***INCOMPLETE***)
-Computablity: A Turing machine is a set of rules for erasing and writing binary symbols on infinitely long, one-dimensional tape which is partitioned into squares, one per symbol, and Turing proved that for any computer program or algorithm or otherwise, there existed a Turing machine that could perform exactly the same thing. Such functions, whose outputs at given inputs are able to be evaluated by a Turing machine (and therefore, any equivalent, including every modern computer and laptop, pen and paper, etc.) are known as computable functions, specifically, Turing-computable functions. The idea of a computable function is closely related to the formalized version of an algorithm. Other models of computation that are equivalent to Turing-computable functions are mu-recursive functions, deterministic as well as non-deterministic finite state automata, lambda calculus, pushdown automata, combinatory logic, etc. The Church-Turing thesis states that any function over the natural numbers is "computable" in the informal, everyday sense of the word (ie a person with enough pen and paper could work it out) if and only if the function is computable by a Turing machine (ie is Turing-computable). The set of Turing-computable functions is countable, whereas the set of non-Turing-computable functions is uncountable.
    -Halting problem: Turing proved, using self-reference, that the halting problem is not computable. The halting problem queries the existence of a function which, given any program or function, and an input for that program, returns whether or not the inputted function halts, ie terminates (as opposed to running forever, eg in an infinite loop) on the specified input to give to that function.
        -Proof that the halting problem is uncomputable: Turing's original proof is a proof by contradiction. Assume such a function, H(P, x), existed, such that P is a program (which can be represented as a finite string of 1s and 0s, ie a bitstring), and x is some input (which can also be represented as a finite bitstring), and H outputs 1 if P(x) halts and 0 otherwise (notice that technically H is a function over the naturals, since both inputs can be represented as bitstrings, which can in turn be represented (uniquely) as natural numbers). Since H exists, define a function f(P), where P is a program as follows: 
        f(P) = 0 if H(P, P) = 0, and f(P) loops forever otherwise. 
        Thus, f halts if P(P) does not halt, and if P(P) does halt, then f does not halt. Such a function is perfectly well defined, since we assumed H(P, x) exists.However, now consider the behavior of f(f). f(f) will halt if H(f, f) = 0, that is if f(f) does not halt, and f(f) will loop forever if H(f, f) = 1, ie if f(f) does halt. This is a contradiction: we have essentially shown that f(f) halts precisely if f(f) does not halt, and f(f) does not halt only if f(f) halts. Therefore, f cannot exist, and since f is built completely from H, H cannot exist either. Therefore, the halting problem is uncomputable.
    -Busy beaver sequence: There are a finite number of arbitrary Turing machines which conform to exactly n rules, similar to how there are a finite number of words which have exactly n letters. Some of those functions halt, and others do not. Because this collection of functions is finite, it follows that for every natural number n, there exists a Turing machine that runs for the largest number of steps but still halts, ie there is a Turing machine which, subject to the constraint of conforming to exactly n rules, runs "as long as possible" without running infinitely long, where "as long as possible" is quantified by the number of symbols the Turing machine writes on the tape. This idea was first put forward by Tibor Rado, who proceeded to define the Busy Beaver sequence, with the n-th Busy Beaver number being the maximum number of operations (ie erasing and writing to tape) a Turing machine subject to n rules can perform while still halting eventually. However, the Busy Beaver function, denoted BB(n) and which maps a natural number n to the n-th Busy Beaver number, is uncomputable, since if it were, we could use it to solve the halting problem by simply running a program with m steps and checking if it terminated or not after BB(m) operations - if it had, it halted, and if it hadn't, then it would never halt. Since the halting problem is uncomputable, so if BB. 
        -Time complexity: An interesting property of BB is that the sequence grows (referring to big-O notation for order of growth) faster than any computable function, because if we could compute a sequence D such that D(n) > BB(n), we could solve the halting problem by waiting for a program to run D(n) operations and seeing if it halts or not. Thus, any computable sequence in existence must grow slower than BB(n). Thus, intuitively, the BB function is uncomputable because it grows to fast for any computer of any kind to keep up with it.
        -First few values: BB(1) = 1, BB(2) = 6, BB(3) = 21, BB(4) = 107. We don't know the precise size of BB(5), but in 1984 it was shown that BB(5) was at least 2,133,492, an estimate which was revised to BB(5) > 47,176,870 in 1989, and then to BB(5) > 8,690,333,381,690,951 in 1997. 
        -Extensions with super-Turing computability: Given a hypothetical super-Turing machine, which is a Turing machine with access to an oracle which can solve the halting problem, a new problem arises: how to tell if the super-Turing machine halts on a given input? This give rise to a "higher order" halting problem, and even if we define a super-Turing machine of order 2, which can solve the order 2 halting problem, the problem of whether it itself can halt is still uncomputable, and so no matter how high up in the hierarchy, known as Kleene's hierarchy, one goes, one can never have a fully complete Turing machine. Each order in that hierarchy defines a new Busy Beaver sequence, adapted for the ability of super-Turing machines on that order to solve the halting problems of all orders below itself; the Busy Beaver sequence of order m, denoted BB_m, grows faster than all the busy beaver sequences below it, ie for all natural numbers m > 0, O(BB_m) > O(BB_(m - 1)). In fact, even if there existed a super-Turing machine that could compute all BB_m and below sequences (which it could, if it could solve the halting problem, and vice versa), BB_(m + 1) would still be uncomputable.
    -Hypercomputation: (A.K.A. super-Turing computation) A hypothetical and impossible model of computation in which computational machines are used which are granted access to restrictions on Turing machines and their equivalent, such as access to an oracle that can solve the halting problem, a Turing machine which can complete infinitely many operations (ie a Zeno machine), real computers (see below), a quantum mechanical computer using an infinite number of superpositions, etc.
    -Computable number: When doing arithmetical manipulations of numbers, a computer is not really performing rigorous, mathematical operations, in the sense that it is not using the abstract and conceptual entities in the set of real numbers; rather it is only possible for a computer to work with approximations of the reals. Many real numbers (uncountably many, in fact; they are the irrational numbers) do not have a finite decimal expansion, and so it follows that they do not have a finite numerical expansion in any rational number base (the use of irrational number bases falls prey to the problem of computing the base itself, as well as other problems). Because a computer needs to be able to perform arithmetic for any arbitrary numbers, it must follow some algorithm for how to manipulate numerical symbols, since there is no finite computable representation of all real numbers. It follows that a computer can only work with "computable numbers", which are defined as real numbers which can be approximated by an algorithm to arbitrary precision, which usually means in computers that one can work out an arbitrary number of digits of the number in base-10 following an algorithm, such as pi. 
        -Size: The set of computable numbers is countable. Since the reals are uncountable, it follows that almost all real numbers are not computable and so are restricted from a computer. 
        -Real computation: A theoretical hypercomputer which is able to work with infinite-precision real numbers instead of simply the computable numbers. Such hypothetical computers, which cannot exist in real life, could be used to do things that normal computers cannot. One could use a real computer to solve NP-complete problems in polynomial time.It is not possible to have a computer which uses infinite-precision numbers due to the Beckenstein bound (see below).
            -Beckenstein bound: An upper bound on the amount of information entropy that can be contained within a finite region with a finite amount of energy; infinite-precision numbers would thus require infinite amounts of energy to store in finite space. The bound also implies the existence of a maximum information-processing rate, which is Bremermann's limit. 
            -Equation: S = (2 * pi * k * R * E) / (h-bar * c), where S is the entropy, k = Boltzmann constant, R = radius of sphere in which information is being stored, E = total mass-energy of system (including rest mass), h-bar = reduced Plank constant = h / (2*pi), c = speed of light.
-Goldbach conjecture: Every even number greater than 4 can be expressed as the sum of two primes. The conjecture was put forward in 1742 and has not been able to be proven or disproven since. 
-Dijkstra's algorithm: One of the fastest known shortest path algorithms. Given a weighted directed graphs (weights representing costs; weights are non-negative) and a source node, the algorithm finds the shortest path to every other node in the graph, producing a shortest path tree. In other words, the algorithm takes a graph and returns the subgraph with the lowest total cost of traversal from the source node to any other node in the original graph. The algorithm uses a data structure known as a priority queue. The algorithm runs in time complexity O(E + V log(V)), where E = number of edges in the graph, V = number of vertices (ie nodes). The algorithm is guaranteed to find the shortest path tree.
    -Priority queue: A data structure that stores elements under the condition that the set of elements that can be stored must have a defined partial order. A priority queue is a type of queue (first in, first out structure) with the modification that each element has an associated priority with it. Rather than "popping" (removing and returning) the first element inserted (as a regular queue would do), the element with the highest priority is popped, thus imposing an ordering on the elements. If two elements have the same priority, then they are ordered as a regular queue would order them (the higher element was the one that was inserted first).
    -Algorithm description: Dijkstra's algorithm is a greedy algorithm, because it follows the following line of intuitive reasoning. Starting at the source, consider all the child nodes that one can travel to directly from the source (ie the source's neighbors). Assign each child node's distance as the weight of the edge between it and the source. Then, move on from the source to the node with the smallest distance associated with it and repeat the process on that node's children. Continue in this fashion, updating every node's distances (from the source) as one moves on by leaving a node's associated distance untouched if it's smaller than the distance it would take to travel to the current node and then to that node, and otherwise if the sum of the current node's associated distance and the distance from the current node to the child node is greater than the distance associated with the child node alone, then update it to the sum.
        -Algorithm: To find the shortest path tree given a source vertex, perform the following algorithm. To find the shortest path between one vertex, the source, and another vertex, the target, perform the following algorithm with the addition of a set of visited vertices, adding a vertex to this set whenever the current node is set to the vertex, and halt the algorithm once the destination node has been marked visited.
            1. Initialize an empty shortest path tree.
            2. Associate each vertex with a "cost", ie a non-negative number. Initialize the source's cost to 0 and initialize the cost of every other vertex to infinity.
            3. Initialize an empty priority queue to store graph vertices. The priority of one vertex is higher than that of another if it has a lower cost.
            4. Set the current node to be the source node.
            5. do:
                    a. For each child node of the current node:
                        i. Compute a tentative cost = current node's cost + weight of the edge from current node to child node.
                        ii. If the tentative cost is lower than the child node's cost:
                            1. Update the child node's cost to the tentative node's cost.
                            2. Add the edge from the current node to the child node to the shortest path tree.
                        iii. Add the child node to the priority queue.
                    b. Pop the element with the highest priority from the priority queue and make it the new current node.
                while the priority queue is not empty.
            6. Return the shortest path tree.
    -A* algorithm: Dijkstra's algorithm can be inefficient in practical applications (eg Google Maps) where one wants the shortest path between a source and a destination, and often times the two paths can be very far away. For example, to find the shortest path between Denver and New York City, Dijkstra's algorithm will explore every location within a radius of 2,000 miles of Denver before finding New York, as it builds the shortest path tree, even though the overwhelming majority of the information in the shortest path tree is completely unnecessary. The A* algorithm is an extension of Dijkstra's algorithm and is more commonly used in practice. It models that intuition that rather than exploring a radius of paths from Denver, we should, in general, explore eastwards and completely avoid the west side of the country entirely in our search. The algorithm is based on the idea that rather than assigning costs to vertices in the way that Dijkstra's algorithm does, which is the shortest distance from that node to the source node, the assigned costs should also take into account some heuristic guess of the distance between the node and the destination node as well. Thus, if g(v) is the estimated distance from the source node to node v, then Dijkstra's algorithm uses Total Cost = g(v), whereas A* uses Total Cost = g(v) + h(v), where h(v) is a heuristic estimate of the distance from v to the destination node. It follows that Dijkstra's algorithm is a special case of A*, where h(v) = 0 for all v (ie no estimate is made). 
        -Heuristic function: The heuristic used does not need to be perfect and is only an approximation, though it should generally get better over time as it learns from experience. A common heuristic in practical applications is to simply use standard as-the-crow-flies distance from a vertex to the destination (note that this is different from standard Euclidean distance because the surface of the Earth is curved).
            -Admissibility: A heuristic is said to be admissible if, for any vertex, it never overestimates the distance to the destination. This definition is motivated by the fact that A* always finds the shortest path between two vertices if the heuristic used is admissible.
-Israel-Palestine conflict: (***INCOMPLETE***)
-World Wars: (***INCOMPLETE***)
    -World War 1: (***INCOMPLETE***)
    -World War 2: (***INCOMPLETE***)
-Rat Park experiment: An experiment in the theory of drug addiction that took place by Bruce Alexander at Simon Fraser University in the late 1970s. At the time, there was a lot of (and still is) public stigma against drug use and addiction, mainly due to a series of studies that seemed to demonstrate the harms of drug addiction. The highly prevalent series of studies placed rats in cages with two sources of water - one normal and one laced with heroin. Almost every rat chose to drink the heroin-laced water until it died, neglecting even basic needs such as food. This was interpreted to mean that drug addiction was a swift and inevitable consequence of drug use, and always led to the destruction of the user's life. The Rat Park experiment was a reaction to these series of studies, and had the hypothesis that the trend of rats choosing heroin-laced water was caused not, at least completely, by the addictive properties of heroin itself, but rather the extremely sub-optimal conditions in which the rats were housed, which put the rats in severely depressed mental states that urged heroin use as a coping mechanism. An implication of this hypothesis is that rats, and potentially by extension humans, wouldn't excessively use or become addicted to heroin if the environments they were in were optimal, containing many of the basic necessities of most mammals, such as plentiful nutritious food, space to roam around in, the company of other fellow rats, sex, etc. Specifically, the hypothesis was that excessive drug use is due to the drug acting as a coping mechanism to individuals for whom even basic needs on Maslow's hierarchy of needs aren't satisfied, rather than due to the drug's addictive qualities themselves.
    -Experiment: The basic experimental design was the same, with two water drips - one normal and one laced with heroin. A control group of rats were placed in cages, in identical conditions as the original experiments, and the experimental group was placed in "Rat Park", an enclosure specifically constructed to be optimal for a colony of rats, containing leaves, grass, and other natural materials, lots of open space and other rats, etc. Halfway through the experiment, some rats were moved from cages to Rat Park, and some rats were moved from Rat Park to the cages. As in the previous experiments, the caged rats became addicted to the morphine-laced water very quickly. The rats in Rat Park tried the morphine-laced water 19 times less often, and showed a statistically significant preference for normal water. Rats who were raised in cages but then moved to Rat Park also tended to prefer normal water.
        -In another experiment, rats who were forced to drink morphine-laced water for almost two months and were then moved to Rat Park showed signs of dependence on the morphine, but nonetheless chose the normal water over the morphine-laced water.
    -Reception: At the time, the two major science journals, "Science" and "Nature", both rejected the paper, which was instead published in "Psychopharmacology" in 1978. Two studies that followed ("Environment is not the most important variable in determining oral morphine consumption in Wistar rats." (http://prx.sagepub.com/content/78/2/391.full.pdf) and "Influence of housing conditions on the acquisition of intravenous heroin and cocaine self-administration in rats.") failed to replicate the results, the first finding that rats in both cages and Rat Park showed decreased preference for morphine (perhaps due to a genetic difference) while the second found that although Rat Park rats were much slower to self-administer morphine, by the fifth week of testing, the amount by which both caged and Rat Park rats used morphine was the same.
-Agonist: A chemical that binds to a biological receptor on a cell, and activates the receptor. This initializes a signal transduction which ultimately leads to a biological response.
    -Antagonist: (A.K.A. blockers) The opposite of an agonist - a chemical that binds to receptors and has the effect of dampening or inhibiting the biological response that would occur by the corresponding agonist. Non-inverse antagonists have only affinity for receptors, and no efficacy - this means that antagonists can bind to receptors, but can't induce any biochemical responses. Depending on how high the affinity of an antagonist is to the receptor, antagonist activity may be either reversible or irreversible.
        -Types:
            -Inverse antagonist: An antagonist that bonds to receptors and induces the opposite biological response that the corresponding agonist would.
            -Neutral antagonist: An antagonist that has no biological effect in the absence of the corresponding agonist or an inverse antagonist, but can block the effects of either when they're present.
            -Competitive antagonist: Antagonists that block the effect of agonists by binding to the agonist's receptors but without activating the receptor, thereby preventing the agonist from binding to the receptor and subsequently inducing the resulting biological response. Competitive antagonists don't induce any signal transduction pathway at all (they simply block agonist ligands from binding to receptors by physically situating the binding site) and thus don't activate the receptor - therefore, all competitive antagonists are neutral antagonists.
        -Blockers:
            -Alpha-blocker: Either a neutral or inverse antagonist to alpha-adrenergic receptors (G protein-coupled receptors) typically activated by norepinephrine and epinephrine, which are heavily involved in the sympathetic nervous system and the fight-or-flight response.
            -Beta blocker: Competitive antagonists that block the binding sites of adrenergic beta receptors (the particular epinephrine and norepinephrine receptors that are, when the fight-or-flight response is activated by the sympathetic nervous system, specifically responsible for inducing increased cardiac output (this is achieved by increasing heart rate, increasing the heart's conduction velocity (the speed at which an electrochemical impulse propagates down a pathway), and increasing stroke volume (the volume of blood pumped from the left ventricle of the heart per beat))). As drugs, they are commonly used to manage cardiac arrhythmias (irregular, too fast (tachycardia, defined as over 100 beats per minute in adults), or too slow (bradycardia, defined as below 60 beats per minute) heartbeat) or treat hypertension.
    -Types:
        -Endogenous / exogenous: Antagonists produced naturally by the body, as opposed to exogenous agonists, which are externally administered, such as drugs.
        -Superagonist: An exogenous agonist capable of producing a maximal response (the largest possible biochemical response) greater than the natural, endogenous agonist for the corresponding receptor. Thus, superagonists are more efficient are invoking a biochemical response from a receptor than the body's natural means for invoking that response, and have an efficacy higher than 100%.
        -Full / partial agonists: Full agonists are exogenous agonists that have the same efficacy (ie 100%) as the natural, endogenous agonist for a receptor. Partial agonists, in contrast, have efficacy lower than 100% and thus induce only a partial biochemical response.
        -Irreversible agonist: An agonist that binds with permanent covalent bonds to a receptor, producing a (semi-)permanent biochemical response.
    -Potency: The potency of a pharmacological drug is a measure of, informally, how powerful a drug is; this is quantified by how low of a concentration of the drug is required to produce an effect of given intensity. Very potent drugs are able to produce intense macroscopic effects even at very low concentrations, while drugs with low potency would require much higher dosages or concentrations to reach the same level of intensity. Potency is dependent on the affinity and efficacy of the drug.
        -Affinity: How tightly or strongly a ligand binds to a protein receptor. This is directly proportional to the strength of the intermolecular forces between the ligand and the protein. The strength of the bond between a ligand and a receptor influences how long the bond will continue and thereby controls the duration of the induced biological effects, but does not influence that strength or intensity of the biological response itself, which instead depends on the signal transduction pathway set in motion by the bond.
        -Efficacy: This is the intensity or magnitude of the biological response induced by a ligand binding to a protein receptor, as opposed to the strength of the binding itself. Agonists of lower efficacy are less efficient are thus provoking the intended biological response.
-Opioid receptors: A class of neural (inhibitory, 7-transmembrane (passes through the cell membrane seven times), G protein coupled) receptors that are widely found throughout the brain and play a large role in the brain's reward pathways. Opioid receptors have been widely linked to pain relief and euphoria.
    -Types: The major sub-families of opioid receptors are the delta, kappa, and mu receptors. Each has distinct functions.
        -Delta-opioid receptor: An opioid receptor with enkephalins (polypeptides that are involved in regulating pain and nocioception) as ligands. In humans, delta-opioid receptors are most heavily concentrated in the basal ganglia and neocortex, suggesting that delta-opioid receptors are linked to regulation of mood and emotion; specifically, the regulation of depressive disorder.
            -Functions: Analgesia (pain relief), antidepressant
        -Kappa-opioid receptor: Opioid receptors with dynorphin (opioid peptides with very high potency involved in analgesia) as its ligand. This receptor is involved in regulating pain, consciousness, motor control, and mood.
            -Functions: analgesia, anticonvulsant effects, depression, dysphoria (an intense state of discomfort and unease; the opposite of euphoria) etc.
        -Mu-opioid receptor: Receptors with high affinity for enkephalins and beta-endorphins and low affinity for dynorphins. This receptor has the highest efficacy in terms on pain relief and euphoria, and the most powerful opioids are mu-opioids, as they cause activation of central dopamine reward pathways.
            -Functions: Analgesia, respiratory depression, euphoria, vasodilation.
    -Naloxone: A strong opioid antagonist that's commonly used to block or counteract the effects of opioids. They are competitive antagonists with very high affinity for opioid receptors. They tend to induce opioid withdrawal in people with opioids in their system, but have been widely used in overdose situations to save lives.
-Kessler syndrome: First proposed by NASA engineer Donald Kessler in 1978, the Kessler effect or syndrome is a hypothetical positive feedback loop that could occur in outer space in which the amount of space debris gets so high that it hits more and more active satellites in orbit, creating more space debris, and thereby taking more satellites out of commission, in a positive feedback loop with exponentially growing amounts of space debris. This could happen if the density of space debris in lower earth orbit becomes high enough. There are about 17,000 objects in orbit that are tracked, 7% of which are active satellites (the rest being debris), but because smaller objects are not tracked, the true number is closer to 150,000 - 500,000 for objects between 1 and 10 centimeters, and one million for objects above 1 millimeter. Although such objects are tiny, they orbit are such great velocities (in the neighborhood of 17,000 miles per hour) that even a tiny collision would have as much impact as an exploding hand grenade.
-Principle of maximum entropy: A concise and complete definition of the principle's statement is that the probability distribution representing the current state of knowledge (formalized by Bayesian prior data) is the one with largest entropy.
    -Shannon entropy: The term "entropy" here refers to the information theoretic definition of the term, known as Shannon entropy. Shannon entropy is meant to serve as a means for quantifying the amount of information encoded in a message or event. More precisely, it's a way of measuring the amount of information provided by a random variable, and varies inversely with the amount of information; thus, Shannon entropy can be conceptualized as precisely measuring the amount of uncertainty in a random variable. The Shannon entropy H of a random variable X is defined as H(x) := sum over all outcomes x of (Pr(x) * log(1 / Pr(x))). This definition fits our intuitions about the information provided by a random variable well - namely, if the random variable is uniform, so that every event has the same probability, then the Shannon entropy is maximized, as X provides us with zero information. Alternatively, if X is deterministic, meaning one outcome has probability 1, then the Shannon entropy is minimized. H also has the natural and appealing properties that it's continuous, and also conforms to our expectation that if two independent events occur together, our uncertainty about them should be additive. It turns out that H is, up to a constant factor, the only function that satisfies these properties.
        -Intuition: Intuitively, the function H(X) = sum over all outcomes x of (Pr(x) * log(1 / P(x))) is essentially the expected value of the quantity -log(P(x)), which, if we use the very common logarithm with base 2, can be seen as the minimum number of bits required to encode the event x (since if we expand the quantity n := log(1 / P(x)) out, we find that n is the number (1/2)^n = x).
-Efficient market hypothesis: The theory that the market prices of economic resources fully reflects all available information in the economy. In other words, financial instruments always trade at their true, fair, market value. An implication of this view is that it's impossible to consistently beat the market, and profit can only be made by trading financial instruments through chance. A 2012 study by Eugene Fama, who developed the hypothesis, and Kenneth French showed that the distribution of abnormal stock returns among US mutual funds was not statistically different from chance. The theory depends on the rational expectations assumption (which states that the population of an economy, on average, makes rational predictions about the economy based on the information they have). A necessary implication of this theory is that investors' actions should appear more or less random; more specifically, they ought to reflect a Gaussian distribution.
    -Weak form: The most general form of the hypothesis, which states that asset prices reflect all publicly available past information.
    -Semi-strong form: A stronger version of the hypothesis which states that asset prices reflect both publicly available past and current information.
    -Strong form: A form of the hypothesis which states that asset prices reflect all information, past or present, public or hidden, in the market.
-Solomonoff induction: An algorithm for universal inference. Specifically, given a computable sequence of binary numbers, Solomonoff induction can, in theory, be used to predict the next bit of the sequence. Moreover, it will actually learn the underlying program generating the sequence by converging to it the more information about the sequence (i.e. the more bits of the sequence) its given. Solomonoff induction is provably optimal in this regard, converging as fast (dependent on the amount of the sequence given, i.e. the length of the current sequence) as possible. This is, of course, an extremely (perhaps the most) fundamental topic in artificial intelligence and information theory, since, as far as we know, every possible event in the universe can be encoded in a bitstring. In this sense, Solomonoff induction is often seen as a "universal truth finding machine". Because it's predicated on Occam's razor, but formalized and applied, it's sometimes known as Solomonoff's lightsaber. Unfortunately, actually computing Solomonoff induction is equivalent to solving the Halting problem, and hence is undecidable.
    -Description: Suppose we're given a finite bitstring y. We want to find the prior probability of a reference Turing machine with random input producing y. It doesn't matter what Turing machine we use, since they're all universal and compile to each other in constant time, so fix a reference Turing machine U. This turns out to be the sum over programs p such that U(p) = y of 2^(-l(p)) where l(p) is the length of p.
    -Kolmogorov complexity: This is a mathematical definition closely related to Solomonoff induction, and is often used as a mathematically precise way of formalizing Occam's razor. The Kolmogorov complexity of a bitstring is the length of the shortest input program which yields the bitstring when computed over some reference Turing machine. Denoting K(b) as the Kolmogorov complexity of bitstring b, it's common then to implement Occam's razor by weighting programs by the factor 2^(-K(b)), which is the probability of randomly selecting the true generating program of b. Because our definition was dependent on a reference Turing machine, the exact values depend on the Turing machine we use. The dependence is merely a constant factor though, since all Turing machines compile to each other, and hence the relative rankings of complexities of bitstrings is preserved.
-Veil of ignorance: A method, proposed by political philosopher John Rawls, for evaluating the morality of a societal issue or law; it's a thought experiment designed to allow one to "objectively" consider moral choices without interference from their personal biases, priviledges (or lack thereof), and limited empathy. The thought experiment is meant to lead to sociopolitical decisions which would evoke a truly egalitarian society, not in the sense of legal equality for all but rather in the sense of true equality for all, accounting for differences in one's characteristics which might give them unfair or unearned advantages or disadvantages. The thought experiment goes as follows: suppose you haven't yet entered a society, and when you do, all your socioeconomic and political characteristics, such as social class, wealth status, minority status and race, talents and abilities, disadvantages, and other factors influencing position in social order are randomly assigned. Under this framework, one is inclined to judge a political issue or law from all perspectives, accounting for the implicit advantages and disadvantages of each, since one may end up in any position. Assuming rational actors which are risk averse, the rational thing to do then is to "hedge one's bets" by creating laws which favor the worse-off disproportionately more than they do the better-off. The canonical example is that of progressive taxation, for which the veil of ignorance is an old argument. The point of the exercise is to average out all the morally irrelevant persistent biases one faces, such as one's own strength or intelligence.
-RF resonant cavity thruster: (***INCOMPLETE***)
-Submarine transatlantic cables: (***INCOMPLETE***)
-Interest rates: (***INCOMPLETE***)
-Samy worm: A computer worm written by Samy Kamkar that infected MySpace accounts in 2005. The virus is known for being the fastest spreading virus of all time. The worm itself was relatively harmless; when a user viewed Samy's MySpace page, the string "but most of all, samy is my hero" would be copied onto the user's own MySpace page, and any user viewing any MySpace page infected with the worm would also become infected. The worm spread extremely rapidly; it had infected around 8,000 people in two hours but reached over one million in twenty hours. Kamkar was arrested and prohibited from using a computer for three years.
-Emily Howell: (***INCOMPLETE***)
-Pareto principle: (***INCOMPLETE***)
-Categorical imperative: (***INCOMPLETE***)
-One-time pad: (***INCOMPLETE***)
-Quantum logic: (***INCOMPLETE***)
-One electron universe: (***INCOMPLETE***)
-Soliton: (***INCOMPLETE***)
-C code optimizations: (***INCOMPLETE***)
-Brochet wine experiment: (***INCOMPLETE***)
-Bloom's taxonomy: A set of three hierarchies that attempt to model and classify broad, overarching educational objectives. Benjamin Bloom, an American educational psychologist, undertook the goal of classifying and precisely delineating the thinking behaviors and processes that underly learning, in an attempt to build an educational framework for use in facilitating learning. This framework encompasses three domains, each of which is a hierarchy: the cognitive domain, the affective domain, and the psychomotor domain. The original version of Bloom's taxonomy was foramlized in 1956, and was later revised by a group of cognitive psychologists in 2001. Today, among educators, curriculum planners, administrators, teachers, and researchers, Bloom's taxonomy is the de facto standard.
    -Cognitive domain: This domain encompasses academic, ideo-conceptual learning. As with the other domains, the taxonomy is a hierarchical pyramid of classifications of thought and understanding (within the context of the cognitive domain). This means that operating at any level of the hierarchy indicates a capability to operate at any of the lower levels. The six levels are as follows.
        1. Remembering: The ability to recognize and mentally recall semantic facts, figures, images, terms, and basic concepts. This level of the hierarchy is intentionally aimed at pure memorization, and doesn't require the ability to actually understand the object of recall. Instead, all that's required to operate at this level is to simply be able to regurgitate or paraphrase an elementary idea or term.
            -Example: Name three common varieties of apple.
        2. Comprehending: This level involves the ability to actually understand and comprehend remembered knowledge and facts. This indicates a capability for organizing facts mentally, comparing different facts against each other, describing a fact or term in more detail, and distilling facts into a summary by abstracting their salient points.
            -Example: Compare the identifying characteristics of a Golden Delicious apple with a Granny Smith apple.
        3. Applying: Beyond just understanding a set of facts or ideas, the next level involves the ability to apply the facts in new situations, both practical and academic. Generally speaking, this means the ability to solve new, unfamiliar problems (as opposed to problems that can be solved by simply drawing on a similar or identical experience) by utilizing stored knowledge, techniques, and ideas. This level of understanding also includes the ability to identify connections between disparate facts, especially among groups of facts or ideas that weren't explicitly taught together as connected ideas.
            -Example: Would apples prevent scurvy, a disease caused by a deficiency in vitamin C?
        4. Analyzing: Whereas the first three levels deal with storing information, integrating it into one's understanding of that particular domain, and drawing upon the information in new situations, respectively, the next three levels deal with manipulating and judging information much more intimately, integrating it into one's broader global worldview and conceptual understanding. The analysis domain involves the ability to break information down into smaller component parts and being able to identify how the parts relate to each other, identifying motives and causes between the structure of the information, implications and conclusions that can be drawn from the information, its constituent parts, and their structure, making inferences based on the information, and generalizing it. This goes beyond simply drawing upon comprehended information to solve a problem, since the capacity to break that information down into its constituent parts and fully understand the relationship between those parts, the motivation for structuring the parts as they are, and the position of those parts and the information in a broader conceptual context necessarily implies a deeper level of comprehension that entails being able to use that information as a source of ideological or procedural inspiration in solving new problems.
            -Example: List four ways of serving foods made with apples and explain which ones have the highest health benefits. Provide references to support your statements.
        5. Evaluating: Evaluation is an even deeper level of understanding of information. Being able to evaluate information requires the ability to understand it intimately enough to recognize the source of the information and the motivations, biases, strengths, and perspective of that source, to identify shortcomings and advantages of different intepretations of the information, to judge and categorize different paradigms of thinking and understand how specific information fits into and connects these different paradigms. It requires the capacity to defend opinions and make arguments about information, weighing the information against and connecting it with other forms of evidence, both internal and external to the given information. Finally, this also involves making value judgments about the information, and analyzing its relationship with respect to certain braoder goals, and judging the validity and value of those goals themselves.
            -Example: Which kinds of apples are best for baking a pie, and why?
        6. Synthesizing: Finally, whereas analysis and evaluation involve a deeper level of comprehension of information in a way that's much more intimate with the structure of the information, synthesis goes a step further by necessitating the ability to apply analysis of information to create new logical-conceptual structures. This goes much further than the application level, which only requires the ability to draw on comprehended information to solve a problem; rather, synthesis requires integrating a deeper understanding of the structure of certain information with one's creativity and capacity to build new ideas, theories, models, inferences, and perspectives. It involves using diverse elements to compose a novel, emergent whole structure or pattern.
            -Example: Convert an "unhealthy" recipe for apple pie to a "healthy" recipe by replacing your choice of ingredients. Explain the health benefits of using the ingredients you chose vs. the original ones.
    -Affective domain: The affective domain describes peoples' capacity for emotional sensitivity to the pain and joy of other living things, to empathize with others and understand their thoughts, feelings, desires, motivations, and worldview. There are five levels of understanding:
        1. Receiving: The lowest level; the student passively pays attention. Without this level no learning can occur. Receiving is about the student's memory and recognition as well.
        2. Responding: The student actively participates in the learning process, not only attends to a stimulus; the student also reacts in some way.
        3. Valuing: The student attaches a value to an object, phenomenon, or piece of information. The student associates a value or some values to the knowledge they acquired.
        4. Organizing: The student can put together different values, information, and ideas and accommodate them within his/her own schema; comparing, relating and elaborating on what has been learned.
        5. Characterizing: The student at this level tries to build abstract knowledge.
    -Psychomotor domain: (***INCOMPLETE***)
-Laszlo Polgar: (***INCOMPLETE***)
-Bloom's 2-sigma problem: (***INCOMPLETE***)
-Cleanroom: (***INCOMPLETE***)
-End of Greatness:  (***INCOMPLETE***)
    -Cosmological principle: (***INCOMPLETE***)
-Cosmic web:  (***INCOMPLETE***)
-Great attractor: A mass located about 220 million light years away with a mass of 1 quadrillion solar masses. It is pulling in objects in its vicinity for hundreds of millions of light years away. 
    -Hubble flow: The apparent movement of galaxies away from each other due to the expansion of the universe. Any deviation from this "base" velocity caused by gravity, collisions, or some other force is called peculiar velocity. 
    -Zone of avoidance: About 20% of the true celestial sphere around the earth is unable to be seen because the Milky Way itself is blocking our view. The Great Attractor is in this zone. 
-Mathematical universe hypothesis: (***INCOMPLETE***)
-Dining philosophers problem: (***INCOMPLETE***)
-NORAD: (***INCOMPLETE***)
-Lotus effect: (***INCOMPLETE***)
-Musa I of Mali: The tenth Mansa ("sultan" or "emperor" of Mali empire) of the West African Mali empire. He is the richest recorded person in human history; he controlled vast amounts of gold and other resources in his empire, and his inflation-adjusted net worth has been calculated to be around $400 billion.
-Color confinement: (***INCOMPLETE***)
-Alexander the Great: (***INCOMPLETE***)
-Neuron conduction velocity: (***INCOMPLETE***)
-Biological classification: (***INCOMPLETE***)
-Graphics processing unit: (A.K.A. GPU) GPUs are special purpose processors used in embedded systems, personal computers, gaming consoles, and more. They are highly specialized for computer graphics and image processing, and are essentially electronic circuits (in the same sense that a CPU is) that contains very fast RAM (called VRAM) and is highly optimized for reading and manipulating memory concurrently. Whereas a CPU will typically have a few (typically up to 16) cores, modern GPUs have thousands, which is what makes them so beneficial for computer graphics, which essentially consists of performing independent, basic arithmetic on millions of pixels at once. Like CPUs, GPUs are Turing complete and so can, in principle, do anything a CPU can, but CPUs are highly optimized for sequential programming. GPUs will excel by a huge margin at performing SIMD instructions, that is, performing the same simple operation on a large amount of data at once (in parallel). CPUs are optimized for performing complex instructions on a single datum. Due to the initial flat overhead in parallel computing, each operation in a GPU core requires more transistors to implement than in a CPU, meaning less transistors overall, each of which generates more heat and is less efficient, and supports a less comprehensive or complex ALU and therefore instruction set. In short, individual CPU cores can perform any computation considerably faster than individual GPU cores, but there are much fewer (by a few orders of magnitude) CPU cores than GPU cores. CPU cores more efficiently implement branch prediction, virtual memory, interrupts, caching and pipelining mechanisms, out-of-order execution, etc.
    -Due to their extremely parallel nature, GPUs have also found application in the implementation of many AI deep search algorithms and other non-graphics applications. GPUs have different architecture than CPUs, so writing code meant to be run on a GPU often involves entirely new programming languages and paradigms (eg CUDA (Compute Unified Device Architecture), an API released by NVIDIA, who pioneered the first GPUs, in 2007 that provides a direct software interface between a programming language (designed to be C, C++, or FORTRAN) and the virtual instruction set of a GPU; the API was founded on the GPGPU (General-Purpose computing on Graphics Processing Units) movement, which strives to perform all OS computation, typically done by a CPU, on a GPU).
    -Interrupt: A signal to the processor of a computer to halt its current execution thread and temporarily switch to a more urgent thread. Normally a CPU is performing many operations and running many threads in the background, context switching between them, but when a certain high priority event occurs, such as mouse movement or a keyboard key press, an interrupt occurs. The CPU will then suspend execution of its current thread, save its state to memory, and use the interrupt handler (a special function for switching to the interrupt thread) to begin executing the interrupt thread. After finishing, it will load its state and resume execution of its previous thread. There are two types of interrupts: hardware and software.
        -Hardware interrupts: When devices in a computer require attention from the operating system, they will send special electronic signals, integrated into the hardware, to signal a hardware interrupt. Hardware interrupts are asyncronous (ie do not necessarily sync with the clock period (ie start right on the rising edge of the clock cycle)) and so may occur mid-instruction, unlike software interrupts.
        -Software interrupts: These interrupts are used to signify errors and other unexpected events that can't be handled within the program that caused the error itself, such as the processor receiving a command to divide by zero. These are similar to throwing exceptions in modern programming languages (eg Java, Python).
-Ahmdal's law: An equation giving the theoretical speedup offered by parallelizing a program. The equation is: speedup = 1 / (1 - p + p / s), where p = proportion of source code that can be parallelized and s = number of threads used in parallelization (ie the speedup of the purely parallel code). Note that as s approaches infinity, the speedup approaches 1 / (1 - p). The law also demonstrates that the speedup offered by parallelizing code is severely bottlenecked by the proportion of the code that must be performed sequentially and can't be parallelized.
    -Parallel vs concurrent computing: Two terms, often erroneously used interchangeably, are parallel computing and concurrent computing. Concurrent computation is a more general phenomenon than parallel computation; in short, concurrent computation is a paradigm in which multiple tasks begin executed within the same time frame; parallel computing is a special case where that time frame is infinitely thin, meaning the multiple tasks are executed simultaneously. Concurrent computing simply implies that if two tasks, say A and B, are to be completed, then execution of B begins before the execution of A finishes. One (very common) way of accomplishing this is to perform the two tasks in parallel (ie run both tasks on separate cores, starting execution of both at the same time), but it's not the only way (for example, the processor may perform a context switch (similar to an interrupt) in the middle of executing A and begin executing B).
-Specious present: (***INCOMPLETE***)
-Kirchoff's current law: The law states that at any junction in an electrical circuit, the sum of currents (from different wires) entering the junction equals the sum of currents leaving the junction.
 Kirchoff's voltage law: The directed sum of electrical potential differences in a closed circuit is zero.
 -Balloon effect: (***INCOMPLETE***)
 -Horizontal gene transfer: (Contrasted with vertical gene transfer (the transfer of genes through reproductive processes)) The transfer of genetic material in procedures not involving any sort of reproduction. The ability of bacteria to horizontally transfer genes is the primary reason for the build-up of bacterial resistance to antibiotics. It essentially speeds up the evolutionary process (though not necessarily natural selection) greatly. Main mechanisms of transfer:
    -Transformation: (***INCOMPLETE***) The altering of a cell's genetic material due to the uptake of foreign DNA or RNA into the cell's existing genetic material. Very common in bacteria, but not as common in eukaryotes.
    -Transduction: The process of transferring DNA from organism to organism through the use of a virus. This exploits the natural process of a virus' cycle: viruses infiltrate the host and insert genetic material into the cell in order to create more viruses.
    -Bacterial conjugation: During cell-to-cell contact, DNA can be transferred through a plasmid (a small DNA molecule that can replicate independently of the chromosomal DNA inside a cell; essentially just independent, small (usually circular rings) of DNA.
    -Natural competence: The ability of a cell to absorb extra DNA in the cell's environment. In nature, competence usually occurs when DNA becomes available in the environment of a cell because a nearby cell died through lysis (bursting). The cell performing competence then absorbs the DNA through its membrane (and cell wall, if present), then breaks the DNA down into nucleotides that are used for both (DNA) replicatory and metabolic functions.
-Business judgment rule: Because it is impossible for any director, CEO, or other executive of a company to guarantee financial success for the company, and because of the difficult-to-quantify and diverse nature of business, in corporate law courts will, in the case of any judicial decision or lawsuit, grant the company executive the default assumption that the executive's decision were made with what the executive thought were in the best interests of the company. Formally, this means that if the executive of a company is the defendant in a lawsuit or is otherwise under legal review, so long as the executive performed his fiduciary duties, the court will not review the executive's business decisions.
    -Fiduciary duties: There are three fiduciary duties that must be followed; if a plaintiff can prove that a company executive did not uphold any one of the triage, then that executive's business decisions are open for scrutiny.
        1. Good faith: Fair and open dealings in human interactions, which entails sincere and honest intentions in all interaction and communication. It implies that an entity will deal with other business entities openly and fairly.
        2. Due care: Acting with the amount of care that an ordinarily prudent person in that position would have exercised.
        3. Loyalty: Acting in a manner in accordance with the expectations held by the directors of the company.
-Ding zui: The uncommon yet not unheard of Chinese practice of people convicted of crimes hiring an imposter to serve their prison sentence for them. Reportedly, the practice is relatively common among wealthy Chinese elite.
-Cubic zirconia: The cubic crystalline form of zirconium dioxide. The material is artificially synthesized, and is very hard and optically flawless, making it a common competitor and fake substitute for diamonds. This is supported by its low cost of synthesis, extreme durability, and close visual likeness to diamond.
-Panama papers: (***INCOMPLETE***)
-Stages of the sleep cycle: During a period of sleep, the body goes through periodic sleep cycles which consist of five stages and last between 90 and 110 minutes.
    -Stages:
        1. Stage 1 - Light sleep: The first stage of the cycle that marks the transition from awake to asleep. In this stage, one can be awakened very easily. The eyes move slowly and muscle activity slows.
        2. Stage 2: Eye movement stops and brain waves slow down, with occasional bursts of rapid waves called sleep spindles.
        3. Stage 3: Extremely slow brain waves (delta waves) interspersed with smaller, faster waves. Stage 3 is sometimes grouped in with deep sleep.
        4. Stage 4 - Deep sleep: The brain produces almost exclusively very slow delta waves. It is very difficult to wake someone in deep sleep up. There is no eye movement of muscle activity. 
        5. Stage 5 - REM sleep: Breathing becomes more rapid, irregular, and shallow. Eyes jerk rapidly in various directions, and the body releases a natural, temporary paralytic to halt limb movement. This is the stage in which people dream. 
-Triffin dilemma: (***INCOMPLETE***)
-Cryonics: (***INCOMPLETE***) Process of freezing a clinically "dead" patient for future revival, when such technology becomes available. Body is gradually cooled to -196 degrees Celsius, with the blood being replaced with a cryoprotectant to prevent the freezing process from damaging cells.
    -Procedure: Begins somewhere between time the heart stops beating and the patient is pronounced dead.
        1. After the heart stops beating, the body is cooled by placing it in an ice bath.
        2. An HLR (heart lung resuscitator) is placed on the body.
            -The compressions performed by the HLR on the heart circulate the blood around the body; most importantly, to the brain.
        3. Tubes are attached to the body's major arteries and veins. Tubes suck out all the blood, and a low concentration of cryoprotective solution (cryoprotectant: solution that keeps cells from fracturing and crystallizing) is circulated for 2 minutes to flush out the last of the blood.
        4. A 50% cryoprotectant solution is circulated for 2 hours.
        5. A 100% cryoprotectant solution is circulated for 1 hour.
        5. Computer controlled fans administer nitrogen gas around the body. The nitrogen gas is circulated by the fans for 3 hours, cooling the body to -124 degrees Celsius.
        6. Body is placed in an aluminum holding container. This container is placed in an electronically monitored storage tank, which is then filled with liquid nitrogen.
            -Over 2 weeks, the body gradually reaches -196 degrees Celsius, the cooling temperature.
    -Thawing has not yet been perfected.
-Harm principle: A tenant in political philosophy attributed to John Stuart Mill which essentially states that if an action doesn't harm anyone (except perhaps the person doing the action) then the state has no grounds to criminalize the action. Technically, it posits "The only purpose for which power can be rightfully exercised over any member of a civilized community, against his will, is to prevent harm to others".
-Bitcoin: Bitcoin is a cryptocurrency (a medium of exchange (essentially, a currency) in which transactions are cryptographically secured) created by Satoshi Nakamoto in 2008. It's a decentralized virtual currency, which means that (a) it doesn't correspond to any physical storage of value (eg gold, which was used during the gold standard) and is instead purely abstract, and (b) it isn't backed by any kind of central authority (eg the US dollar is backed by the US Treasury, which means that the dollar has value because the US government guarantees it as a legally accepted form of currency, thereby guaranteeing to the population that the currency will be accepted, ensuring that one physical dollar bill must be accepted by merchants in exchange for one dollar's worth of goods and services (as opposed to the gold standard, when carrying one physical dollar was simply a legal placeholder for carrying the equivalent amount of gold)). The system as a whole which makes the digital currency possible consists of a computer network, connected through the Internet, in which each node is a user with its own Bitcoin wallet. Since there is nothing physical being exchanged and since there is no centralized authority to manage transactions, transactions are managed publicly through a construct called the block chain. The block chain is completely distributed, meaning every node in the network maintains its own copy of what is essentially a database, and the database is kept in sync with traditional distributed database algorithms. The block chain also needs to accurately determine the order of transactions, to prevent double spending (a scenario that arises when the order of transactions is ambiguous and there isn't a centralized ledger to check - essentially, if a node has a certain amount of money in its account, it can make two transactions, the sum total of which exceeds its account's balance (hence, it's spent more money than it has) to separate recipients, and if both recipients don't communicate with each other and merely check their personal ledgers, both will be unaware of the other transaction and will believe that the sender has enough money to send).
    -Block chain: Transactions are maintained in a public record, called the block chain, which is publicly distributed so that every node in the network has a copy. The block chain is essentially a set of all transactions that have ever occurred since the inception of Bitcoin; each transaction consists of a sender, a receiver, and an amount of Bitcoin. A "block" in the block chain refers to what is basically a caching mechanism used in updating the full ledger, and is defined as an ordered set of contiguous (in the record) transactions of fixed size. At any given point in time, there is a block of transactions being updated; when the size of the block reaches the fixed limit, it's added to the existing block chain and a new block is created. Thus, blocks are simply caching mechanisms in place to avoid updating the entire record every time a record is made. Blocks are not arranged randomly in the block chain; they are linked in chronological order, with each block maintaining a hash to its neighbor blocks. Each time a transaction is made, it's broadcast (simultaneously transferred to every node in the network) to the network; every node validates the transaction, updates it personal copy of the main ledger, and broadcasts the update to the network, until the ledger maintained by every node in the network is in sync. Periodically (approximately 6 times per hour) blocks are broadcast to the whole network so every node is fully synced. This is imperative in determining the order of transactions and hence preventing double spending.
        -Distributed database: A database that's not maintained on any central CPU, but is meant to be abstracted as a regular database which is in reality stored across nodes in a network. Each node may have its own copy of a portion of the full database or may have the entire database. To keep the database updated, and to avoid race conditions, two central algorithms are used - replication and duplication.
            -Replication: A algorithm, run periodically on a distributed network, which keeps the database updated. It firsts checks with every node in the network, in order to identify any changes or inconsistencies with the current version of the database; once updates are found, the algorithm then pieces all the updates together to resolve the new, fully updated database transmits the appropriate information to the appropriate nodes in the network to ensure that upon completion every node now has the same, updated version of the database. This is typically a very computationally taxing and complex process and can be implemented in many ways.
            -Duplication: An algorithm which selects one node in the network as a master database, and duplicates that node's copy of the database to every other node, overriding the other nodes' existing copies of the database. This is significantly less complicated than replication.
    -Bitcoin network: 
        -Digital signature: A cryptographic scheme, based on mathematics, which allows a sender of a message to authenticate itself to the recipient, proving to the recipient that indeed the message is coming from the intended, known sender and not from a third party, as well as that the message was not altered during transit. At a high level, the way digital signatures work is by using public key encryption. The purpose is to provide two main guarantees - authentication, that the message did indeed come from the expected sender, and integrity, that the message was not altered during transit. THe process is as follows.
            1. First, the sender uses a public key encryption scheme (eg RSA) to generate a public key and a private key.
            2. The sender then uses a one-way hash function to generate a hash of the message. This hash is then encrypted using the sender's private key.
            3. The "signature" is comprised of three pieces of information: the encrypted hash of the message, which hash function was used, and the public key. The message is sent along with the signature.
            4. The recipient now receives the message, and verifies it as follows. 
                a. First, the recipient decrypts the encrypted hash of the message using the sender's public key, to obtain a string s.
                b. Next, the recipient applies the hash function to the received message, to obtain a hash string t.
                c. If s doesn't match t, then the received message is not secure. If someone had impersonated the sender and sent the recipient the message claiming to be the sender, then because he imposter wouldn't have the associated private key to the known public key, applying the public key to the hash, which was encrypted using the wrong private key, would produce the wrong string s. If instead the correct sender sent the message but the message was altered during transit, then its hash wouldn't match the received hash.
            -Certificate authority: A centralized, trusted entity (eg the government) which issues digital certificates (an electronic document containing the owner's digital signature, and public key). In the context of client-server communication, a server has a constant public key (displayed on the certificate) and a constant corresponding private key; if the client uses the right public key, it won't matter who the information is sent to, since only the true server can decrypt it. So, the client must only find the correct public key to use for the corresponding server, and this information is provided by a digital certificate.
    -Mining: Because there is no central authority to "print" Bitcoin into circulation, an alternate method of creating Bitcoin is required (though by design, the total number of Bitcoin that can ever be in circulation, as per the current network parameters, is 21 million). The Bitcoin network ties the problem of introducing new Bitcoins into circulation with the problem of keeping the block chain consistent and secure (unchangeable and trusted). Each block maintains a cryptographic hash to the previous block, and the hash is based not only on the transaction data in the block but also on the cryptographic hash maintained by the block (so if a block chain consists of consecutive blocks A, B, C, then C maintains a hash to B, and B maintains a hash to A, and the hash kept by C depends not only on the transaction data in block B, but also on B's hash (which links B to A)); therefore, altering a block would alter the hash, and hence alter the hashes of every block that comes after the altered block, alerting the network and making it extremely difficult to alter the block chain (one would need to simultaneously alter every block after the altered block, and do so through a majority of the nodes in the network to give the appearance of legitimacy). The hash currently used is SHA-256. Thus, the process of "mining" Bitcoin simply means confirming new blocks to the block chain so the new block can be broadcast to the network and added to the distributed, general ledger (ie the block chain); since the block maintains a hash, the process of confirming a new block involves computing the SHA-256 hash of the current block at the end of the block chain. The miner which confirms the block (by broadcasting the block along with the correct hash to the network) earns a reward (which is currently 25 Bitcoin, and halves every 210,000 blocks (about every 4 years)), as well as gaining Bitcoin from transactional fees (this is to create an incentive for people to mine, since the reward will dwindle lower and lower over time). To add additional difficulty to the mining process, in order to make altering the block chain even more difficult, a proof-of-work is required in the hash. The proof-of-work is essentially a computationally taxing problem (whose solution is easy to verify) that miners must solve in order to confirm a block, so that attackers trying to modify the block chain would have to redo the computationally difficult proof-of-work of the block they wish to alter as well as that of every subsequent block in the block chain. More specifically, the Bitcoin network uses the Hashcash proof-of-work system.
        -Hashcash: The Hashcash proof-of-work system relies on the fact that SHA-256 is a one-way function, ie a function that's easy to compute but difficult to invert. The system uses a certain bound known as the difficulty threshold, which is an upper bound on the hash of the newly created block which is about to be added to the block chain, so lower difficulty thresholds imply more work in the proof-of-work. It is very unlikely that the hash of the block, based only on the transactions in the block and the currently most recent block in the block chain's hash, will be within the difficulty threshold, so miners must add another piece of data on to the data derived from the block itself, in order to modify the hash so that it meets the difficulty threshold. The additional data appended before hashing, represented as a number, is known as a nonce. Although for any given pre-image to SHA-256 there are multiple possible nonces that will work, they are extremely rare, and become rarer the lower the difficulty threshold is. Because SHA-256 is a cryptographic one-way hash, its output is effectively "random" in the sense that it's extremely difficult to engineer a particular pre-image that will hash to a desired value; therefore, the only currently known way to find a nonce to a block's data is brute force - to simply try natural numbers, incrementing every time the hash of the number and the block's data doesn't meet the difficulty threshold (ie is too big) until the right nonce, known as the golden nonce, is found. The difficulty threshold is automatically adjusted every 2016 blocks (which takes approximately 2 weeks) depending on the average performance of the network's mining so far (which depends on the number of miners and the average processing power or efficiency of the miners) in order to keep the average time to confirm a block at 10 minutes - between 2014 and 2015, the average number of values tried before finding the golden nonce rose from 16.4 quintillion to 200.5 quintillion. This proof-of-work system deters attackers since if they wish to modify a block, the transaction data would change, so the golden nonce found for the unmodified block would not work for the modified block, and the attacker would have to find the new nonce himself, as well as for every subsequent block, a computationally infeasible task as of today. As a last caveat, to reduce the chance of birthday collisions, Hashcash actually uses SHA-256^2 instead of SHA-256, ie hashing the pre-image twice, not once.
            -Mining in practice: Since the difficulty in mining rises with the number of miners and with advances in computational power, mining has gotten more and more difficult over time, and so mining has evolved from using traditional commercial CPUs to using high-end GPUs to ASICs (Application Specific Integrated Circuits) and FPGAs (Field-Programmable Gate Arrays), which are processors manufactured specifically to be optimized for mining Bitcoin.
            -Mining pools: Nowadays mining is so difficult that it's becoming more and more uncommon for individual miners to mine from desktop PCs. As a result, some miners have started banding together and pooling their computational resources, or pooling money to purchase expensive but very effective and specialized equipment (eg ASICs) to mine Bitcoin and distributing the earned Bitcoin among the miners in the pool, each receiving a share proportional to the amount of work contributed. Cloud mining is a recent venture in which mining takes place in rented data centers. 
            -Rough overview of mining in chronological order:
                1. Current block reaches the block size and is ready to be added to the block chain, so a new block can be created.
                2. All miners work (usually not together) to find the golden nonce.
                3. Once the golden nonce is found, the miner who found it broadcasts it, and the block, complete with a hash which matches the difficulty threshold, to the network.
                4. All other nodes in the network validate any incoming transactions with the now confirmed block, rejecting if the transaction doesn't exist or doesn't match the block.
                5. Nodes accept the block by incorporating the block in the distributed block chain, and a new block is created.
            -51% attack: The above system ensures that in order for an attacker to alter the block chain and get every node in the network to accept the change (by believing it to be legitimate), the attacker must possess computational power greater than that of 50% of the network's shared computational power, which is highly unlikely. Moreover, if an attacker did possess this amount of computational power, using it to honestly mine Bitcoin might prove more profitable than counterfeiting Bitcoin in the first place. 
    -Volatility: Since its inception, Bitcoin exchange rates have been extremely volatile (7 times more so than gold, and 18 times more so than the USD), though the general trend has been for Bitcoin to get progressively more expensive over time.
-Data dredging: The (mathematically invalid) practice of using data mining techniques to find patterns in data, and then presenting those patterns as statistically significant evidence of some hypothesis that was devised after the fact (as opposed to devising a specific hypothesis beforehand and then searching data for confirmation or rejection). The process involves testing several different hypotheses about the structure of the data, and for each hypothesis, attempting to search the data for patterns that "confirm" the hypothesis. This is in opposition to conventional data mining, wherein hypotheses are generated, not confirmed; indeed, any pattern or relation found in data during data mining needs to be confirmed with additional independent studies that take the relevant patterns as hypotheses. Data dredging, on the other hand, is invalid because if enough different arbitrary hypotheses are tested, then purely by chance at least one of them is likely to appear statistically significant for one of the hypotheses, by pure chance (recall that for any result to be statistically significant, there's a positive probability (namely, the p-value) that the data confirms the hypothesis by pure chance, despite the hypothesis being false in reality with respect to the full population of data).
-Filesystem hierarchy standard: The common convention and standard adhered to by all Linux (and more generally, most Unix-like) systems, in determining the default directory structure of the filesystem. Under the convention, all directories are sub-directories of the root directory, "/". The default sub-directories under root are below.
    -Default directory structure:
        -/bin: This directory contains the command binaries (ie executables for programs) for all of the default, essential Linux commands (eg "ls", "cp", "cat", etc.).
        -/boot: Directory where bootloader files (eg code for the OS kernel).
            -Bootloader: A small program (stored as a binary in read-only memory) that's the first program executed by a computer on startup. On startup, the computer has no OS; upon executing the bootloader, the sequence of loading OS code and data into RAM and initializing the OS kernel.
        -/dev: Directory where default, essential device files (interface between the computer OS and device drivers, often for peripheral devices such as printers, serial ports (for USBs), headphone jack, etc.) are located.
            -/dev/null: A special default "device" file that simply discards all data that's written to it, and returns a successful return code.
            -/dev/zero: A special device file that provides a continuous, unterminated stream of null characters to standard output.
        -/etc: Directory where system-wide configuration files (eg .bashrc, .i3config, etc.) for programs and system settings are stored.
            -/etc/opt: Config files for any add-on packages (which are stored in /opt).
            -/etc/X11: Config files for the X Window system, version 11.
        -/home: Directory where user home directories are stored.
        -/lib: External libraries that are needed for the binaries in /bin and /sbin.
        -/lib<qual>: External libraries needed for the binaries in /bin and /sbin, but stored in an alternate format than those in /lib.
        -/media: Mount points (directory where an additional filesystem, often stored on an external device such as a USB that's connected to the computer OS with a driver, is logically attached) for all removable media devices, such as CD-ROMs.
        -/mnt: Mount point for temporarily mounted non-media filesystems.
        -/opt: Directory for optional application software to be installed, with each application storing relevant files in its sub-directory.
        -/proc: Completely virtual (ie all sub-directories and files are symbolic links) directory meant to act as a filesystem that stores information about running processes and the kernel in files.
        -/root: Home directory for the root user.
        -/run: Directory where system daemons (in particular, ones that start very early on in the boot sequence, such as systemd) store temporary runtime files, often information about the system's state since the last boot.
        -/sbin: Essential system binaries that aren't meant to be run by normal users (eg fsck, init, route, etc.).
        -/sys: An interface for the kernel that provides a filesystem-like view of relevant information provided by the kernel, such as configuration settings. The directory is similar to /proc. It contains information about devices connected to the computer, such as the screen, keyboard, etc.
        -/tmp: Directory meant to store temporary files; may not be preserved between system boots.
        -/usr: Directory for read-only user data, as a secondary filesystem. This directory contains the majority of user utilities and applications.
            -/usr/bin: Non-essential command binaries for all users.
            -/usr/include: Standard include files (files that are referenced in #include directives in certain programming languages (eg C, C++, etc.), which cause the content of the file to be inserted into another file).
            -/usr/lib: Necessary libraries for the command binaries in /usr/bin and /usr/sbin.
            -/usr/lib<qual>: Alternate format libraries.
            -/usr/local: Directory meant to act as a tertiary filesystem storing local user data. This directory usually has additional sub-directories such as bin, lib, share, etc.
            -/usr/sbin: Non-essential system binaries.
            -/usr/share: Stores read-only architecture independent data files.
            -/usr/src: Source code of the computer (eg kernel source code, etc.)
        -/var: Variable files which are constantly written to, and hence whose content changes as the computer runs (eg log files, temporary email files, etc.)
            -/var/cache: Directory where applications can store data for caching purposes.
            -/var/lib: Stores state information about the system (ie data that programs will continuously modify as they run), as well as dynamic data files.
            -/var/lock: Lock files and files keeping track of which system resources are currently in use.
            -/var/log: Log files.
            -/var/opt: Variable data that's maintained by add-on packages in /opt.
            -/var/run: Runtime variable data, including information about the system since when it was last booted.
            -/var/tmp: Temporary files that are guaranteed to be preserved between reboots.
-Change blindness: A perceptual phenomenon in which people fail to notice significant changes in a visual stimulus, such as major differences in the content of an image after the image flickers.
-Meditation: (***INCOMPLETE***)
    -Resarch on Meditation and intelligence:
        -"Accelerating Cognitive and Self-Development: Longitudinal Studies with Preschool and Elementary School Children"
            -Published in "Journal of Social Behavior & Personality"; 2005, Vol. 17 Issue 1, p65-91.
            -Authors: Carol A. Dixon, Michael C. Dilibeck, Frederick Travis, Horis I. Msemaje, Mawiyah B. Clayborne, Susan L. Dilibeck, Charles N. Alexander
            -Results: 45 week study with a control group and experimental group, both consisting of 25 elementary school kids, found "increases in principal components of self-concept, analytical ability, and general intellectual performance" versus a control group with no improvement. The technique of transcendental meditation appeared to accelerate the natural development of consolidation of self-awareness in children, often at a deeper level.
            -Link: http://tmhome.com/benefits/study-on-cognitive-and-self-development-in-children/
        -"Three randomized experiments on the longitudinal effects of the Transcendental Meditation technique on cognition"
            -Authors: Kam-Tim So, David W. Orme-Johnson
            -Published in "Intelligence"
            -Results: Three studies on 362 Taiwanese high school students practiced "transcendental meditation" for 15 - 20 minutes, twice a day over the course of 6 - 12 months. Six intelligence metrics were used:
                1. Test for Creative Thinking-Drawing Production (TCT-DP)
                2. Constructive Thinking Inventory (CTI)
                3. Group Embedded Figures Test (GEFT)
                4. State and Trait Anxiety (STAI)
                5. Inspection Time (IT)
                6. Culture Fair Intelligence Test (CFIT)
            The experimental group saw "significant effects" on all seven variables (p-values for the variables ranged from 0.035 to below 0.0001) versus a control group (that napped for 20 minutes per day instead) with no improvement.
            -Specifics: Transcendental meditation (TM) is said to induce what's classified as a "wakeful hypometabolic" state, characterized by similar physiological characteristics as observed in sleeping patients, such as decreased metabolism, lowered heart rate, lowered respiration rate, etc. in conjunction with mental alertness, as was implied by observed increased alpha brain waves on an EEG. The hypothesis of the study was that TM would aid cognitive development in intelligence and IQ. This was based on observed physiological changes that occured in subjects during TM, such as increased blood flow to the brain and an increased EEG coherence (a mathematical measure of similarity/correlation of an EEG measurement (of voltage fluctuations in the brain, caused by ionic current resulting from action potentials) in two sensors, typically indicating correlated brain activity (specifically, neuronal oscillatory activity, ie simply recurring, repetitive, often rhythmic neural activity) in the brain regions measured by the sensors, and hence functional connectivity in the regions) between several parameters which are known experimentally to be correlated with intelligence. Moreover, when tracking event-related potentials (the measured brain response, in terms of strength of action potentials in neurons, to a direct sensory, cognitive, or motor event/stimulus) during TM, brain responses showed lower latency, higher amplitude, and a broader cortical representation (meaning, the responses were observed to be more holistically distributed throughout the corresponding brain cortex). Again, these parameters, with respect to event-related potentials, have been shown to be experimentally correlated with cognitive performance.
            -Specific results on the intelligence tests for TM vs napping:
                -TCT-DP:
                    -TM: Pre-test mean and SD: 19.2, 7.4. Post-test mean and SD: 21.2, 7.7.
                    -Napping: Pre-test mean and SD: 46.1, 6.1. Post-test mean and SD: 47.4, 6.0.
                    -F-statistic of 8.89 corresponding to a p-value of 0.003.
                -CTI: 
                    -TM: Pre-test mean and SD: 57.5, 5.6. Post-test mean and SD: 60.8, 6.1.
                    -Napping: Pre-test mean and SD: 57.0, 5.9. Post-test mean and SD: 58.0, 5.5.
                    -F-statistic of 21.34 corresponding to a p-value of less than 0.001.
                -GEFT:
                    -TM: Pre-test mean and SD: 4.0, 2.3. Post-test mean and SD: 6.8, 1.9.
                    -Napping: Pre-test mean and SD: 4.1, 1.9. Post-test mean and SD: 5.4, 2.6.
                    -F-statistic of 12.81 corresponding to a p-value of less than 0.001.
                -STAI:
                    -TM: Pre-test mean and SD: 46.3, 5.1. Post-test mean and SD: 44.1, 6.6.
                    -Napping: Pre-test mean and SD: 46.1, 6.1. Post-test mean and SD: 47.4, 6.0.
                    -F-statistic of 29.43 corresponding to a p-value of less than 0.001.
                -IT: 
                    -TM: Pre-test mean and SD: 68.2, 11.3. Post-test mean and SD: 57.4, 9.0.
                    -Napping: Pre-test mean and SD: 68.3, 12.4. Post-test mean and SD: 61.1, 10.9.
                    -F-statistic of 11.66 corresponding to a p-value of 0.001.
                -CFIT:
                    -TM: Pre-test mean and SD: 22.7, 4.5. Post-test mean and SD: 24.2, 3.7.
                    -Napping: Pre-test mean and SD: 22.4, 4.3. Post-test mean and SD: 23.4, 3.8.
                    -F-statistic of 1.94 corresponding to a p-value of 0.165.
            Based on these results, it would appear that TM does in fact result in a significant increase in scores on all the tests, except for CFIT.
            -Link: http://tmsw.org.uk/tmsw/files/Increased-IQ.pdf
        -"Meditation and flexibility of visual perception and verbal problem-solving"
            -Authors: Michael C. Dillbeck
            -Results: (***INCOMPLETE***)
            -Link: https://link.springer.com/article/10.3758/BF03197631
        -"Teaching Students How to Meditate Can Improve Level of Consciousness and Problem Solving Ability"
            -Results: (***INCOMPLETE***)
            -Link: http://www.sciencedirect.com/science/article/pii/S1877042812053803
        -"Hormonal control in a state of decreased activation: potentiation of arginine vasopressin secretion"
            -Results: (***INCOMPLETE***)
            -Link: https://www.ncbi.nlm.nih.gov/pubmed/3906711
        -"Transcendental meditation and improved performance on intelligence-related measures: A longitudinal study
            -Results: Two year study with an experimental group of 45 university students from Maharishi International University and a control group of 55 students from the University of Northern Iowa in which the experimental group practiced transcendental meditation twice a day. Metrics:
                1. Cattell's Culture Fair Intelligence Test
                2. Hick's reaction time
            Experimental group "improved significantly" on Cattell's test (p-value below 0.005) and on the reaction time (p-value below 0.00001) versus a control group with zero improvement. The following potential confounding variables were controlled for with covariance analysis: subject's age, education level, level of interest in meditation, father's education level, and father's annual income.
            -Link: http://www.sciencedirect.com/science/article/pii/019188699190040I
        -"Interaction between Neuroanatomical and Psychological Changes after Mindfulness-Based Training"
            -Results: Study found significant cortical thickness increase in individuals who underwent a 8 weeks in a mindfulness meditation training program.
            -Link: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4203679
        -"Mindfulness meditation improves cognition: Evidence of brief mental training"
            -Results: Students from the University of North Carolina were divided into a 24 person experimental group and a 25 person control group. The experimental group underwent mindfulness meditation training by professionals for 20 minute sessions, while the control group spent an equivalent amount of time listening to an audiobook of J.R. Tolkien's "The Hobbit". After only four days of practice, statistically significant results (p-value below 0.05) on tasks measuring sustained attention, brain processing efficiency, visuo-spatial processing, and working memory (specifically, the Symbol Digits Modality test (on this test, the forward or backward speed didn't change, but the overall score did), a verbal fluency test, and the n-back test (on the n-back test, participants' speed didn't vary over the control or experimental group, but the average number of hits did) were administered).
            -Link: http://jtoomim.org/brain-training/Zeidan2010_Mindfulness_Meditation.pdf
        -"Physiological effects of transcendental meditation"
            -Authors: Robert K. Wallace
            -Results: Several physiological measures were taken as subjects performed transcendental meditation (TM); specifically, oxygen consumption, heart rate, skin resistance, and EEG measurements were recorded before, during, and after the TM session. There were significant changes in the measurements of all indicators between the experimental group and the control group (which did nothing) - in the experimental group, oxygen consumption and heart rate decreased, skin resistance increased, and the EEG showed changes in brain wave activity and distribution. The observed physiological responses appear to be significant and unique enough to distinguish the resulting mental state from commonly encountered states of consciousness.
            -Link: https://www.ncbi.nlm.nih.gov/pubmed/5416544
-Social proof: The psychological phenomenon in which people will, especially in socially unfamiliar or ambiguous situations, use others' actions and behavior as a cue for how they themselves should behave, independent of the actual behavioral stimulus that evoked the initial behavior in the group. One example of this is that when a person enters a new conversation, someone tells a joke, and everyone starts laughing, the newcomer is like likely to laugh themselves, irrespective of the comedic value of the joke; in contrast, if no one laughs, bystanders are likely to find the joke unfunny or in poor taste. This concept is one possible contributing factor to the tendency of large groups to become hijacked by "groupthink", "mob mentality", or "herd behavior", subsequently conforming to choices based on the opinions of the members rather than the quality of the choice itself, which is often incorrect. Analytically, it is rational to incorporate social proof as a relevant factor giving insight to the information and opinions held by others, but research shows that people tend to heavily and disproportionately weigh it.
-"The Experimental Generation of Interpersonal Closeness": A paper by Arthur Aron (Interpersonal Relationships Lab at Stony Brook University, New York) in 1997 in the Personality and Social Psychology Bulletin. The paper posits 36 questions which are meant for two strangers to ask either; the questions are designed to attempt to build trust, familiarity, and emotional connection between the two people, by forcing both to become vulnerable and exposing the unique and positive aspects of each other.
    -Questions:
        1. Given the choice of anyone in the world, whom would you want as a dinner guest?
        2. Would you like to be famous? In what way?
        3. Before making a phone call, do you ever rehearse what you're going to say? Why?
        4. What would constitute a perfect day for you?
        5. When did you last sing to yourself? To someone else?
        6. If you were able to live to the age of 90 and retain either the mind or body of a 30-year old for the last 60 years of your life, which would you choose?
        7. Do you have a secret hunch about how you will die?
        8. Name three things you and your partner appear to have in common.
        9. For what in your life do you feel most grateful?
        10. If you could change anything about the way you were raised, what would it be?
        11. Take four minutes and tell you partner your life story in as much detail as possible.
        12. If you could wake up tomorrow having gained one quality or ability, what would it be?
        13. If a crystal ball could tell you the truth about yourself, your life, the future or anything else, what would you want to know?
        14. Is there something that you've dreamt of doing for a long time? Why haven't you done it?
        15. What is the greatest accomplishment of your life?
        16. What do you value most in a friendship?
        17. What is your most treasured memory?
        18. What is your most terrible memory?
        19. If you knew that in one year you would die suddenly, would you change anything about the way you are now living? Why?
        20. What does friendship mean to you?
        21. What roles do love and affection play in your life?
        22. Alternate sharing something you consider a positive characteristic of your partner. Share a total of five items.
        23. How close and warm is your family? Do you feel your childhood was happier than most other people's?
        24. How do you feel about your relationship with your mother?
        25. Make three true "we" statements each. For instance, "we are both in this room feeling..."
        26. Complete this sentence "I wish I had someone with whom I could share..."
        27. If you were going to become a close friend with your partner, please share what would be important for him or her to know.
        28. Tell your partner what you like about them: be honest this time, saying things that you might not say to someone you've just met.
        29. Share with your partner an embarrassing moment in your life.
        30. When did you last cry in front of another person? By yourself?
        31. Tell your partner something that you like about them already.
        32. What, if anything, is too serious to be joked about?
        33. If you were to die this evening with no opportunity to communicate with anyone, what would you most regret not having told someone? Why haven't you told them yet?
        34. Your house, containing everything you own, catches fire. After saving your loved ones and pets, you have time to safely make a final dash to save any one item. What would it be? Why?
        35. Of all the people in your family, whose death would you find most disturbing? Why?
        36. Share a personal problem and ask your partner's advice on how he or she might handle it. Also, ask your partner to reflect back to you how you seem to be feeling about the problem you have chosen.
-Great recession: (***INCOMPLETE***)
-Landmark supreme court decisions: (***INCOMPLETE***)
-Frege-Geach problem: (***INCOMPLETE***)
-OSI model and the Internet: (Open Systems Interconnection model) A set of seven abstraction layers meant to standardize the communication protocols in a telecommunication or computing network. In information technology, a network has the precise definition of a a graph of nodes connected by communication paths. The OSI model is a general reference model (a conceptual framework) for how applications can connect over a network. Although the model is widely known and very useful in guiding the construction of communication protocols such as TCP/IP (which essentially define the Internet today), they don't map cleanly to the OSI model; indeed, the model is rarely implemented in its entirety and down to every detail. OSI is the official international standard by ISO (International Organization of Standards). The essence of the model is to break down the communications process between two endpoints of a telecommunications network into seven related layers; data from one endpoint to the other flows down through the layers in the source computer, travels through the network, and then back up the layers to the destination computer. The seven layers are implemented by a combination of the source and destination computers' operating systems, specialized applications, network card device drivers, networking hardware, physical network cables, WiFi, etc.
    -Client-server model: An application framework in which there are two separate, main modules or structures - the client and the server. The application is modeled as controlling the flow of resources or data, similar to how data is transmitted between the frontend and backend of an application; the purpose of the server is to provide resources or services to the client, and the client determines when certain services are needed, and in such situations, opens a communication channel to the server, requests the resource or service it needs, and upon receiving it, proceeds. The actual "client" and "server" can be anything, from simply separate modules of a single application to entirely different computers or devices. Although the communication process between the client and server often takes place over a computer network, this is not a requirement.
        -Servers: To provide further structure, typically one server isn't used to channel all resources to the client; different servers are stratified by the types of services they provide (eg web servers provides HTML / CSS / JavaScript web pages, file servers provide files, etc.).
            -Uniform Resource Locater: (URL) The Internet is just one (very, very large) computer network which makes use of the client-server model. In the Internet Protocol, each server is assigned a URL which other nodes in the network can use as a reference to the server's location in the computer network (ie the Internet, in this case). URLs are distinct from IP addresses in two main ways - first, they are more "human friendly" in that rather than simply being a sequence of numbers like the IP address, a URL is designed to act more like the linguistic idea of a name ("eg "https://www.facebook.com"), and second, they often, directly or indirectly, contain the IP address, along with other information, such as the protocol used, the path, an optional fragment identifier, etc. The Internet parses a URL to obtain all the necessary information to retrieve the appropriate information from the correct server when a request is made by the user. A URL contains a domain name which maps (through DNS) to the corresponding IP address, and also contains other important metadata about the referenced server that's relevant to the process of reaching the server and retrieving the information.
                -Domain name system: Since URLs are made up primarily of names (eg "facebook"), this name must be resolved into 
    -Layers: The seven layers stack on top of each other, from the application layer on which high-level APIs and web applications run, down to the HTML, CSS, and JavaScript that render them, down to the HTTP, TLS, etc. protocols that manage communication sessions, and so on all the way down the a physical layer, eg Ethernet cords, USBs, DSLs (digital subscriber lines, a set of technology used to send data over telephone lines), etc. The transmission of data through the OSI model can be described recursively - on layer N, two endpoints representing communicating devices, called layer N peers, wrap the data they wish to transfer in a PDU (protocol data units, a basic unit of information, as defined by whatever communication protocol is used to transfer the said data, delivered between peers of a network). The PDU is passed to layer (N - 1), and is known as a SDU (service data unit, a unit of data passed to a layer from the layer above; represents more basic, unstructured data that hasn't yet been wrapped in a PDU by the lower layer), where it is wrapped into a layer (N - 1) PDU by concatenating it with a header or footer or both. Layer (N - 1) then passes the PDU as an SDU to layer (N - 2), and so on, until the base case, layer 1, physically transmits the data to a receiving device, through a wire, electromagnetic waves, etc. At the receiving device, PDUs are successively unwrapped until the original data makes its way to the intended recipient. The seven layers are:
        1. Physical layer: This layer is the most basic, and defines the electrical and physical specifications of the data connection; it's the base layer which controls the actual transferring of bits through a physical transmission medium (eg copper, fiber optic cable, radio wave, etc.). Thus, the physical layer includes specifications for voltages in the cable, line resistance and dimensions, signal timing, channel frequency (for wireless devices), etc. It also defines the network topology. It's in this layer that the data to be transmitted is encoded in a pattern of bits. This layer is also the most complex layer in the OSI hierarchy, due to the number of ways to physically transmit information and the failure cases and complexity associated with it. Parameters related to the physics and engineering specifications of the transmission medium and mechanism, such as what frequency to broadcast on, what modulation scheme to use, the shapes and properties of the electrical connectors (an electromechanical device used to join two electrical terminations (the specific point of a conductive device marking the start or end) in order to create a closed electrical circuit), are specified on this layer.
            -Network topology: The general structure of the graph representing a network; typically stratified into the following formats - bus (all node are connected to a common, central line, called a bus, rather than to each other directly), mesh (general graph structure, with nodes connected to other nodes and with no restrictions on types of edges between nodes, other than prohibiting self edges and double edges, so the graph is well defined), ring (nodes are circularly connected in a ring), etc.
            -Internet infrastructure: 
                -Internet service provider: (ISP) An organization (ranging from public to private, for-profit to non-profit) that allows customers to access the Internet. Servers are the only devices that connect directly to the Internet (ie connect directly to one of its sub-networks; the means of connection could be by connecting to one of the underground or submarine cables, wirelessly through radio waves to another server or satellite, etc.); clients such as personal computers do not connect directly to the Internet. This is because in order to visit a website, a device must be connected to the server. If every device that wanted to visit a set of websites was directly connected to each of the servers hosting those websites, it would be extremely impractical for that many cables to connect that many devices. Instead, ISPs act as connection hubs (they are simply highly connected nodes in the Internet graph) that are single nodes connected to many servers and sub-networks, so individual personal computers only have to connect to an ISP rather than individual servers. The alternative to using an ISP to connect to the Internet would be to set up a private cable or other means of connection to an exchange point and negotiate a peering agreement.
                    -Peering: The mutually beneficial, voluntary, and free interconnection of two computer networks. Two independent networks have several motivations to expand their reach by connecting to other networks, such as increased redundancy (more connected nodes means more possible paths between any two nodes, making the network more resilient), increased capacity (more nodes allows the overall network to handle more traffic), better average performance (more nodes increases the liklihood of shorter paths between nodes), etc.; the process of negotiating a connection between the networks is called peering. Traditionally, peering is settlement-free, which means neither network charges money from the other for any traffic; the traffic of both networks is free to use any of the nodes in the two combined networks as if it were part of its own network, for no extra charge. The actual process of peering involves setting up a physical connection between the networks and then exchanging routing (the process of finding the shortest path between two nodes in a network) information (typically through BGP).
                        -Exchange point: Physical locations (typically large buildings) where two or more networks peer, as in interconnect their networks through a single physical port.
                        -Border gateway protocol: (BGP) A standardized protocol for exchanging routing information between computer networks. It's used by ISPs to establish the most efficient routes to different networks, with the paths stored in routing tables. BGP is typically used by ISPs; end nodes (ie clients) usually use a predetermined default route to get data to the ISP, who then uses BGP to efficiently route the data to the appropriate network in the Internet.
                    -Three types of ISPs: The three types of ISPs are categorized by how they connect to customers. Dial-up ISPs connect to customers through a public switched telephone network. High-speed internet, or broadband, ISPs connect to customers through any medium with the ability to transmit multiple signals and traffic types at once (eg coaxial cable, optical fiber, radio waves, etc.). Digital line subscribers (DSL) ISPs connect to customers through telephone lines.
                    -Tiers: All ISPs connect to the Internet, but ISPs are categorized into three tiers based on how "directly" connected they are to the Internet. A Tier 1 network can reach every network in the Internet without ever paying settlement (ie going through another ISP to get to a network); therefore, every Tier 1 network peers with every other Tier 1 network. Tier 2 networks are connected to some Tier 1 networks and so can access some networks on the Internet without paying, but must purchase Internet Transit (ie pay a Tier 1 ISP for access) to access some other networks. Tier 3 networks must pay Internet Transit to access any network; they aren't peered at all.
            -Relevant hardware:
                -Network interface controller: (NIC, A.K.A. network adapter) Broadly speaking, an NIC is a piece of hardware which allows a computer to connect to a network. This hardware contains the necessary circuitry to translate the data sent by the computer into whatever format it needs to be in so the data can be communicated to the physical layer or data link layer (eg ethernet, WiFi, etc.) of the network in question. Network adapters are used by computers and computing devices as modules which abstract away the process of connecting to a physical network, handling all the physical protocols required to send messages to the network. Typically, network adapters are either wired (ie ethernet) or wireless (ie WiFi).
                -Modem: (MOdulator-DEModulator) A device that modulates a carrier wave so as to encode digital information, or two demodulate a receiving carrier wave to decode the contained information.
            -Radio communication: The overwhelming majority of wireless communication (including two-way radios, cell phones, GPS units, garage door openers, wireless computer mouses, satellite TV, etc.) in the world uses radio to communicate; radio is essentially the technology of encoding information in radio waves, which can then be transmitted wirelessly over distances to a receiver which can decode the information. Radio waves are chosen for this (as opposed to any other type of light wave on the electromagnetic spectrum) because of the very large wavelength of radio waves, which is so large compared to the molecular structures that compose most everyday objects that radio waves are able to pass right through most solid objects (though there are some molecular structures, such as iron walls, which prohibit radio waves), an ideal property for a transmission medium. Radio frequencies tend to be between 3 kHz and 300 GHz.
                -Process: Given a digital signal, the process of transmitting it begins with a transmitter, which uses a source of electrical energy to encode the signal into alternating current oscillating at a desired frequency. This encoding is done by a modulation system in the transmitter, which changes some property of the underlying sine wave of the alternating current (amplitude, period, frequency of transmission (eg simply turning the source on and off)) in such a way as to bijectively relate to the original digital signal. The alternating current is sent to an antenna, which converts the electrical current into an electromagnetic wave. This is accomplished when the alternating current of electrons is forced through an arrangement of electrically connected metallic conductors, which creates an oscillating magnetic field and an oscillating electric field, which move away from the antenna as transverse electromagnetic waves. Conversely, to decode radio signals, oscillating electric and magnetic fields that compose a radio wave will hit the antenna and exert electromagnetic forces on the electrons in the antenna, causing them to move back and forth and hence producing an alternating current that can be translated into a digital signal. The radio waves produced propagate through space in all directions, and can be picked up by another antenna. Although the waves travel at the speed of light, the average speed may be lower depending on the medium; moreover, inherent instrument issues at each step of the process as well as transmission through space (due to air molecules and other objects) will alter the signal slightly, causing noise. This can be mitigated with error correcting codes and other mechanisms. In some cases the path taken by the radio wave is intentionally indirect, such as by reflecting it off the earth's ionosphere between 60 and 600 kilometers above the surface.
        2. Data link layer: This layer provides for node-to-node transfer of data and abstracts the idea of a link between two (directly) connected nodes (in the same local area network). It defines the protocol to establish and close a connection between two connected nodes in a network (which are, in layer 1, two physically connected devices). The data link layer is used by higher layers to transfer data between nodes, assured that the data link layer will ensure that traffic reliably gets from one node to its neighbor (a process which involves arbitrating and scheduling between two adjacent nodes, which both may be trying to send several streams of data at once to each other in both directions - when two devices try to use the same medium simultaneously, it's known as a frame collision). It also detects and corrects errors that occurred in layer. In this layer, the data transferred is communicated all at once (ie no "packet" or partition structure) and typically upper bounded by size, so only sufficiently small data can be transferred on this layer. The data link layer uses physical (ie MAC) addresses (which, unlike IP addresses or other routing address mechanisms in higher layers, are flat and provide no hierarchical information about the location of a device in a network (as opposed to, for example, IP addresses, which are hierarchically structured and assigned based on network location)) to deliver data, and is typically subdivided into two sub-layers:
            1. MAC layer: (Media/Machine Access Control layer) The lower sub-layer responsible for providing addressing (the mechanism for resolving the MAC address of the destination node) and channel access mechanisms (mechanisms or protocols that allow several devices connected to the same transmission medium (eg fiber optic cable, WiFi router, copper wire, etc.) to share the medium; more abstractly, controls the data traffic between the devices connected to the same network topology to allow all data to flow unobstructed). The MAC layer provides an interface between layer 1 and the LLC layer.
                -MAC address: (a.k.a. physical address) A unique identifier (in principle, no two NICs should have the same MAC address) for a device (specifically, an NIC) connected to a telecommunications network used as network addresses that are used to figure out where to send data so that it reaches its intended recipient. MAC addresses are assigned by the manufacturer of the device, and are stored in the device's hardware (usually in read-only memory). MAC addresses are assigned based on one of three numbering systems (MAC-48, EUI-48, EUI-64), managed by IEEE. Thus, because MAC addresses are completely independent of any given computer network or protocol, and depend only on and are unique to a specific device, a MAC address is also called a physical address. In a typical computer network, MAC addresses are used on the lowest level of abstraction in network telecommunication - they are used to direct communication from single nodes to nodes in the network. Therefore, for example, if a computer is connected to the Internet through a router, the computer's MAC address won't go any further than the router.
            2. LLC layer: (Logical Link Control layer) The upper sub-layer which provides multiplexing mechanisms (methods by which multiple signals are combined into a single signal that can then be transmitted over a shared medium, thereby allowing a single medium to cater to multiple data transfers and thus saving resources, eg allowing multiple phone calls to use the same telephone line). These mechanisms allow multiple network protocols to coexist in the same network, and be transmitted over the same shared physical medium. The LLC layer provides an interface between the MAC layer and layer 3.
            -Address Resolution Protocol: (ARP) (***INCOMPLETE***)
        3. Network layer: Whereas the layer 1 defines the physical mechanisms by which data is actually transmitted, and layer 2 defines the abstract methods by which data is transferred between two adjacent nodes in a network (ie nodes that are directly connected), the network layer uses layer 2 to define the means by which variable-length data (partitioned into packets) can be transmitted from any node in a (local area) network to any other node in the same (connected) network, using a path between the two nodes and consecutive node-to-node transfers (which layer 2 takes care of) to carry the data from the source node to the destination node. This layer resolves a node's abstract, logical address to its physical (ie MAC) address. The network layer will typically route the data between intermediate nodes as necessary. If the data is to large to be transmitted using node-to-node transfer, the data may be split apart into fragments that are then transmitted separately. This layer is also responsible for maintaining quality of service, ensuring that data is sent reliably and speedily.
            -Internet Protocol: (IP) The main communications protocol used in the Internet protocol suite (the model and set of communication protocols used to facilitate the Internet) used to transfer datagrams (basic units of data transfer employed in a packet-based network (a network that transmits all data in fixed size blocks, to increase efficiency)). The Internet as a whole can be thought of as a network (currently connected through radio waves and huge cables) whose nodes are sub-networks, connected physically with nodes being actual servers, devices, and other endpoints; IP, then, is the protocol which determines how, specifically, packets are sent from a node in one physical network to a node in another physical network, and the protocol is based solely on the IP address in the packet header. To do this, the protocol must figure out a path from the source node to the destination node and then send the data along the path. Every node in the path routes the data to the next node in the path, and determines the next node to route to based on the details of the protocol and the information encoded in the IP address. The mechanism used to transmit data is unreliable and error prone, so data packets include in their headers various kinds of error correction and detection codes and information (such as a checksum).
                -Network communication mechanisms:
                    -Packet switched network: A network that uses packet switching, which is a network communications method which groups all transmitted data into fixed-size blocks, called packets. The core idea is simply that all data meant to be communicated over a network is broken into packets, and the packets are the only things that are ever actually communicated (and may be reassembled into the original data at the destination if necessary). The other main communications method is circuit switching, which is useful when transferring time-sensitive data, such as audio or video, but when data can afford to withstand a bit of delay (eg emails, web pages, etc.), packet switching is more efficient and robust. The principle is similar to segmentation; data is partitioned into addressed packets, which are then all send down a network or other channel, and reassembled upon arrival. 
                    -Circuit switched network: A network that uses circuit switching, a method in which a line or path in a network is allocated to serve as a dedicated channel for communication between two nodes, and no data other than that exchanged between the two nodes will travel through that line. The main difference between circuit switching and packet switching is that in circuit switching, a dedicated communications path (from the source node to the destination node) through the network is selected and reserved, and all data instantly travels through the path, whereas in packet switching, once the data is decomposed into packets, each individual packet will take the most efficient path (which may vary depending on which packet is being sent) to its own destination (which may, and probably does, differ from the destination of most of the other packets). Thus, packet switching is a bit slower because the process of decomposing the data and wrapping the chunks in a packet and then selecting the path to the destination all requires overhead, as opposed to circuit switching, where data is immediately sent down the dedicated channel.
                -IP address: Unique identifiers (in IPv4, they are 32-bit numbers; in IPv6, they are 128-bit numbers) assigned to every node in the network, specifically structured to contain all the information necessary to figure out how to get from the source IP address to the destination IP address through the Internet, similar to how a geographical address is structured so that it contains all the information necessary to determine the physical location of the address (eg street name, city, state, country, etc.). Therefore, the mechanism by which IP addresses are assigned to nodes in the network is designed very specifically; a node's IP address depends completely on its location in a hierarchy of networks and sub-networks in the Internet, with the lowest point of the hierarchy typically being the local private network created by a router (so this tiny network consists of all the devices connected to the same WiFi network) and the highest point of the hierarchy being Regional Internet Registries (RIRs; large geographical regions of the world which are organizationally chunked into a single sub-network), and several hierarchical sub-networks in between, such as ISPs' networks. If two devices are in the same sub-network then they'll share the same prefix in their IP addresses; the IP addresses will only differ at the highest level in the hierarchy where the devices' location in the Internet diverges. Thus, two computers connected to the same WiFi network will be on the same local, private network, which is the lowest level of the hierarchy and so the two computers will have almost identical IP addresses, differing only in the final few digits. By contrast, computers in entirely different parts of the world will have completely different IP addresses with no elements in common. Thus, IP addresses are temporary in that although they are unique identifiers to nodes in the Internet, they are temporarily assigned and depend on which network the computer is connected to; MAC addresses, by contrast, are permanent and always refer to the same physical hardware.
                    -Internet Corporation for Assigned Names and Numbers: (ICANN) A non-profit organization responsible for maintaining several procedures and protocols in the Internet. Specifically, one department in ICANN is the Internet Assigned Numbers Authority (IANA), which overlooks global IP address allocation.
                    -IPv4 structure: IPv4 addresses are split into four chunks, each of which is a number between 0 and 255, representing 8 bits. The first two numbers are the network identifier, and specify which network the address is in (networks correspond to geographic regions and sometimes smaller entities such as corporations). For example, 192.168, 172.16 through 172.31, 10.__ (the blanks can be anything) all correspond to private networks, while 81.__, 217.__, and 62.__ all correspond to European IP addresses, and 9.__ corresponds to IBM's intranet. The last two numbers specify the host identifier, a unique identifier assigned to a device on the network.
                -Routing: IP address routing refers to the process of one node in the Internet determining the next node to forward its data packets to, in order to deliver the packets to some destination IP address. The process must balance many factors, such as path length, efficiency, redundancy, error correction, etc. Data packets travel through the Internet hierarchy, jumping from sub-network to sub-network using routers, and then from network switch to network switch within the correct destination sub-network. When a router receives packets, it reads the packet header to find the destination IP address, and if it isn't connected directly to the sub-network containing the IP address (if it is, it sends it to that sub-network), searches its routing table to find the shortest path to the sub-network containing the destination IP address, and forwards the data packets to a router directly connected to the very next sub-network in that path, and so on until the packets reach a router directly connected to the sub-network containing the destination IP address.
                    -Internet infrastructure: To understand the IP routing process, it is necessary to understand the high-level architecture of the Internet. The Internet is a network of networks, and at the highest level, the connections between large sub-networks make up what is called the Internet backbone. These large sub-networks (ie very highly connected nodes in the Internet graph) are controlled by network service providers (NSPs); NSPs peer with each other (at Internet exchange points) in order to connect the Internet. NSPs sell bandwidth and access to the Internet backbone they create to smaller networks, such as ISPs and smaller bandwidth providers, who in turn sell access to smaller entities such as consumers, local ISPs, etc. Thus, the Internet graph is hierarchical, and data travels, in the most general case, up the hierarchy, from personal routers to local ISPs to regional ISPs to NSPs, from smaller to larger sub-network, until it travels from NSP to the NSP containing the IP address of the destination, and back down the hierarchy.
                        -Routers: A router is simply a packet switcher which connects (ie has data lines to and from) two or more sub-networks; that is, a router is what connects the different components of the Internet hierarchy, forwarding packets between them, and are what allow packets to jump from sub-network to sub-network to get from the source sub-network to the destination sub-network. Although most routers are small, private home or office routers, there are more sophisticated enterprise routers used by businesses, which connect directly (as opposed to home routers, which have an indirect connection through one or more nodes) to core routers (routers which connect one or more NSPs; these routers are designed to operate in the Internet backbone, and so is equipped to handle multiple streams of data packets at once effectively, and forwards packets through very high-speed optical fiber).
                            -Routing table: A table, unique to and maintained by a router, which contains information about the topology of the Internet graph immediately around the router. Based on the IP address, a router can figure out which sub-network the destination computer is in; routing tables contain mappings from sub-networks to gateways (paths between routers to get to the sub-network). It isn't practical to contain information on every sub-network, so in practice routing tables act more as caches and only store gateways to the most heavily used sub-networks or where traffic tends to be very high.
                                -Routing protocols: Protocols that specify how routers communicate with each other, allowing information to be disseminated that aids routers in determining the shortest paths / gateways to other routers. Routing protocols and routing algorithms are used to construct default gateways and routing tables.
                            -Default gateway: If a routing table doesn't have an entry for a sub-network, the packets are sent along a default gateway, specifically to a default sub-network. This default sub-network tends to be a sub-network one level higher in the Internet hierarchy (eg local ISP to regional ISP, etc., up until NSPs are reached); at the top level, with NSPs, there is guaranteed to be information on how to get to any NSP from any other.
                        -Network switch: A networking device which connects nodes in the same network and forwards data through the network. 
                -Domain Name System: (DNS) A large distributed database system which stores the corresponding IP addresses of domain names. Web servers can be accessed through the Internet with their IP address, but because IP addresses are not human-friendly (in that it's easier to remember a name than a sequence of numbers), the Internet also contains another namespace in addition to the IP address namespace - the domain name namespace; each domain name refers to a particular web server, and DNS is essentially a lookup table for domain names to corresponding IP addresses. In addition to being human friendly, domain names are also useful because they are constant and don't change in the web server they refer to, whereas IP addresses are simply temporary labels subject to change (eg if the hosting web server switches networks, or if a new web server is used as a host, etc.). The process of using DNS to translate a domain name into an IP address is known as domain name resolution, and is one of the first things that happens when a browser parses a URL (which contains the domain name in it). DNS is regulated by ICANN and IANA, and physically stored in a distributed architecture with name servers (servers connected to the Internet which have DNS software installed) all over the world. 
                    -Domain name: A hierarchical sequence of strings separated by dots with each string naming a domain (a hierarchical system of domains and sub-domains corresponding to name servers); the tree structure is useful in quickly performing lookups.
                    -Name servers: The lookup table itself is stored on a large distributed database whose nodes are known as name servers. There are two types of name servers - recursive and authoritative DNS servers. When a domain name "www.example.com" is to be resolved, first the browser checks its cache for the corresponding IP address, and then, failing that, the operating system's cache. If the mapping isn't found, the browser makes a DNS query to a recursive DNS server, which forwards the query to root servers which would forward the query to a ".com" TLD server which would then forward the request to a "example.com" name server, which would then forward the request to a "www.example.com" name server, which would then return the IP address of the website (all of the above steps assume that each name server couldn't find the IP address mapping in its cache).
                        -Iterative approach: The above approach is recursive, but an iterative one is sometimes used, where rather than each authoritative server recursively making a DNS query to another authoritative server in a sub-domain, the servers return the IP address of the next server back to the recursive DNS server, which repeats its DNS query to that next authoritative server, until it reaches the IP address of the website itself (rather than an IP address of another authoritative DNS server).
                        -Recursive DNS server: These are name servers owned by ISPs, wireless carriers, or third parties (such as large companies whose web sites experience heavy traffic and so have their own DNS servers to minimize the chance of the website being offline because of a DNS server crash, eg Google) and are the DNS servers that web browsers and computers interact with. Computers are configured, in read-only memory during manufacture, with the IP address of a preferred (recursive) DNS server (though the request might instead be forward to the ISP (through the router), who will then perform the DNS query) to whom the DNS query is sent. If the recursive DNS server has the corresponding IP address for the queried domain name, it'll send it back; otherwise, it will recursively make the DNS query, acting as a DNS client instead of a DNS server in this capacity, to an authoritative DNS server.
                        -Authoritative DNS server: A server containing a set of mappings corresponding to a particular "zone" in the domain name hierarchy. The mappings that authoritative DNS servers store are, in general, mappings from domains to the IP address of another authoritative DNS server. Domain names are structured in a hierarchy of domains which contain sub-domains, and for each domain there is a set of authoritative DNS servers mapping that domain's sub-domains to authoritative DNS servers for that sub-domain, and so on recursively, until at the bottom level individual websites are finally mapped to the IP address of their hosting web server.
                            -Root servers: The top of the domain name hierarchy is root, and the contained sub-domains are top-level domains (eg ".com", ".gov", etc.). There are 13 root server addresses in thousands of locations, strategically selected based on highest Internet traffic; the decision to maintain precisely 13 addresses was made to account for various practical limitations in DNS, most notably the practical size of unfragmented UDP (User Datagram Protocol) packets.
                            -Top-level domain server: (TLD server) Servers corresponding to top-level domains, such as ".com" and ".gov". Below this server is the rest of the hierarchy.
            -Internet Control Message Protocol: (ICMP) Another main protocol on the network layer in the Internet protocol suite, responsible for relaying error messages between hosts. ICMP is designated protocol number 1, and is used whenever another protocol hits an error (eg that a requested service is unavailable or a host/router couldn't be reached).
        4. Transport layer: Whereas layer 3 allows nodes in a connected network to transfer data, the transport layer allows variable-length data to be transferred between nodes that reside in entirely separate (local area) networks (recall that a network is, at the physical level, a set of devices connected through a physical medium, such as a fiber optical cable or copper wire, and so the transport layer allows data transfer to transcend physical connections such as these altogether). The most common transport-layer protocol is implemented in the standard Internet stack: the TCP (Transmission Control Protocol), implemented on top of IP (Internet Protocol). This layer manages flow control (how the data flows through the network, and making sure the rate of transmission is such that a fast sender doesn't overwhelm a slow receiver), segmentation (dividing data into multiple parts that can be transmitted individually and recombined later) and de-segmentation, reliability, multiplexing (combining multiple data streams into a single entity to be transmitted at once over a medium, in order to share an expensive medium of transmission), congestion avoidance (load balancing and other congestion control techniques that prevent too much data from overwhelming specific paths in the network), and error handling for layer 3. The transport layer also offers connection-oriented communication (as opposed to connectionless communication; connection-oriented means that a communication session must be established between the source and receiver before any data is sent, after which all data is sent sequentially, reliably, along the same path, and in the right order).
            -Transport protocols: The OSI model defines five protocols for transferring data between networks, TP0, TP1, TP2, TP3, and TP4 (a TPN is Transport Protocol of class N), with the protocols ascending in complexity by class.
                -TP0: Performs SAR (Segmentation And Reassembly; breaking large data into chunks that can be individually transported, and reassembling at destination; useful to decrease latency or meet connection size restrictions) tasks.
                -TP1: Also performs SAR tasks, but performs error recovery in conjunction. It does this by assigning a number to each PDU sent, so when the receiver receives a PDU it's able to confirm its arrival, thereby allowing the source to resend any unconfirmed PDUs.
                -TP2: Performs SAR tasks and error recovery, but also adds multiplexing and demultiplexing (sending multiple signals over the same communications channel by combining the signals into a single, more complex signal) capabilities. 
                -TP3: Similar to the lower class transport protocols, but with a bit more redundancy; this protocol is able to reinitiate closed connections.
                -TP4: The OSI equivalent to TCP; performs all of the functions included in lower class transport protocols, but adds reliable transport functionality to ensure that data reaches the recipient. TP4 is the most commonly used protocol.
            Full chart of different transport protocol classes and functionality:
                Feature                       | TP0 | TP1 | TP2 | TP3 | TP4
                Connection-oriented network   | Yes | Yes | Yes | Yes | Yes
                Connectionless network        | No  | No  | No  | No  | Yes
                Concatenation and separation  | No  | Yes | Yes | Yes | Yes
                Segmentation and reassembly   | Yes | Yes | Yes | Yes | Yes
                Error recovery                | No  | Yes | Yes | Yes | Yes
                Reinitiate connection         | No  | Yes | No  | Yes | No
                Multiplexing / demultiplexing 
                  over single virtual circuit | No  | No  | Yes | Yes | Yes
                Explicit flow control         | No  | No  | Yes | Yes | Yes
                Retransmission on timeout     | No  | No  | No  | No  | Yes
                Reliable transport service    | No  | Yes | No  | Yes | Yes
            -Transmission Control Protocol: (TCP) A central protocol used widely in the Internet protocol suite, built on top of and designed to complement the IP (Internet Protocol). TCP provides reliable, ordered, error-checked transmission of data (specifically, a stream of bytes) from a source application to a destination application, both of which are running on hosts which communicate over an IP network; IP is not sufficient to communicate over the Internet due to its unreliability. Many applications central to the Internet, such as the World Wide Web (what most people are referring to when they think of the Internet; more precisely, the World Wide Web is an information space in which documents (and other web resources such as files, web applications, etc.) are identified by URLs (Uniform Resource Locater; a reference to a web resource that specifies its location in a computer network, and a mechanism for reaching and retrieving it; all URLs follow a specific syntax which allows the URL string to specify all relevant information) and linked together with hypertext links), email, and file transfers, rely on TCP. Unlike IP, TCP is optimized for accuracy instead of speed of delivery, and so isn't suitable for time sensitive data transfer, such as VoIP (voice over IP).
                -Structure: TCP is built on top of IP, whose purpose is simply to get data (in the form of packets) from one computer to another through the network; TCP handles handshakes, security, reliability (IP is fast but unreliable and so used in situations where a larger threshold of data loss or corruption is acceptable but speed is vital), etc. and is an intermediate abstraction between applications and IP. The Internet's load balancing, congestion, etc. and the inherent unreliability of IP often cause data packets to be lost, duplicated, or arrive out of order; it's TCP's job to detect and fix these errors (eg by requesting retransmission of data, arranging out-of-order packets, etc.).
                    -Positive acknowledgment with retransmission: To guarantee accurate data transfer over unreliable networks, TCP requires the destination of a data stream to respond to the source and acknowledge that it received all the data, uncorrupted. In case the acknowledgment message is lost or corrupted, the source node maintains a timer for when each packet was sent, and if an acknowledgment message for a given packet isn't received then the source automatically retransmits that packet after the timer expires.
                    -Segments: TCP partitions the data stream to be sent into fragmented segments, which IP then wraps into IP packets (by wrapping a header containing meta-information relative to IP, such as the source and destination IP addresses, a checksum, etc.). TCP reassembles the segments (after unwrapping the IP packets) at the destination node. The process of splitting a data stream into segments involves more than slicing up the stream; each partition is wrapped with a TCP header to create the TCP segment.
                    -Sequence numbers: TCP uses a sliding window protocol, in which each fragment in a set of fragmented data is assigned a unique, consecutive sequence number, useful in uniquely identifying a fragment and in reassembling the fragments are the destination. In TCP, the fragments are each individual byte of data in the packet.
                        -Initial sequence number: The first byte of data in the set receives the initial sequence number (ISN). In an ideal world, the ISN would always be 1, but this poses problems if the TCP connection is interfered with or corrupted, so for reliability the ISN is usually a randomly selected (for security reasons) 32-bit number. Each subsequent byte consecutively receives the next sequence number (ie ISN + 1, ISN + 2, ...).
                        -Acknowledgment number: Bytes are also assigned corresponding acknowledgment numbers used to determine if any data was lost during transit.
                    -Three phases: The TCP protocol can be divided into three phases - connection establishment, data transfer, and connection termination. During the execution of the TCP protocol (ie during the three phases), the connection is managed through the Internet socket (a network socket (a handle (an abstraction / "wrapper class" of the idea of a reference (any data that provides indirect access to other specific data, such as a pointer, the physical memory address, the offset of the address from some base address, etc.) to a resource (any physical or virtual component in the computer, such as files, memory, network sockets, external devices, etc.)) that the operating system can use to send data through) used to connect to the Internet), which acts as the local end-point for Internet communication; at any given time, the Internet socket has a particular state, which specifies specifically what the socket is doing at that moment (eg polling for incoming data, sending data out, waiting, etc.) and over the course of the TCP connection, the Internet socket's state changes many times. State changes are communicated between two parties with signals (eg SYN, ACK signals); a signal is essentially an empty TCP message with certain flags (ie bits) set in the TCP header, which by default are not set.
                        -Connection establishment: A TCP connection is established with a three-stage handshake. For the connection to be established, both parties must send a SYN signal (short for "synchronize"; this signal is used to propose a connection) and, upon receiving the other party's SYN signal, send back an ACK signal (short for "acknowledge"; this signal is used to confirm a proposed connection). The source sends a SYN, and the destination replies with an ACK. The destination then sends a SYN of its own, and the source replies with an ACK. Because it's inefficient for the destination to send the ACK and SYN in separate messages, it sends a single SYN-ACK signal which accomplishes both; hence, the protocol has three stages instead of four. In preparation for the TCP connection, both the client and server create a fresh TCB (transmission control block; a data structure used to store important data about the connection; specifically, a TCB is an entry in a lookup table maintained by the operating system which maps a TCP session (which is uniquely identified by the device address and the port the session uses; this is because TCP sessions don't have their own session identifiers) to a running OS process using that TCP session). Steps:
                            1. Opening a socket: A socket is an endpoint on a device for receiving and sending data over a network, and can be in one of several different states. There are two ways to open a socket in TCP - active and passive opens. An active open is typically performed by a connection initiator, in which the initiator seeks to establish a connection and sends a signal communicating this. A passive open is less complex and entails sending a signal back which lets the other party know that the sender has opened a socket for data transfer and is listening (ie awaiting data to arrive). Before the three-way handshake in TCP, both parties begin in the CLOSED state. A server connected to the Internet has now way of knowing who a potential client will be, and may have to deal with multiple clients at once, and so will specify a generic socket to remain open to all communication; this socket switches state to LISTENING when the server goes online.
                            2. SYN: In order for the client and server devices to communicate via TCP, they must exchange information about where (specifically, which socket, identified with a socket number) information should be sent to on the receiving device. The client performs an active open by sending a SYN signal to the server, initiating the connection. The client switches state from CLOSED to SYN-SENT. The client also sets the ISN of the first segment sent to a random number.
                            3. SYN-ACK: Upon receiving the active open SYN signal (which comes from a generic, unspecified, open port), the server switches state from LISTENING to SYN-RECEIVED and replies with a SYN-ACK signal, which acknowledges arrival of the SYN signal, confirming a connection, and also performs a passive open which lets the client know that the server has opened a TCB and specific socket for the TCP connection, and is listening. The server also sets the acknowledgment number to one more than the SYN segment's ISN, and sets the ISN for the SYN-ACK packet to a random number.
                            4. ACK: Upon receiving the server's SYN-ACK, the client replies with an ACK of its own, acknowledging the server's passive open. The client switches state from SYN-SENT to ESTABLISHED, as does the server upon receiving the client's final ACK signal. The client also sets its acknowledgment number to one more than the ISN of the SYN-ACK segment.
                        -Data transfer: This is the actual data transfer TCP provides.
                        -Connection termination: Unlike connection establishment, in connection termination the side initiating the termination must behave differently than the side receiving the termination signal, and so rather than a 3-way handshake, connection termination involves two separate 2-way handshakes. This discrepancy comes from the fact that the party sending the termination request will only do so after it's ready to terminate, meaning the application side of that party is already finished, whereas the party receiving the request must inform its application side of the termination and wait for acknowledgment from the application. Both sides send a FIN signal and wait for an ACK signal before closing the session (which basically means informing the network device to cut the connection to the given IP address, and clearing metadata related to the session). Typically, sessions are terminated when an application on one of the devices informs its TCP layer that it no longer needs to connect with the other device. Steps:
                            1. Client FIN: The client application signals to its TCP layer that the session can be closed, and the client TCP layer sends a FIN signal to the server. The client transitions state from ESTABLISHED to FIN-WAIT-1. At this stage the client can still send data to the server, but the client TCP layer will stop accepting input from its application layer.
                            2. Server CLOSE-WAIT: The server's TCP layer receives the client's FIN, sends back an ACK signal, and informs the application using the server that the connection will be terminated. The server now waits for its application layer to close. It transitions state from ESTABLISHED to CLOSE-WAIT.
                            3. Client FIN-WAIT-2: The client receives the ACK, and is now waiting for the server to send its own FIN. It transitions state from FIN-WAIT-1 to FIN-WAIT-2.
                            4. Server ACK: After the server's application layer informs the server's TCP layer that it's done (which will happen after the application is done doing any cleaning up that needs to be done), the server responds by sending its own FIN signal to the client. The server now transitions state from CLOSE-WAIT to LAST-ACK.
                            5. Client ACK: The client receives the server's FIN and replies with an ACK. It transitions state from LAST-WAIT to TIME-WAIT.
                            6. Server closes: The server receives the client's ACK and closes its end of the connection, transitions state from LAST-ACK to CLOSED.
                            7. Client closes: The client waits for a period of time equal to double the maximum segment life (MSL) to ensure that the ACK was received, and then closes the connection, transitioning state from TIME-WAIT to CLOSED.
            -User datagram protocol: (UDP) Another core protocol of the Internet protocol suite on the transport layer. Compared to TCP, UDP is a simple protocol; it's connectionless (ie there's no session establishment or anything; data is simply immediately sent, out of order, unreliably, and its arrival is unconfirmed) and offers little protection against any inherent unreliability of the network. Thus, it's useful in cases where error correcting and airtight reliability aren't important (as opposed to TCP), and instead offers much faster speed gained by bypassing the overhead associated with error correcting and reliability. Unlike IP, UDP does offer error detection (though not correction), as it by default includes a checksum in data packets, and also allows packets to be addressed to specific ports on the destination and sent from specific ports on the source, which allows for differentiating between multiple services. UDP works well in unidirectional communication.
        5. Session layer: This is the first of the three upper layers of the OSI model, which are concerned with software application issues rather than details of network and data transfer implementation like the first four layers. Whereas the purpose of the transport layer is to relay data packets between two nodes, even nodes in entirely different networks, reliably, the session layer is responsible for using the transport layer to open, manage, and gracefully close communication sessions between hosts. In managing the session, it covers many error cases the session may encounter (eg recovering a lost connection, automatically restarting after a period of inactivity, handling simultaneous traffic along both directions (ie from host A to B and B to A at the same time), session checkpointing, etc.). Note that in the Internet Stack, TCP covers much of the functionality (ie session establishment, management, and adjournment) offered by the session and presentation layers of the OSI model.
        6. Presentation layer: The presentation layer is responsible for translating data between the representation used by the application layer above it and the raw network and session layers below it, accounting for differences in formatting (eg translating various machine differences arising from different operating systems, computer architectures, and network devices, to a common standard, between network endpoints being managed by programs in different programming languages, character-code translation (converting encoded characters (eg ASCII, UTC-8, etc.) to integers), etc.) and integrating into a common standard understood by the layers below. The presentation layer is also responsible for any necessary data transformations such as compression or encryption, and thus both speeds up data transmission (especially for large, highly compressible data transfers such as graphics (eg streaming video)) and is the layer that provides secure communication by encrypting and decrypting data. Notice that in some specific forms of communication, the presentation layer is not required at all (eg public, non-real-time communication with a pre-established data format).
            -Transport layer security (TLS; formerly Secure sockets layer (SSL)) An important and widely used presentation-layer cryptographic protocol used to secure communications over a network. This is accomplished by encrypting all data transferred between two hosts, typically by first using public-key encryption to agree upon a set of symmetric keys, which starts the TLS session and allows the hosts to communicate with symmetric key encryption from there onwards. Additionally, the identity of both hosts can be authenticated (this is optional but typically is required for at least the server) by using public-key encryption to verify a digital signature. Data integrity (ensuring that the data is not tampered with during transit) is maintained by including a message authentication code (usually a hash of the private key with the message cleartext, providing both integrity and authenticity) with the message.
                -Protocol:
                    1. TLS handshake: An initial exchange of relevant information, such as which cryptographic algorithms to use, the public key, etc., which establishes the session. The handshake occurs in two sub-steps:
                        a. Negotiating the cipher suite: A cipher suite is a set of cryptographic algorithms (eg RSA for public key encryption, SHA-256 for cryptographic hash functions) and key sizes (in bits); in this phase, the client and server negotiate which cipher suite to use. 
                        b. Authenticating the server: (optional) This step is optional but commonly implemented in secure applications. It involves authenticating the server as being who the client expects it to be. Clients are typically not authenticated because in the client-server model, usually a server will be serving many public anonymous clients (eg a website). This is done by having the server provide a digital signature.
                    2. Sending data: After the handshake, the client and server will have a shared secret key. This shared secret is passed through the cryptographic hash chosen in the handshake to produce an HMAC, which is appended to the message cleartext and, along with the secret key and symmetric key encryption algorithm chosen in the handshake, encrypted using the encryption algorithm. The encrypted, hashed data can be securely sent over a network and decrypted by the other party.
        7. Application layer: The application layer is the top layer of both the OSI model and the Internet Protocol Suite (ie the TCP/IP stack), and consists of functionality directly used by user-facing applications, such as Internet browsers, email, etc.; essentially any application (computer, mobile, or otherwise) which requires access to a computer network (usually the Internet) will probably employ, either directly or indirectly through other layers or 3rd party apps, the application layer (most of the time, a computer's operating system automatically uses the application layer to access a computer network to provide programs with resources, without the programs ever having to know that the resources were on another machine in the network (rather than the same machine) at all). Because there is no layer above the application layer, its main responsibility is to provide functionality as abstracted as possible which end users will require. The application layer standardizes all communication across a computer network, abstracting it sufficiently for users to carry out a diverse and basic set of functionality on the network without worrying about any of the internal details of the network; ideally, the abstraction should be sufficiently strong for the user to be able to assume that the resources needed from across the network are essentially on the same machine.
            -It's important to acknowledge that the top 3 layers in the OSI model are usually not distinguished in the Internet Stack (ie TCP/IP); the "application" layer is right on top of the transport layer and uses TCP, handling the functionality of the top 3 OSI layers.
            -Hypertext transfer protocol: (HTTP) An application layer protocol which governs the communication of files (specifically, hypermedia files, which includes text, graphic images, sound, video, multimedia, etc.) through the World Wide Web; it's the underlying protocol of the entire World Wide Web and serves as its foundation of data communication. As an application layer protocol, HTTP is intended to run on top of transport layer protocols - specifically, on top of TCP - though it can be adapted to run on other protocols as well, such as UDP. HTTP is a request-response protocol (in the client-server architecture), which means that when a client needs to retrieve information from some source through the Internet, it makes an HTTP request for the information, entailing which information it needs. The server receives the HTTP request and replies with resources (an umbrella term referring to any requested content) encapsulated in an HTTP response, which contains the requested information and some metadata including a status code (an ID associated with a particular connection status, eg 200 for "OK" or 404 for "NOT FOUND") representing the completion status, or possibly an error message. These responses are typically HTML files and other relevant content; any computation necessary to deliver the appropriate response is done by the server. HTTP is designed with caching and other optimizations in mind, allowing intermediate (between the client and server) nodes in the network to speed up the network or reduce network traffic by caching high-demand HTTP content (an extreme example of this is a client's web browser, which caches HTTP responses).
                -HTTP sessions: Like many other protocols, HTTP makes use of sessions, characterized by HTTP session tokens. This is because HTTP is stateless; sessions allow a server, which is typically receiving HTTP requests from many different clients at once, to associate the right HTTP requests to the right client. Thus, unlike cookies (which are similar tokens but which are transmitted in the URL), session information is primarly stored on the server. HTTP sessions are initiated when a TCP connection is established on port 80 of the server.
                -Request verbs: The protocol defines a set of available request types which delineate the kind of resource the request is requesting. Every HTTP request must fall under one of the following request methods, known as "verbs". Requests typically consist of the verb in question and a URI on the server as an argument (specifying, for example, where to get the data from or where to post data to).
                    -GET: This is the most basic and general request method, and represents a request which simply requests some specified representation (eg a specific file type) of some resource (eg a file). Thus, GET requests only fetch data from the server, and don't affect the server in any other way.
                    -HEAD: This is the same as a GET request, but asks for the response without the response body. Thus, this is useful for getting HTTP response metadata and content headers without having the server send over the unnecessary corresponding data.
                    -POST: This is, in some sense, the opposite of the GET method in that POST requests have data associated with them, and request that the server store the data as a resource at the specified URI. Thus, this is the main request used to actually modify the server. POST requests are useful whenever the server needs to store information about a client, such as a username or password, an article or video, etc. If the server already has data stored at a URI, it's updated as specified in the POST request.
                    -PUT: This request method is similar to the POST request in that it can also be used to create a resource associated with a URI if the server doesn't already have anything associated with the URI. However, if the server already has data at the URI, then whereas POST requests are meant to update the resource, PUT requests are meant to completely override them. Thus, POST requests are not idempotent whereas PUT requests are (analogous to the difference between "x = 5" and "x++" for a variable x).
                    -DELETE: Deletes the resource at a specified URI.
                    -TRACE: This method simply echoes a received request, allowing the client to see the path of the request and any modifications made by intermediate network elements.
                    -OPTIONS This method simply requests a list of the HTTP request verbs supported by the server.
                    -CONNECT: This method asks the server to convert the existing HTTP session to a transparent TCP/IP tunnel.
            -File transfer protocol: (FTP) An application layer protocol used to transfer general files between nodes in a network. FTP offers authentication (username and password) functionality, and can be run with TLS to secure the file transfer. FTP uses the traditional client-server model, and begins by establishing a TCP connection from a random port of the client to port 21 of the server. FTP actually uses two simultaneous communication channels, which are separate TCP connections - the control channel, which is meant for the communication of commands and responses, and the data channel, which is used for actually transferring files. The use of two channels allows for communication between the client and server without having to wait for a data transfer to finish. After the control channel established (this is the connection referenced above), there are two ways FTP can create the data channel - active and passive. In active mode, the client chooses a random port on its system to listen for FTP packets from the connection, and sends the port number to the server, which replies by initiating the data channel from its port 20. In passive mode, the client is the one establishing the data channel - the client sends a PASV command to the server, requesting for the server to choose a port for it to listen on, and then establishes the connection from an arbitrary port to the server on the server's chosen port. Active mode is the default way FTP establishes connections, but passive mode is useful when the client is behind a firewall that prevents the server from establishing a connection to the client. After either passive or active mode is used to establish the data channel, files can now be transferred. The server uses the command channel to respond with status codes, and the data channel is used for actually transferring requested files.
                -Data channel representations: When transferring data over the data channel, it must be in one of the four following data representations, often making it necessary to convert on the server's end and de-convert on the client's end.
                    -ASCII mode: Data is sent as ASCII characters. This mode is only used for text data.
                    -Image mode: Also known as binary mode, the server sends the data byte by byte, with the client capturing bytes from the stream as it receives them. This is the most common data representation for non-text data.
                    -EBCDIC mode: The EBCDIC is another character set used as an alternative to ASCII. If two hosts are using this character set, this data representation is used.
                    -Local mode: If two computers have identical setups, they can use local mode to send data in a proprietary format rather than converting it.
            -Simple Mail Transfer Protocol: (SMTP) The Internet standard for the transmission of electronic mail (ie email), used by user-level client mail applications (eg Gmail, Microsoft Outlook) to send an email to a mail server, which then handles the forwarding of the message to the appropriate recipient. SMTP runs on top of TCP. The retrieval of messages is usually handled by the same client application, using the IMAP or POP3 protocols. As specified below, the full path of an email is: user software -> MUA -> MTA -> ... -> MTA -> (optionally MRA) -> MDA -> MUA. Each transmission (ie "->") uses SMTP, with the exception of transmission from MDA to MUA, which uses either IMAP or POP3. In more detail, first an MUA submits email to an MSA (using SMTP on TCP port 587), which then transmits the email to an MTA (also using SMTP, but on port 25). It's not uncommon for the MSA and MTA to be different software on the same machine, or on machines in the same local network; the two are often implemented with the intention to run together. This MTA, the first one to receive the email, is one of the boundary MTAs. It first queries the MX record (using DNS) for the recipient email's domain (eg gmail.com, yahoo.com, etc.), which will contain the hostname (human-readable nicknames or labels assigned to nodes in a computer network; in the Internet, it's usually a combination of the host's local name and its parent's domain name, separated by dots) of a mail server designated to accept emails on behalf of the recipient, with the intention of forwarding to the recipient. Upon receiving the hostname, the MTA attempts to establish an SMTP connection to this mail server (representing the recipient). Once the connection is established, the MTA sends the email to the target host through a series of hops from MTA to MTA, until it reaches the target host, which forwards the email to an MDA, which then either stores the email in its own storage or forwards it over a network (using SMTP or LMTP) to a local mail server. Authenticated MUAs then periodically retrieve emails from this trusted storage, using either IMAP or POP3.
                -Message transfer agent: (MTA) The SMTP protocol specifies a client-server architecture for the sending (from clients) and receiving (for servers) email; MTAs implement both these portions of the protocol, and thus allow for receiving and sending of email. MTAs use TCP port 25 to send and receive email. MTAs are often referred to as "mail servers".
                -Mail user agent: (MUA) Also known as an email client, an MUA is software used be users to access and manage their email, with the capabilities of sending and recieving email at the user-level. Examples include Gmail, Yahoo Mail, and Microsoft Outlook. MTAs operate recursively, first searching for an MDA to forward the email to directly, in case the recipient is in the same network, and otherwise relaying (a term used the same way "routing" is used in IP) the email to another MTA in a higher network (specified by SMTP). Each MTA adds a "received" trace header to the top of the email packet's header, so that the packet incrementally records its path from source to recipient.
                -Mail submission agent: (MSA) Software that receives email from an MUA and forwards it to an MTA. Unlike an MTA, and MSA can't receive emails from arbitrary sources, only an MUA. It can be the case that an MUA relays an outgoing email directly to an MTA, bypassing any MSAs along the way, but it's not guaranteed that an MTA will be locally available to the MUA, whereas an MSA will be. MSAs use port 587 to receive email.
                -Mail delivery agent: (MDA) Software that handles the delivery of emails to a local recipient's mailbox, usually by implementing LMTP.
                    -Local Mail Transfer Protocol: (LMTP) A simple derivative of SMTP designed for situations in which the recipient doesn't have access to a mail queue, an essential component in SMTP. In emails with multiple recipients, it's not possible to SMTP to differentiate success or failure of delivery between all or any of the recipients (all emails are delivered correctly or not at all), whereas LMTP can handle success / failure on a per-recipient basis, allowing the client to handle the queueing. LMTP is meant for use in local area networks, and runs on top of TCP (though using a different port than 25, which is used by SMTP).
                -Mail retrieval agent: Software that fetches email from a remote MTA and works with an MDA to deliver the email to the recipient. MRAs are not a canonical component of SMTP or the email architecture.
-Onion routing: Onion routing is a technique used to implement a communications protocol that allows for anonymous, confidential communication over a computer network. The protocol defines an overlay network, which is a computer network that runs on top of another computer network (in the case of Tor, the underlying network is the Internet). As in any other computer network, the message will be passed node by node through a series of intermediate computers until it reaches the destination; these intermediaries are known as onion routers. The first thing that happens before sending a message is the sender asks a special kind of node, known as a directory node, for a list of available nodes. To maintain confidentiality, the message sender then computes the network path the message needs to take to reach the destination (this is done randomly for security), i.e. exactly which nodes from the list that will be used to transmit the message; next, the sender "wraps" the message in layers of encryption by iteratively encrypting as many times as there are intermediate nodes, with each layer also containing the (encrypted) location of the next node in the path. When the sender sends the wrapped message, routers iteratively decrypt the topmost layer of encryption, revealing the next node to send to, and transmits the message, now encrypted with one fewer layer than when the node received it, to the next node, which repeats the process. This way, any intermediate node only knows the identity of the node that sent it the message and the identity of the node it passes the message on to, and can't read the message contents. No node knows the number of nodes in the chain, or who the sender or recipient nodes are. One exception to this is that exit nodes, ie nodes at the end of the chain, which send the message to the intended recipient, are able to determine their position in the chain. All the encryption is handled with public-key cryptography - the directory node provides, in addition to a list of available onion routers and their IP addresses, each node's public key (of course, nodes maintain a private key they use to decrypt the message). Thus, data travels from a source computer, through the router and the ISP to an onion router; ISPs in the middle, which usually log connections, their duration, and the amount of data transferred (the data itself can be encrypted with eg TLS), are unable to tell what the true destination of the data is, or even how large the data is (since size of the message depends on the number of nodes in the path, which is unknown).
    -Weaknesses: While it's not possible to keep records of entire connections between the source and recipient, connections between specific computers (ie the nodes) are logged. Attackers could try to look for patterns of data sent between computers they've logged, and try to reconstruct the chain of nodes by searching for data of the exact same size transmitted between computers. This is known as traffic analysis. Another major flaw is that exit nodes are required to decrypt the message's final layer, revealing the raw message. This can be easily solved simply by encrypting communication with the destination as well (eg by using HTTPS), in addition to onion routing.
    -Tor: Tor, an acronym for "the onion router", is the most widely used implementation of onion routing, where onion routers are implemented with volunteer computers known as relays. It's used to conceal users' locations and usage, even from attackers performing traffic analysis or network surveillance.
-Yamanaka factors: Four genes encoding four transcription factors (proteins which regulate the speed of transcription (the first step in gene expression, where DNA is copied onto messenger RNA) and hence regulate the expression of a gene), respectivey, which can reprogram adult cells to become pluripotent stem cells. The resulting cells are known as induced pluripotent stem cells (iPSCs). The reprogramming technology was developed by Shinya Yamanaka in Kyoto, Japan in 2006, for which he was awarded the 2012 Nobel prize in biology. iPSCs have potential in the field of regenerative medicine for the same reason that embryonic stem cells are appealing - iPSCs could represent a source of cells which can be grown and used to replace lost tissue or cells in a subject.
    -Cellular differentiation: The process by which cells change from one cell type (cell types are classifications based on appearance and function of cells (as opposed to genotype, which may be identical (though the degree to which certain genes are regulated might differ among cell types))) to another, most commonly from a more general type to a more specific one. Stem cells are undifferentiated cells with the ability to differentiate into a more specific cell type. In animals, cell types fall into three generic classes, known as germ layers - endoderm (interior stomach lining, gastrointestinal tract, the lungs), mesoderm (muscle, bone, blood, urogenital), and ectoderm (epidermal tissues and nervous system).
        -Potency: Different stem cells can have the ability to differentiate into a wider or narrower range of cell types; the more cell types a stem cell can differentiate into, the more potent it is.
            -Pluripotent: Stem cells which can differentiate into any of the three germ layers (though not necessarily into any cell type within the germ layer; at least one cell type in that class is guaranteed).
            -Totipotent: A stem cell which can differentiate into any cell type. In animals, only zygotes and resulting blastomeres (a cell type resulting from the division of the zygote) are totipotent; in plants and other kingdoms, totipotent cells are much more common, and totipotency can often by induced chemically in plant cell.
-Vocal register: The full possible range of tones that can be produced by a human voice. Such tones depend on vibratory patterns in vocal folds. The registers include modal voice (normal speaking), vocal fry (the lowest register), falsetto (the second highest register), and the whistle register (the highest register).
-Weber-Fechner law: A law that actually relates two separate laws in psychophysics (a field which studies the connection between the physics of neurology and the perceptons, qualia, and psychological behavior they produce) - Weber's law and Fechner's law. Weber's law is a statement on the Just Noticeable Difference (JND; this is the minimum difference in a continuous stimulus which will be detected by a mind (e.g. if someone's holding a weight, there's a minimum amount of weight that can be added below which the added weight won't be felt by the person)), and states that the JND is always a constant fraction of the initial stimulus. The law has been shown to break down at stimuli that are near the limits of perception. The constant propotionality factor between the JND and initial stimulus is known as Weber's constant. Fechner's law, though based on experimental data, can be mathematically derived from Weber's law. Different people percieve the same objective stimulus differently, indicating a distinction between the objective stimulus and the subjective perception of that stimulus. Fechner's law essentially states that the intensity of the subjective perception is proportional to the logarithm of the intensity of the objective stimulus (and not to the intensity of the stimulus itself, as one might intuitively expect). This means, for instance, that geometrically varying stimuli, such as a sound that doubles in volume periodically, will be percieved as arithmetically progressing in intensity by the mind (in fact, this is why decibels, the unit of measurement of sound, are proportional to the logarithm of sound intensity (which is proportional to the amplitude of sound waves, which governs their energy)). The two laws, taken together, are often referred to as the Weber-Fechner law; the law explains the rule of thumb that humans think multiplicatively, not additively. As with Weber's law, on which Fechner's law is based, the law breaks down towards the extemes.
-Jevons's paradox: The counter-intuitive observation that as technologies become more efficient at using resources, the rate of resource usage tends to rise as a result. This is caused by the resulting increase in demand that the increased efficiency brings, and is counter to the often made assumption (by policy makers and governments) that increased efficiency should lower total resource usage. The increased efficiency of resource usage lowers the relative cost of the resource, increasing the demand for the resource, which drives economic growth which in turn further increases demand, thus causing a feedback loop. This can be enough to overcompensate for the lessened amount of resource used per unit due to the better efficiency. The paradox gets its name from the English economist William Jevons, who observed the effect in technologies for burning coal for energy - as the technology progressed and burned coal more efficiently, the total demand for coal increased. Although the effect doesn't necessarily occur in every case of technological progress, it is seen often enough to call into question the assumption that technological progress will lessen fuel consumption.
-Induced demand: Also known as latent demand, induced demand is an economic phenomenon in which increased supply leads to increased demand, contrary to the conventional theory of supply and demand as inversely related forces. This is most commonly observed in traffic analysis - building more and wider roads can often end up increasing congestion, instead of reducing it, since the more highway space there is, the more people are encouraged to use cars in the first place, as opposed to other modes of transport which don't cause congestion and traffic. Put simply, drivers drive less when the cost of driving is high; more highways means the cost of driving is, initially, lower, which means people will, on average, drive more. This of course increased traffic from the expected amount (ie the amount of traffic expected from the new highway space and old driving habits) since driving habits have changed, which increases the cost of driving. The feedback loop continues until it reaches equilibrium, effectively cancelling out any relief in congestion load afforded by the increased highway space. Another area where induced demand is relevant is in the black market for drugs.
-Negativity bias: An observed bias in human psychology in which humans tend to percieve negative events more intensely than positive events of equal magnitude. In other words, humans tend to be disproportionately risk averse. The phenomenon is well documented in studies that show, for example, that the brain registers negative stimuli much faster than positive ones (eg being shown an angry face or a happy one, for less than a tenth of a second), and moreover that stronger emotional responses are produced to negative visual stimuli than equally loud and bright positive visual stimuli. There's even evidence that around two-thirds of the neurons composing the amygdala fire on negative stimuli. Negative stimuli are thus transferred from short term to long term memory much more quickly (less than a second) than positive stimuli (which can take up to a dozen seconds). The mammalian brain is thus prone to overestimating threats and underestimating opportunities and resources. One proposed explanation for why this is the case is that events the brain registers as positive, such as mating, food, etc. can often be postponed to later in time and pursued later, without a serious penalty for doing so, whereas negative events, such as predators or disasters, only need to successfully strike once to inflict serious, often fatal damage. The penalty for not avoiding most negative stimuli is far greater than the penalty for temporarily avoiding a positive stimulus, simply because the world is, to most animals, a far more dangerous place than a rewarding one.
-Terror management theory: (a.k.a. TMT) A theory initially proposed by psychologists Jeff Greenberg, Sheldon Solomon, and Tom Pyszczynski which attempts to explain conventional social and cultural values and norms. The idea is that the human brain, with a deeply primal self-preservation instinct and desire to live, inevitably comes to the conclusion at some point that it will eventually die, creating a deep seated cognitive dissonance which produces terror in the mind. Psychologically, the mind reacts and copes with this terror by embracing social and cultural systems which provide life with meaning and value. Prominent cultural beliefs that seem to conform to the theory primarily include belief systems which allow one to attain immortality, not in a biological sense (which is currently impossible) but rather in a symbolic sense - eg believing in an afterlife, having kids to "keep a part of you" alive, leaving behind a strong legacy to live on through others' memories, making a significant impact on the world (through humanitarian, scientific, artistic, technological etc. work) to leave behind a lasting contribution to the species, etc. TMT goes beyond these social beliefs and links many other cultural values to this fear of death, such as patriotism, sexual identity and beliefs, the human superiority over animals, and others. The commonality between all these values is that they allow the individual to symbolically live longer than their physical life, by being part of some abstract whole that's greater than the self, and often give the self meaning beyond its biological limitations (which are of course responsible for death), eg through the belief in a soul or something like it. According to TMT, because all these cultural values by definition give life meaning, achieving the goals set by these values and aligning one's life with them is strongly linked to one's self esteem. The theory is based on many of the ideas proposed by Freud, Kirkegaard, and Rank - whereas Freud ascribes sexuality and aggression to be the two dominating forces that account for most of human behavior, TMT replaces these motivators with the fear of death.
-Intention-behavior gap: (***INCOMPLETE***)
-Pygmalion effect: The observed sociological phenomenon wherein the imposition of higher expectations from a "leader" or authority figure on a group of subjects tends to increase performance in the subjects. Conversely, it's also been observed (and is sometimes referred to as the golem effect) that lower expectations correlate with lower performance. The effect has been difficult to accurately measure; the one major study, the Rosenthal-Jacobsen study, contains within it signs of correlation but inferring causation from these signs would be erroneous. The study gave a group of elementary school students an IQ test at the beginning of a school year, and told the teachers that certain (randomly selected, unbeknownst to the teachers) students were "intellectual bloomers"; these students' IQ scores at the end of the study showed a statistically significant increase relative to those of the other students.
-Gaia hypothesis: The Earth has long supported geological conditions that are highly amenable for life. The Gaia hypothesis, first proposed by chemist James Lovelock, conjectures that existing life forms on earth interact with their inanimate surroundings and with other organisms in such a way as to form an emergent, self-regulating system that maintains these conditions for life. Thus, the resiliency of life might be explained by the existence of life itself forming a positive feedback loop wherein the conditions of life are constantly perpetuated by existing life forms and their interactions. Such interactions would entail the impact of life forms on aspects of the biosphere ranging from global temperature, ocean salinity, atmospheric oxygen levels, etc. A necessary condition and consequence of the Gaia hypothesis is that organisms would need to co-evolve with their environment; clearly, organisms evolve to adapt to a shifting environment, but according to the Gaia hypothesis, the environment should be shifting according to the organisms that inhabit it as well. The entire biosphere, thus, would undergo a form of homeostatis that's maintained by the emergent influences of different biota. According to the theory, the resiliency afforded by the self-regulating, homeostatic properties of this emergent system would hence allow the Earth biosphere to react and adapt to cosmic changes (for instance, the luminosity of the sun has increased by about 30% in the past four billion years, and yet life persists) in such a way as to effect the continuation of life.
    -Scientific credibility: Scientific evidence of the Gaia hypothesis is mixed and scattered, and though there do exist examples of such symbiotic behavior between the inorganic biosphere and life forms creating a self-regulating feedback loop, the theory has nonetheless been validly criticized; as of yet, there is no consensus. For instance, proponents of the Gaia hypothesis have tried to use organic negative feedback loops to explain the adaptation of the Earth's global temperature to the rising luminosity of the sun (this is known as the CLAW hypothesis), which has increased by about 30% in the past four billion years, with the temperature on Earth remaining more or less constant. Similarly, ocean salinity levels have remained at very close to 3.5% for the past several billion years; seeing as most cells cannot tolerate levels about 5%, and that there is no conclusive theory explaining the stability of ocean salinity thus far, the Gaia hypothesis remains one of a few different possible explanations. The Snowball Earth hypothesis (hypothesis that proposes that at least one point in geological history (likely about 650 million years ago), the Earth's surface became completely or almost completely frozen solid in ice; the hypothesis is not conclusive, and has both proponents and opponents, each with evidence in favor of and against the hypothesis), on the other hand, directly contradicts the Gaia hypothesis, in particular the CLAW hypothesis.
-Raven paradox: A counter-intuitive result in the philosophy of induction, specifically on what constitutes evidence in favor or against a hypothesis. Induction allows for empirical patterns to be generalized into laws or general hypotheses, and though the underlying hypothesis can never be proven no matter how many instances of the pattern are observed, a foundational tenet of induction is that each additional instance or empirical example increases the likelihood of the hypothesis, ie it adds evidence that confirms the hypothesis. The paradox arises when one considers hypotheses of the form "All A's are B's" and their contrapositives. For example, consider the hypothesis "all ravens are black". According to induction, every instance of a black raven that's observed provides additional evidence for the hypothesis. The contrapositive of this statement is "all non-black things are non-ravens", and again according to induction every instance of a non-black non-raven provides evidence for this hypothesis. Support for a hypothesis must support logically equivalent (ie biconditional) hypotheses as well, so now it would appear that every instance of an object that isn't black and isn't a raven provides evidence for ravens being black. Observing a green apple, for example, would provide non-zero evidence for the hypothesis "all ravens are black", a deeply counter-intuitive statement seeing as one would think that evidence not involving ravens shouldn't have bearing on properties of ravens. One proposed resolution of the paradox is to say that non-black non-ravens do provide evidence for black ravens, but because there infinitely more non-ravens than ravens, observing non-black non-ravens only provides infinitesimal (but non-zero) evidence for ravens, and this contradicts our intuition because we "approximate" the evidence gain as zero, not infinitesimal or extremely small.
-Mathew effect: Closely related to the Pareto principle, the Mathew effect is the sociological phenomenon characterized by the adage "the rich get richer and the poor get poorer"; more generally, the effect notes the tendency of a small proportion of a given population receiving and controlling the majority of its resources, in domains from businesses competing in a market to plants competing for sunlight in a rainforest to authors competing for a book deal. The effect can be explained by the combination of accumulative advantage with a network of winner-take-all systems. In the majority of the aforementioned competitive situations, in which multiple parties from a population compete for some reward or resource, the majority or even all of the reward often goes to the best or most fit party - the winner takes all or most of the reward. This means that if one party is even slightly better than the rest, even if by a razor thin, almost negligible margin, that party will nonetheless receive hugely disproportionate rewards relative to the rest. Further, this extreme discrepancy in resource distribution arising from winner-take-all systems creates a positive feedback loop, since members of the population with access to the most resources are most likely to "win" the next competitive situation, and again take all of the reward. This means that in a normally distributed population, parties with even slight initial advantages will end up winning the entire reward in each competitive scenario, and will continue exponentially accumulating rewards until they end up controlling a disproportionate fraction, even a majority, of the resources.
-Recognition primed decision: A psychological model for how humans make very fast and complex yet effective decisions. In the framework, the unconscious thought process when making a decision in a complex or ill-defined scenario under time constraints is expressed in two looping stages: (1) Generate a possible action, (2) Evaluate the action against the problem's constraints, repeating until an action satisfying all constraints is found. Thus, decision makers will choose the first course of action that comes to mind which satisfies the perceived constraints imposed by the problem, iteratively rejecting actions which fail this evaluation metric. The strategy is effective in scenarios of time pressure and imperfect information (in particular, in cases of a poorly defined problem and goals). The strategy is also highly dependent on the experience level of the decision maker, as experience with similar situations determines how quickly a decision maker can come up with courses of action, and how good those actions are. The RPD decision model can be split into three cases, or variations, which together further demonstrate the dependence of the strategy on experience.
    -Variation 1: The decision maker recognizes the situation as typical, and has experience with a prototypical situation in the past. The model will then reduce to a lookup table style "if-then" statement. The decision maker will draw on experience with similar prototypical situations to generate actions based on those that have worked in the past, possibly adapted. The decision maker has good knowledge of both the problem statement or situation as well as possible actions.
    -Variation 2: The decision maker understands his possible courses of action and their ramifications, but has incomplete information about the situation. In this case, the decision maker must attempt to use available information to model the situation, and then select a course of action. More experienced decision makers are more likely to better model the situation.
    -Variation 3: The decision maker understands the situation, but doesn't know what possible actions can be taken, or the ramifications of the actions he is cognizant of. The decision maker will then mentally perform a trial-and-error process, trying out different actions abd trying to imagine their implications. When an action is found that appears appropriate, that action is selected.
-Supervenience: A central concept in metaphysics, specifically ontology, that's defined as an ontological relation (that is, an abstract relation between objects concerning the ontological properties of those objects, ie those properties related to the existence and being of the objects) between two sets of properties of an abstract object; for some arbitrary abstract object (eg paintings) a set of properties A supervenes on a set of properties B if two instances of the abstract object (eg two specific paintings) differing with respect to A-properties implies that they differ with respect to B-properties as well. It's impossible to differ in A-properties without also differing in B-properties, or put another way, if two objects are identical with respect to their B-properties, then they must be identical with respect to their A-properties. The term is often used in cases where A-properties are "higher level" than B-properties, in the sense that the former are determined by, or "built out of", the latter. Intuitively, if A-properties supervene on B-properties, then it would seem that B-properties are on a lower layer of denotational abstraction than A-properties, since the equivalence relation induced by B-properties (namely, the mathematical relation that two objects are identical if and only if their B-properties match) is stronger, in fact fully inclusive of, the equivalence relation induced by A-properties. Because of the way supervenient properties relate to each other, many philosophical problems can be rephrased in terms of a question on supervenience. For example, the mind-body problem can be stated as the question "Do mental properties supervene on physical or neurological properties?". The question of where the aesthetic value of a work of art comes from can be phrased "Do the artistic or aesthetic properties of the work supervene on its physical properties?".
-Axons: Axons, also known as nerve fibers, are long, thin appendages protruding from the cell body of a neuron, with terminals on the other side intended to connect with the cell bodies of other neurons. Axons are how neurons form connections; indeed, synapses are precisely the area connecting an axon on the source neuron with the dendrites on the cell body of the target neuron. Axons are covered in a Myelin (a fatty white substance) sheath in order to increase electrical conductivity; axons carry electrical impulses (ie action potentials) between neurons. There are three kinds of axons - A delta fibers, B fibers, and C fibers.
    -A delta fibers: A type of sensory nerve fiber which carries cold, pressure, and some pain signals. A delta fibers are thinly myelinated, giving them a moderate conduction velocity (impulses travel between 2 and 30 meters per second). The nerve fibers are associated with sensations of sharp pain, and are closely related with reflexive responses (eg pulling a hand away from a hot stove). Thus, A delta fibers are axons for sensory neurons (also known as afferent neurons, which carry sensory information), and are linked with the afferent portion of the reflex arc (neural pathways that control involuntary, hard-wired reflexive actions; reflex arcs are possible because most sensory neurons aren't connected directly to the brain but rather to the spinal cord, allowing actions to occur quickly since signals don't need to be routed through the brain) for retracting against harmful substances. Some A delta fibers are also associated with sensations of temperature.
    -Group B nerve fiber: Moderately myelinated axons with a conduction velocity between 3 and 14 meters per second, which are generally the nerve fibers used by neurons in the autonomic nervous system (the part of the peripheral nervous system which unconsciously regulates bodily functions).
    -Group C nerve fiber: Unmyelinated axons with a small diameter and low conduction velocity (under 2 meters per second), which carry sensory information; specifically, slow burning pain. C fibers react to stimuli that can be thermal, mechanical, or chemical in nature, and respond to all kinds of physiological changes in the body. There are different kinds of C fibers: nociceptors, which are responsible for burning pain, warming-specific receptors, responsible for warmth, ultra-slow histamine-selective fibers, responsible for itching, tactile fibers, responsible for sensual touch, and mechano-receptors or metabo-receptors in muscles or joints, responsible for muscle exercise, burning, and cramping.
-Eugeroic: Also known as a wakefulness-promoting agent, a eugeroic is a drug which improves wakefulness and alertness, and as such are commonly used to treat sleeping disorders and excessive daytime sleepiness such as narcolepsy. Most eugeroics have very low addiction potential, and are used to counteract fatigue and increase motivation and productivity, and thus have been used recreationally as cognitive-enhancing drugs to improve productivity. The most common eugeroic in use is modafinil, which acts as a selective, relatively weak dopamine reuptake inhibitor.
-Intensional and extensional definitions: In language, there are, broadly, two ways of defining a term. An intensional definition of the term specifies the term's essence or meaning by specifying necessary and sufficient conditions on the term's usage, thus defining equivalent notions of the term using other pre-defined terms. An extensional definition, on the other hand, specifies the meaning of a term by listing everything that falls under the definition. Thus, it's impossible to give an extensional definition for a term with an infinite set of referents (an object to which a linguistic expression or symbol refers, where reference is a designation or relation between symbols and objects whereby the symbol acts as a means to link or connect to the object).
-Catharsis theory: A psychoanalytic theory that can be traced back to Aristotle but which gained academic prominence with Sigmund Freud, and A. A. Brill who popularized it within the US. The theory is that watching or participating in acts of physical aggression and violence is cathartic and purges violent, tense feelings in a form of venting. The theory has been used to justify violence in media, such as television and video games, and some have gone as far as to claim that it's healthy to regularly purge the "innate, primal urge to kill" by partipating in simulated acts of violence such as viewing a prize fight or playing a violent video game. However, hundreds of studies by social scientists have shown that viewing or participating in violence increases tendency for aggression, rather than assuaging it. Catharsis theory has thus been scientifically shown to be false.
-Large scale brain networks: A large-scale brain network is a set of brain regions (distinct neurological structures, organized by spatial proximity) with functional connectivity (see below)
    -Functional connectivity:
        -Background: To better understand the brain in neuroscience research, and in various medical applications, being able to visualize the brain's structure can be very important. Neuroimaging is a general term that refers to a variety of techniques for creating visual representations of the internal structure of the brain, and falls into two broad categories: (1) structural neuroimaging, in which the spatial structure of different parts of the brain is mapped, with the goal of understanding the gross, high-level spatial structure of the brain, and (2) functional neuroimaging, in which the brain's structure is imaged on a finer scale, directed at measuring brain functions (in terms of mapping onto cognitive constructs and external stimuli) and associating them with mental functions. One of the most common neuroimaging techniques is fMRI (see below)
            -Functional MRI: A non-invasive medical technique which uses MRI (see below) technology for functional neuroimaging, ie to correlate brain activity with psychological/mental functions and external stimuli. In the context of fMRI, brain activity is measured in terms of blood flow (and changes therein) to different parts of the brain, based on the assumption that when an area of the brain is in use, blood flow to that region increases.
                -Magnetic resonance imaging: (MRI) A medical imaging technique (***INCOMPLETE***)
    -Examples:
        -Default mode network: (DMN) A large-scale brain network thought to be responsible for internally directed, introspective thought. It is a collection of disparate areas in the brain which have been experimentally shown to have highly correlative activity in response to certain conditions or stimuli. The DMN is most active in task-negative periods (ie times when the subject is not actively working on a task requiring cognition) when the subject is not focused on the outside world, and instead is in a state of wakeful rest. As such, activity in the network is correlated with mind-wandering, the phenomenon wherein a subject is predominantly preoccupied with his thoughts, undirected at any focal point or stimulus. As such, the DMN is the brain network that's active "by default", when no specific task is being undertaken, hence the name. The scope of "internally directed thought" extends beyond autobiographical recall and mind-wandering to social working memory as well, and so DMN activity is also correlated with thinking about oneself or about others. On the other hand, DMN activity is negatively correlated with attention-based networks in the brain. It's been experimentally shown that though DMN activity correlates negatively with external goal-oriented tasks, such as tasks involving working memory or visual attention, it is positively correlated with tasks whose goals are aligned with the DMN's function, such as social analysis tasks or autobiogrpahical tasks.
            -Function: There are three primary functions carried out by the DMN, roughly categorized as underlying one's sense of self, one's ability to think about and understand others, and one's ability to ponder temporal events such as remembering the past or imagining the future. Thus, it's no coincidence that daydreaming and mind-wandering tends to involve thinking about other people or introspecting, often by getting lost in past memories or by building narratives about the future. Neurologically, different subsets of the brain regions composing the network correspond to each of the three functions.
                1. First, the network forms the basis for one's sense of self and identity, seeing as it's responsible for autobiogrpahical, introspective thought directed at oneself. The network gives rise to one's sense of self, covering autobiographical information, self-reference (traits and descriptions about oneself), and one's emotions. At its core is self-awareness and its influence on personality, such as memories of events and facts about oneself, recognized personality traits about oneself, and one's emotional state.
                2. Second, the DMN is also linked to sociological, empathetic tasks such as thinking about others. Broadly, this covers theory of mind (the ability to map mental states onto oneself and others). This covers thinking about others' minds and what others do and don't know, as well as how they interpret and process this knowledge. It extends to understanding others' emotions and empathizing with them, which is closely related to moral reasoning in determining the goodness of an action. The scope of this function also applies to broader, more general sociological constructs, such as one's attitude and judgments towards social concepts and mores, or reflection on the social characteristics of groups.
                3. Third, the DMN is responsible for a specific, common case of mind-wandering - reflecting about the past or future. DMN activity is correlated with recalling past events, imagining future events, thinking back to time periods and recalling detailed memories from that time period, and story comprehension (remembering and understanding a narrative).
            -Task-positive network: A lesser known and experimentally uncertain opposite of the DMN is the task-positive network, a network of brain regions whose activity correlates with attention-demanding tasks, and are also correlated during wakeful rest (in lack of activity). This makes the network anti-correlated (ie its activity is inversely correlated with) with and hence "opposite" of the DMN. The network consists primarily of brain regions from the prefrontal and parietal areas of the brain.
                -Anatomy: (***INCOMPLETE***)
-Self-organized criticality: The term self-organized criticality refers to a complex property that emerges as a result of a dynamical system satisfying certain conditions. Strictly speaking, self-organized criticality occurs in a dynamical system with a critical point as an attractor. Presumably, we can view the state of a system as a point in its state space, and view the entire system as a dynamical system.
    -Background:
        -Dynamical system: A dynamical system is a mathematical system in which the position of a given point in some (geometrical) state space depends on time. The idea has its origins in physics, but in a pure mathematical context, the definition is often slightly abstracted. The term "time" above really refers to any continuous indexing; studying a dynamical system is really about studying a function which maps points in the state space to the "next" point in the state space, depending on some real-valued variable. This function is the evolution rule of the dynamical system, and it describes how points in the system move over time. Formally, the evolution rule is a function from the Cartesian product of the state space and the positive real numbers back to the state space. Intuitively, this "encodes" for each time step (represented as a positive real number) a lookup-table specifying where each point in the state space goes in the next time step. Sometimes the real numbers are replaced with some other set, when the idea of continuous time needs to be generalized even further. The state space is typically simply a tuple of real numbersm but its geometry may have additional or looser structure than the corresponding Euclidean space. In other words, the state space is a manifold.
            -Manifold: Simply put, a manifold is a locally Euclidean topological space. By locally Euclidean, we mean that every point on the manifold has a neighborhood that's homeomorphic (that is, topologically equivalent) to some finite-dimensional (the dimension is fixed over all points on the manifold) Euclidean space, ie some Cartesian power of the real numbers. Note that this does not necessarily mean that the whole manifold is globally homeomorphic to Euclidean space, just that if one zooms in far enough, the space appears topologically flat and static. Common examples of manifolds are spheres and toruses. Manifolds can be intuitively visualized by extending the following relationship to higher dimensions - a continuous two-dimensional surface (typically thought of as resembling a deformed plane) is to a plane what a general manifold is to the Euclidean space to which it's locally homeomorphic. Informally, this means that we can construct manifolds by taking some n-dimensional Euclidean space, embedding it in (n + 1)-dimensional Euclidean space, and deforming the space through the surrounding space in such a way as to remain locally continuous.
            -Attractor: Intuitively, an attractor of a dynamical system is a subset of the state space towards which points tend to evolve over time. Moreover, once a point nears an attractor it stays near it, and once it enters the attractor it remains within it. Attractors are often classified according to their topological and geometrical structure as a subspace of the state space manifold (eg attractors with a self-similar fractal structure are called strange attractors). 
                -Formal definition: Consider the dynamical system defined by a state space S and evolution rule f: S times R^+ -> S, where "times" is the Cartesian product and R^+ is the set of positive real numbers. For any point x in S, f(0, x) = x and f(t, x) is the point x evolves into after t time steps, for positive real t. An attractor of this dynamical system is defined as a subset A of S such that
                    1. A is forward invariant, so for all a in A: f(t, a) is in A for every t.
                    2. A has a non-empty basin of attraction, defined to be the set of points x in S for which y is in A, where y = limit as t approaches infinity of f(t, x). The basin of attraction is simply the set of initial points which tend closer and closer to the attractor. The limit definition, though intuitive, is not strictly rigorous; we can reframe this definition as: the set of points x in S such that for every neighborhood of A there exists a T such that f(t, x) is in the neighborhood for every t > T.
                    3. A is as large as possible; that is, there does not exist a proper subset of A satisfying (1) and (2).
                -Motivation: Attractors are motivated by instances of dynamical systems in the physical world, which incidentally are the motivating source for the definition of a dynamical system. It's often the case in physics that a dynamical system has certian driving forces and certain dissipative forces (such as friction or heat loss), since dissipative forces almost always exist, and systems with only dissipative forces would just evolve towards a degenerate state of zero motion. In such cases, what often occurs is the driving force and dissipative force reach some kind of equilibrium or balance with each other (not necessarily steady-state); an attractor region in the state space is meant to capture this notion.
        -Critical point: Thermodynamic systems, a foundational idea in thermodynamics, a physical system of particles and radiative energy which are completely described by thermodynamic state variables such as temperature, entropy, pressure, Gibbs free energy, etc.; the system is said to be in equilibrium if there's no net macroscopic flow of energy within the system. A thermodynamic system in equilibrium is at rest and has no global changes in state variables (for example, it has a spatially uniform temperature). Another way to say this is that a thermodynamic system is in equilibrium if and only if it's time-independent. For a given system, different combinations of state variables might put the system in equilibrium, and owing to the smooth nature of the equations of evolution describing thermodynamic systems, such equilibrium points on a graph of state variables (usually pressure and temperature, which are often enough to determine the other state variables) tend to form a continuous curve, known as the equilibrium curve. A critical point of the thermodynamic system is an end point of a phase equilibrium curve (ie an equilibrium curve on a phase diagram, which is a diagram depicting different phases of the matter in a thermodynamic system depending on different thermodynamic variables, again usually pressure and temperature). Normally, in a thermodynamic system representing, say, a pure substance (as opposed to a mixture, for simplicity), what phase the matter is in (ie solid, liquid, gaseous, plasma, or something else) is determined by various state variables, usually pressure and temperature, and there are rigid phase boundaries, which are walls or curves on the phase diagram that partition the state space of the system into different phases depending on where the system is as determined by the values of the relevant state variables. Seeing as the critical point is the end point of the equilibrium curve, when a system is in a state beyond the critical point, the matter contained in the system is able to be in multiple phases at once, spontaneously switching between them (more precisely, this means arbitrarily small changes in state variables can cause phase transitions, eg between vapor and liquid), and all these phases can coexist in the system simultaneously; there are no phase boundaries at or after the critical point. Furthermore, the phases themselves become increasingly more similar in the vicinity of the critical point. For example, in the phase diagram for water, after the critical point the substance exists as a supercritical fluid, with properties of both a liquid and a solid which are very different from either phase on its own.
-Edge of chaos: (***INCOMPLETE***)
-Autophagy: A natural self-degradative cellular process by which cells disassemble and remove unnecessary and dysfunctional components (eg misfolded / misaggregated proteins, damaged organelles, etc.), as well as stray intracellular pathogens. Autophagy is most prominently an adaptation to stress (in particular, nutrient stress) and is hence a survival mechanism. It promotes cellular senescence (the process by which cells eventually stop dividing) and cell surface antigen prevention, both important mechanisms in preventing diseases such as cancer, neurodegeneration, cardiomyopathy, etc. and prevent necrosis. There are three main forms of autophagy: macroautophagy, microautophagy, and chaperone-mediated autophagy. Because autophagy is most prominent as a response to nutrient stress, and because of its detoxifying and anti-disease properties, it is thought to be the main reason for the observed anti-senescence effects of caloric restriction.
    -Macroautophagy: The main pathway for autophagy, used to remove damaged organelles and unnecessary (ie redundanct and unused) proteins. The pathway is carried out by the formation of a autophagosome (a secondary membrane) around the organelle or protein, which is then carried through the cytoplasm to a lysosome, which fuses with the autophagosome and destroys the payload.
    -Microautophagy: This is the same principle of macroautophagy, but the intended organelle or protein is directly consumed by a lysosome instead of transported by an autophagosome. The lysosome membrane can engulf cytoplasmic material through the process of invagination, in which the closest part of the membrane to the object folds inwards toward it, until it encapsulates it.
    -Chaperone-mediated autophagy: A very selective pathway for autophagy in which only certain, specific proteins are targeted for destruction. The proteins are directly translocated to the lysosome membrane for destruction, without the formation of additional vesicles such as autophagosomes.
-Emotional contagion: (***INCOMPLETE***)
-Self-disclosure: (***INCOMPLETE***)
-Social penetration theory: (***INCOMPLETE***)
-Asch conformity experiments: (***INCOMPLETE***)
-Information cascade: (***INCOMPLETE***)
-Lexical hypothesis: (***INCOMPLETE***)
-Baddeley's model of working memory: (***INCOMPLETE***)
-Angular resolution: In the general context of optics, the resolving power of any imaging device is broadly defined as its ability to distinguish close by points (this is where the idea of electronic screen resolution comes from - the higher the pixel density, the smaller and closer two points can be visually distinguished). Angular resolution indicates in imaging device's power to distinguish and pick up on small details of an object. It's defined as the minimum angular distance (the angular distance between two points is defined relative to some reference point as the angle subtended by the two points at the reference point) between two light sources for which the corresponding Airy patterns are distinguishable. Because the diffraction pattern created by the light shining through an aperture (ie the Airy pattern) is dependent on size of the waves relative to the opening they pass through, the angular resolution is propotional to the ratio between the light wavelength and the diameter of the circular aperture. The constant of proportionality has been experimentally found to be about 1.220. The angular resolution of a given imaging device depends on many factors, such as optical aberrations and light diffraction. Angular resolution can hence be increased by improving the optical quality of the imaging system, reducing optical aberrations, but diffraction limits pose a physical upper bound on angular resolution.
    -Airy pattern: (a.k.a. Airy disk) The density (in terms of photons) pattern cast by a light source through a small aperture. Because light is a wave, when it passes through a small opening (ie an aperture) or slit, it self-interferes and produces a diffraction pattern. In general, this applies to all waves, and the nature of the diffraction pattern depends on the relative size of the wavelength to the size of the opening. The Airy pattern of a uniform light source through a circular aperture is a series of concentric bright rings, increasing in size and intensity towards the center, giving rise to the name Airy disk.
    -Diffraction limit: For a given wavelength of light, the diffraction limit is the smallest angular distance between two points for which the Airy patterns can be distinguished. In other words, this is the same definition as that of angular resolution, but applied not to a specific imaging system taking instrument-specific aberrations and details into account, but rather is a theoretical upper bound derived from the physics of diffraction, ie of an idealized perfect imaging system. An imaging system is said to be diffraction-limited if it operates at this theoretical maximum resolution. The aperture diameter of a diffraction-limited system is precisely the size of the Airy disk.
-Snellen chart: The standard eye chart used to measure visual acuity (on the 20/x scale). The chart consists of rows of optotypes, with the optotypes in each row subtending smaller and smaller angles relative to a fixed distance specific to the chart. These subtended angles are designed to correspond, as angular resolutions, to 20/200, 20/100, 20/70, 20/50, 20/30, 20/25, 20/20, 20/15, etc. visual acuity measures. An acuity of 20/x means that the subject can see at x feet what the average person can see at 20 feet. Here, "average vision" is defined as being able to recognize an optotype subtending 5 minutes of arc. Because of the angular resolution of the human eye, human vision hits the diffraction limit at about 15 - 20 arcseconds, and hence, short of a physically larger (in diameter) eye, is upper bounded in visual acuity at about 20/7.
-Types of shadows: The three different parts of a shadow are called the umbra, the penumbra, and the antumbra. When a light source shines on a non-transparent (ie opaque) object, the resulting shadow decomposed into three distinct classes. The umbra is the primary part of the shadow, and is the innermost and darkest part. It's defined as the region of the shadow where the object completely blocks the light source. With an idealized point light source, the umbra is the only part of the shadow; the other two parts don't exist. In reality, point light sources don't exist, and as a result there are two additional regions of the shadow we can distinguish. The penumbra is the region of the shadow where only part of the light source is blocked by the object; in other words, viewing the object from within the penumbra would appear as a partial eclipse. The penumbra consists of two lighter regions of the shadow on either side of the umbra. Finally, the antumbra is a lighter region behind the umbra, from which the object is completely contained in the light source, so that viewing the object from the antumbra appears as an annular eclipse. 
-F statistic: The F-test is a statistical hypothesis test used in a statistical method called analysis of variance (or ANOVA). ANOVA uses the F-test to determine if the means of several (three or more) groups of data are statistically different or not. The defining characteristic of the F-test is that the test statistic of interest in the hypothesis test is distributed, assuming the null hypothesis, according to an F-distribution. Because the F-distribution is defined and motivated in terms of the F-statistic, the core of the F-test is the F-statistic. Consider two independent chi-squared distributions, X_n and X_m, where the former has n degrees of freedom and the latter has m; then the F-statistic if the ratio of the distributions, normalized by the degree of freedom (ie (X_n / n) / (X_m / m)), and the distribution of the F-statistic is defined to be the F-distribution. Informally, F-statistics are simply test statistics based on the ratio of mean squares in a way that accounts for degrees of freedom. In a one-way ANOVA, the F-statistic is defined as F = (variation between sample means) / (variation within samples). The definition is intuitive; given that our goal is to ascertain if a set of samples have the same mean, the more spread out the means are (ie the higher the variance of sample means) the more likely it is that the means are different, but of course this metric alone is flawed, as the sample means might be spread out and still come from the same distribution (and hence be the same) if it's the case that the distribution just happens to have high variance of its own. The denominator accounts for this flaw. Thus, the higher the F-statistic is, the more likely that the sample means are truly different. The F-statistic is simply another measure of statistical confidence in an alternate hypothesis (to the null hypothesis), and bijectively corresponds to a particular p-value.
-Exact real arithmetic: (***INCOMPLETE***)
-Universal grammar: (***INCOMPLETE***)
-Umwelt: (***INCOMPLETE***)
-Rule of three: (***INCOMPLETE***) In writing,
-Spindle neuron:

insular neurons

